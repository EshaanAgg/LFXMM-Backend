--
-- PostgreSQL database dump
--

-- Dumped from database version 16.0 (Debian 16.0-1.pgdg120+1)
-- Dumped by pg_dump version 16.0 (Debian 16.0-1.pgdg120+1)

SET statement_timeout = 0;
SET lock_timeout = 0;
SET idle_in_transaction_session_timeout = 0;
SET client_encoding = 'UTF8';
SET standard_conforming_strings = on;
SELECT pg_catalog.set_config('search_path', '', false);
SET check_function_bodies = false;
SET xmloption = content;
SET client_min_messages = warning;
SET row_security = off;

SET default_tablespace = '';

SET default_table_access_method = heap;

--
-- Name: parentorgs; Type: TABLE; Schema: public; Owner: admin
--

CREATE TABLE public.parentorgs (
    id integer NOT NULL,
    name character varying NOT NULL,
    logo character varying,
    skills character varying[]
);


ALTER TABLE public.parentorgs OWNER TO admin;

--
-- Name: parentorgs_id_seq; Type: SEQUENCE; Schema: public; Owner: admin
--

ALTER TABLE public.parentorgs ALTER COLUMN id ADD GENERATED ALWAYS AS IDENTITY (
    SEQUENCE NAME public.parentorgs_id_seq
    START WITH 1
    INCREMENT BY 1
    NO MINVALUE
    NO MAXVALUE
    CACHE 1
);


--
-- Name: projects; Type: TABLE; Schema: public; Owner: admin
--

CREATE TABLE public.projects (
    id integer NOT NULL,
    lfxprojectid character varying NOT NULL,
    name character varying NOT NULL,
    industry character varying[],
    description character varying DEFAULT ''::character varying,
    skills character varying[],
    programyear integer NOT NULL,
    programterm character varying NOT NULL,
    repository character varying NOT NULL,
    website character varying NOT NULL,
    amountraised double precision DEFAULT 0 NOT NULL,
    organizationid integer NOT NULL
);


ALTER TABLE public.projects OWNER TO admin;

--
-- Name: projects_id_seq; Type: SEQUENCE; Schema: public; Owner: admin
--

ALTER TABLE public.projects ALTER COLUMN id ADD GENERATED ALWAYS AS IDENTITY (
    SEQUENCE NAME public.projects_id_seq
    START WITH 1
    INCREMENT BY 1
    NO MINVALUE
    NO MAXVALUE
    CACHE 1
);


--
-- Data for Name: parentorgs; Type: TABLE DATA; Schema: public; Owner: admin
--

COPY public.parentorgs (id, name, logo, skills) FROM stdin;
24	Program to test errors	https://jobspring-prod-uploads.s3.amazonaws.com/8b5583c6-db96-4efa-8540-ca489341a931-.jpg	{""}
40	Envoy	https://jobspring-prod-uploads.s3.amazonaws.com/d57f9627-b6e9-499e-bd52-95544d565a42-.png	{""}
44	Jenkins	https://jobspring-prod-uploads.s3.amazonaws.com/1f6742ee-c057-4273-bbf7-0c98eac63e5c-.png	{""}
114	Mixcore	https://lff-prod-uploads.s3.amazonaws.com/5a17a10e-7bac-4e75-a058-84e3376b0e66	{""}
27	Buildpacks	https://jobspring-prod-uploads.s3.amazonaws.com/d315d5af-66ae-4c7d-a42c-5f817cef6988-.png	{go,docker,containers,javascript,css,html,design}
71	Choas Mesh	https://jobspring-prod-uploads.s3.amazonaws.com/39d01523-7e2d-4deb-b0f7-6e008fc53803-.png	{kubernetes,go}
82	Cloud Native Buildpacks	https://jobspring-prod-uploads.s3.amazonaws.com/d2648118-333b-4466-beab-df372207a0ee-.png	{go,git,buildpacks,css,react,ruby,rails}
18	COBOL	https://jobspring-prod-uploads.s3.amazonaws.com/2d1e7992-46bf-4787-8e63-473351b7753a-.svg	{cobol}
77	Codeuino	https://jobspring-prod-uploads.s3.amazonaws.com/fa403eab-c87a-4fcf-a871-dd27ce0ccaf1-.png	{oops,javascript,jquery,json,react,mongodb}
91	Crossplane	https://jobspring-prod-uploads.s3.amazonaws.com/618318c8-b31d-46ae-aed8-99c8377b501c-.png	{go}
12	Devfile	https://jobspring-prod-uploads.s3.amazonaws.com/1afb5c35-5691-4422-b133-5f64283432ec-.png	{go,kubernetes,compose}
113	FINOS	https://jobspring-prod-uploads.s3.amazonaws.com/b299c9d8-16be-46a3-8202-ae542dae58ab-.png	{hubspot,wordpress,marketing}
102	GenevaERS	https://jobspring-prod-uploads.s3.amazonaws.com/71138cf8-d11f-49da-9e2d-3d3cfc7cab44-.svg	{java}
68	GraphQL	https://jobspring-prod-uploads.s3.amazonaws.com/39e44dc8-1006-4825-91f0-da228a5f82fd-.png	{graphql}
8	IPP Fax Out	https://jobspring-prod-uploads.s3.amazonaws.com/d8707711-c89e-40da-8273-a15e608ca1c4-.png	{c,python}
56	KernelCI	https://jobspring-prod-uploads.s3.amazonaws.com/3bf6f2b7-5e5d-48b0-a8f1-6735db296459-.png	{linux,python,automation,grafana}
60	KubeVirt	https://jobspring-prod-uploads.s3.amazonaws.com/c60182ac-dbe2-4e58-94f7-6b6f2f62922a-.png	{go,kubernetes}
17	Kyverno	https://jobspring-prod-uploads.s3.amazonaws.com/ca9b9075-e0fe-4e54-bd57-5e1ac6c79b01-.png	{go,kubernetes,testing,security,kyverno,automation,documentation,prometheus,observability,golang,opentelemetry}
42	LF Networking	https://lff-prod-uploads.s3.amazonaws.com/lfn_internshipbadges.svg	{jenkins,bitergia,python,containers,docker,ansible,linux}
61	Linkerd	https://jobspring-prod-uploads.s3.amazonaws.com/f001152c-bf97-496a-8b30-db91ae9ca490-.png	{kubernetes,rust,go,javascript,react}
21	Linux Kernel Mentorship	https://jobspring-prod-uploads.s3.amazonaws.com/e708f297-7ea2-4cc0-b2c5-74e6296d58f1-.svg	{c,shell,kernel,python,linux,r,statistics,patience}
67	ONAP	https://jobspring-prod-uploads.s3.amazonaws.com/299d86b4-c583-4a5c-a923-022ea030ee8c-.jpg	{scripting}
46	OpenEBS	https://jobspring-prod-uploads.s3.amazonaws.com/417db659-c158-47da-b1e8-fbc45967cafd-.png	{go,kubernetes,testing,openebs,docker}
4	OpenHPC	https://jobspring-prod-uploads.s3.amazonaws.com/e566457d-b215-4a23-804a-d058d531d1b8-.svg	{git,bash,latex}
69	Open Printing	https://jobspring-prod-uploads.s3.amazonaws.com/a3ab0628-17a5-40f8-b679-cf17c5311ff7-.png	{c,printing}
45	Open Service Mesh	https://jobspring-prod-uploads.s3.amazonaws.com/839b12ff-5a58-46b4-9970-162d51c5d6a7-.png	{go,rust}
5	Patchwork	https://jobspring-prod-uploads.s3.amazonaws.com/8d7d68e6-3eda-4300-add5-da985ef5fb98-.png	{python,django}
48	Polycephaly	https://jobspring-prod-uploads.s3.amazonaws.com/c00137e1-ecc9-46f0-9f23-b3eff213589c-.svg	{groovy,jenkins}
49	Prometheus	https://jobspring-prod-uploads.s3.amazonaws.com/8b76b332-0137-44b3-bd6f-d2c8db04101d-.png	{go}
43	Service Mesh Interface	https://jobspring-prod-uploads.s3.amazonaws.com/8879a9a8-092c-46d4-8258-7e6619a6639e-.png	{go,rust}
78	Soda Foundation	https://jobspring-prod-uploads.s3.amazonaws.com/ed5d1f73-252e-48f8-bd78-7b02697efdcf-.png	{kubernetes,docker,typescript,shell,go}
76	SPDX Online Tools	https://lff-prod-uploads.s3.amazonaws.com/94002894-2620-4f1d-9f8c-79afe90f97de.png	{python,django}
89	SPIFFE/SPIRE	https://jobspring-prod-uploads.s3.amazonaws.com/970e86a2-32f7-4266-ba69-685248d4f075-.png	{go}
51	TikV	https://jobspring-prod-uploads.s3.amazonaws.com/aaee277a-6feb-4c72-aec2-a17295028198-.png	{rust,go,database}
94	TodoGroup	https://jobspring-prod-uploads.s3.amazonaws.com/1c119e74-4cce-4acb-91ac-cba0472a25b4-.png	{documentation}
31	WasmEdge	https://jobspring-prod-uploads.s3.amazonaws.com/84b404da-1d59-4288-bca8-8d7751ec4e45-.png	{rust,wasm,webassembly,javascript,tokio,kafka,cryptocurrency,wasmedge,filebeat,opencv,database,python,bazel,ffmpeg,elasticsearch}
101	ZOROW	https://jobspring-prod-uploads.s3.amazonaws.com/48f3c7fa-6a99-48ee-8d76-7413737a848d-.svg	{xml}
58	Argo	https://jobspring-prod-uploads.s3.amazonaws.com/e7de97de-970f-45ae-a95d-1b0a7fbeb19c-.png	{kubernetes,go}
111	ATOM	https://jobspring-prod-uploads.s3.amazonaws.com/92bb7ae8-29e6-4883-8124-8c62dd3785cd-.svg	{atom}
128	Mailu	https://lff-prod-uploads.s3.amazonaws.com/f18a507d-34fb-4220-aa1f-6e906887920c	{""}
144	Electron	https://jobspring-prod-uploads.s3.amazonaws.com/12ba39d8-dfc2-4979-a10f-5c7c0362eecd-.png	{""}
131	Pixie	https://jobspring-prod-uploads.s3.amazonaws.com/50a602fe-d1c0-4258-8faf-d7faf916e762-.png	{""}
181	Yocto Projct	https://lff-prod-uploads.s3.amazonaws.com/e6587ed6-d4c8-4318-beb8-43b7946cccb5	{""}
166	Resolve mentorship Issues	https://jobspring-prod-uploads.s3.amazonaws.com/48c7122c-4beb-4931-ad96-068073dbf06a-.png	{""}
122	CDP 2	https://jobspring-prod-uploads.s3.amazonaws.com/3ef4b4f3-a759-4c09-8055-8756f2de4fb8-.png	{arduino}
81	Chaos Mesh	https://jobspring-prod-uploads.s3.amazonaws.com/12e65c85-f438-463b-a768-1d50805a6a6a-.png	{go,python,kubernetes,shell,prometheus,grafana}
119	CNCF - Service Mesh Interface	https://jobspring-prod-uploads.s3.amazonaws.com/8879a9a8-092c-46d4-8258-7e6619a6639e-.png	{go,react}
104	CoreDNS	https://jobspring-prod-uploads.s3.amazonaws.com/3606c77f-8f42-4fe0-b92c-1476e3ad7435-.png	{go,dns}
57	Cortex	https://jobspring-prod-uploads.s3.amazonaws.com/7d80774e-f256-4c5a-b8cc-3f050122b657-.png	{go,kubernetes,prometheus,thanos}
183	CRI-O	https://jobspring-prod-uploads.s3.amazonaws.com/d0a3c555-c0b0-4e23-add7-35a1046ed440-.png	{containers,rust,go}
36	ELISA	https://jobspring-prod-uploads.s3.amazonaws.com/479b4fcd-6d85-4516-a24b-89e92710d7ce-.svg	{linux,c,kernel,python,docker,compiler,english}
118	etcd	https://jobspring-prod-uploads.s3.amazonaws.com/2c893ffb-c016-436a-b125-a33698e9b9b5-.png	{git,documentation}
115	Greenhouse Gas Estimation	https://jobspring-prod-uploads.s3.amazonaws.com/a45207df-40dd-4f14-8a51-4cc0694dfb69-.png	{r,moodle}
140	GSoD	https://jobspring-prod-uploads.s3.amazonaws.com/f6067ddd-ce39-469b-8350-be9462eccc15-.png	{documentation,statistics}
158	Harbor	https://jobspring-prod-uploads.s3.amazonaws.com/290ccbbc-2f5d-4888-9d0a-e4bc9876a544-.png	{golang,angular,javascript,clarity}
185	Istio	https://jobspring-prod-uploads.s3.amazonaws.com/a3a349f6-b0ba-49fb-bc02-bc37992b1727-.png	{writing,advocacy,python,networking}
178	Jaeger	https://jobspring-prod-uploads.s3.amazonaws.com/a254f86d-a176-4934-969f-fc740b441efe-.png	{javascript,go,typescript,npm,yarn,golang,java,elasticsearch}
124	Karmada	https://jobspring-prod-uploads.s3.amazonaws.com/ab4c408a-b18b-4033-9ec4-f18c053bd0c3-.png	{go,kubernetes,redux,react,figma,git,karmada,lua,algorithm}
148	Konveyor	https://jobspring-prod-uploads.s3.amazonaws.com/cd4c2889-4dfe-4a19-a2f4-9eec889dc988-.png	{golang,go,java,eclipse,kubectl,kubernetes,argocd,tekton,wasm,typescript,jest}
154	Kubewarden	https://jobspring-prod-uploads.s3.amazonaws.com/a9f29571-64a2-42ec-b796-49c1b7a1a4f9-.png	{rust,kubernetes,go}
127	Kuma	https://jobspring-prod-uploads.s3.amazonaws.com/99a3a52b-f575-4df7-b2b8-9b80c22b7f65-.png	{kubernetes,go}
132	LFX Engineering Mentorship	https://jobspring-prod-uploads.s3.amazonaws.com/590cf481-f426-420f-8721-53c6234e9b10-.svg	{go,api,angular}
149	NATS	https://jobspring-prod-uploads.s3.amazonaws.com/ec676b28-2dc1-49ad-98ce-9300bc7075e8-.png	{unity,nats}
137	OpenFunction	https://jobspring-prod-uploads.s3.amazonaws.com/8365f121-7a25-46c0-9b6c-ab09b050e76e-.png	{openfunction,python,kubernetes}
139	Open Horizon	https://jobspring-prod-uploads.s3.amazonaws.com/43dfc85f-0de7-4e14-973b-44ecfd7de0b0-.svg	{jekyll,make,yaml,travis,go,bash,git,scala,jenkins,markdown,json}
152	OpenKruise	https://jobspring-prod-uploads.s3.amazonaws.com/74e71819-5f0d-430f-8138-6d0b84a0fe5e-.png	{kubernetes,go,lua,docker}
177	Open Mainframe Project	https://jobspring-prod-uploads.s3.amazonaws.com/63247a7d-2bbc-48f3-afce-959880228e42-.png	{python,mysql,linux,flask}
14	Open Policy Agent	https://jobspring-prod-uploads.s3.amazonaws.com/7fd97524-9ab9-43da-b42f-5f0fe50a9e38-.png	{go,python,mongodb}
47	OpenTelemetry	https://jobspring-prod-uploads.s3.amazonaws.com/70b2681d-00ad-4b94-ad3b-a8b9ca8923ee-.png	{php,go,html,css}
106	Racklet	https://jobspring-prod-uploads.s3.amazonaws.com/b0d0ed88-77e5-458a-9aaf-893349963f33-.png	{go,rust,kubernetes,linux}
171	RISC-V	https://jobspring-prod-uploads.s3.amazonaws.com/ef5cece8-7dd6-4b32-9e1e-3574c9fbb954-.png	{javascript,rtl,python,hdl,webassembly,sail,systemverilog,spidermonkey,html,eclipse,tcl,git,ide,css,ebpf}
126	SERV	https://jobspring-prod-uploads.s3.amazonaws.com/e849ca86-cdde-40dc-a878-dc988454ad13-.png	{verilog,scripting}
174	Strimzi	https://jobspring-prod-uploads.s3.amazonaws.com/b30754d5-6e64-456a-a237-ea7073c5cfcb-.png	{java,mqtt,kafka}
159	TestGrid	https://jobspring-prod-uploads.s3.amazonaws.com/16cd9abb-3010-4a1f-ad7d-d64397a1e9af-.png	{typescript,css,go}
179	Test Mentorship Program	https://jobspring-prod-uploads.s3.amazonaws.com/0b5d4876-97bc-42fd-91f5-dad7c5412b7f-.png	{algorithm}
167	Alpha-Omega	https://jobspring-prod-uploads.s3.amazonaws.com/dd00771c-0818-4fe0-b7b0-c6b379d855a7-.png	{api,azure,compiler,java,python,docker,security,git}
176	Armada	https://jobspring-prod-uploads.s3.amazonaws.com/cbb441a9-8d89-4783-b3a8-bd1d1264298b-.png	{go,sql}
180	Hyperledger Collaborative Learning	https://jobspring-prod-uploads.s3.amazonaws.com/c1939376-b18e-4979-90ab-2b697b8876dc-.png	{dlt,go,blockchain,golang,linux,java,kotlin,bash,git,docker,raft,typescript,chaincode,python,react}
73	Keptn	https://jobspring-prod-uploads.s3.amazonaws.com/fe5972a4-b903-4717-b9a2-4caede7286f8-.png	{go,prometheus,javascript,kubernetes,angular,docker}
9	KubeArmor	https://jobspring-prod-uploads.s3.amazonaws.com/4b023b5f-17d3-4829-94b4-0a6bbb24cc1e-.png	{go,kubernetes,kubearmor,grafana,kernel,c,opentelemetry,rancher,loki,javascript,openshift,oci,fluentd,ebpf}
150	Kubescape	https://jobspring-prod-uploads.s3.amazonaws.com/f8975bc1-eb60-4713-bc79-b276a10efe9b-.png	{go,rego,python,cybersecurity,prometheus,scripting}
116	Linux Kernel	https://jobspring-prod-uploads.s3.amazonaws.com/e41c5498-39dc-467a-9fa1-bb8b2430550e-.png	{c,shell,kernel,linux,perl,bash,python}
13	LitmusChaos	https://jobspring-prod-uploads.s3.amazonaws.com/fd622e15-c598-4f8a-b960-e4de16d8f547-.png	{go,kubernetes,typescript,javascript,cli,prometheus,react}
109	Mainframe Open Education	https://jobspring-prod-uploads.s3.amazonaws.com/bfe14f6a-5fcc-4fc6-846f-0589b018aaa6-.svg	{documentation}
161	Open Daylight	https://jobspring-prod-uploads.s3.amazonaws.com/4774ae55-ada2-4307-98d8-a45413fe2bef-.png	{python,git,bash,packer,jenkins,css,c,oop,wordpress,kubernetes,netlify,docker,gerrit,hugo,java,database,html,cilium,json,vim,go,tensorflow,anuket,django,cnf,nfv,yaml,dashboard,api,ebpf}
136	OpenELB	https://jobspring-prod-uploads.s3.amazonaws.com/51aa20e6-79e6-4529-9495-5b7385e8108c-.png	{helm,docker,go,kubernetes}
2	Open Mainframe Project 2020 Mentorship	https://jobspring-prod-uploads.s3.amazonaws.com/92442f60-8922-451c-b811-ebdb9af7d7aa-.svg	{javascript,typescript,java,html,devops,development,ansible,kubernetes}
97	Open@RIT	https://jobspring-prod-uploads.s3.amazonaws.com/3f9ccc68-e3d7-4e5f-b406-a061f090e4c5-.png	{documentation,design,deployment,git,markdown,javascript,react,express}
121	Service Mesh Performance	https://jobspring-prod-uploads.s3.amazonaws.com/ceb0f1e2-e942-48b9-b56e-e416885133c1-.png	{go,docker,kubernetes,analytics,grpc,cuelang,algorithms,react,algorithm,golang}
11	Software Discovery Tool	https://jobspring-prod-uploads.s3.amazonaws.com/baf7a3ef-2a59-439c-b848-d4306fc49c4a-.svg	{json,python,flask}
66	stress-ng	https://jobspring-prod-uploads.s3.amazonaws.com/236fbb01-5e4d-4118-8cd3-a7fc7c41c25e-.png	{gcov,git,c,linux,kernel}
26	Thanos	https://jobspring-prod-uploads.s3.amazonaws.com/c639e64e-b587-401a-a85d-13a9da606e61-.png	{go,react,typescript,yaml,github,golang,grpc,git,ansible,shell,http,prometheus,kubernetes,dns,linux,python}
90	TiKV	https://jobspring-prod-uploads.s3.amazonaws.com/931ce9b6-1ee4-4b8d-a322-bcfc964f0a51-.png	{rust,clojure,javascript,typescript}
20	Tremor	https://jobspring-prod-uploads.s3.amazonaws.com/159d5cf3-bafd-487b-8242-2c137f1d776a-.png	{rust,linux,testing,http,git,gitops,database,erlang,documentation,make,ci,devops}
107	Vitess	https://jobspring-prod-uploads.s3.amazonaws.com/b5b540c8-8bc5-4513-ae52-337a4e0f0769-.png	{go,sql,yacc,lexers,vite,docker,compiler,react,compilers,django,bash,golang,python,vercel}
34	Volcano	https://jobspring-prod-uploads.s3.amazonaws.com/922b9cb2-2873-4403-8836-1217541a3f76-.png	{go,volcano,kubernetes,golang,jobflow,ut}
108	CNCF	https://jobspring-prod-uploads.s3.amazonaws.com/d72af995-5b27-4835-b0ee-1cf4d7ec57e4-.png	{python,javascript,go,testing,terraform,docker,documentation,kubernetes,rust}
160	CNCF Landscape	https://jobspring-prod-uploads.s3.amazonaws.com/e3c26d3b-3131-484e-b168-bd3101cb2dd9-.png	{shell,terminal,kubernetes,cuelang,golang,figma,prototyping,oci,go}
133	Design and implement a Linux abstraction layer for protected read	https://jobspring-prod-uploads.s3.amazonaws.com/503378ff-805c-4edc-9ea3-5c74200fbea2-.png	{linux,programming}
72	Fluentd	https://jobspring-prod-uploads.s3.amazonaws.com/1cbae570-6955-4ba5-97ba-3c2b357bc2a9-.png	{c,linux}
175	Knative	https://jobspring-prod-uploads.s3.amazonaws.com/cd14e1fd-6c71-4489-807e-c7c6e8eeee0e-.png	{kubernetes,networking,knative,java}
117	Kubevela	https://jobspring-prod-uploads.s3.amazonaws.com/6e226a17-97a7-4207-9802-3b7cee2b70e3-.png	{go,kubernetes,cuelang,terraform,typescript,mysql,postgresql,java,python,cue}
74	KubeEdge	https://jobspring-prod-uploads.s3.amazonaws.com/d1cc594d-42e6-4a1c-9068-006831a0ce02-.png	{kubeedge,kubernetes,javascript,go,html,golang,python,tensorflow,pytorch}
75	Kubernetes	https://jobspring-prod-uploads.s3.amazonaws.com/97f183f1-157e-4dd9-981d-fd3712ffe66c-.png	{go,kubernetes,cli,testing,git,linux,python,automation,cd,ci,html,synthesis,flask,cryptographic,javascript,json,ux,google,bash,english,sql,documentation,css,design,golang}
186	Meshery	https://jobspring-prod-uploads.s3.amazonaws.com/ff7860bc-f890-4f92-b721-1fc5bb1bd140-.png	{go,react,graphql,rego,redux,wasm,golang,oci,docker,css,javascript,api,figma,webassembly,rust,terraform,openapi,kubernetes,jekyll,helm,apollo,temporal,reactjs,cuelang}
155	Notary	https://jobspring-prod-uploads.s3.amazonaws.com/c975096c-8872-43c6-b07f-d65b1f40cad0-.png	{notary,css,oci,docker,git,hugo,javascript,kubernetes,markdown,go,html}
156	ORAS	https://jobspring-prod-uploads.s3.amazonaws.com/26e5d3ce-10c3-4a4c-8500-735dacc21219-.png	{javascript,oras,docker,html,css,hugo,figma,oci,markdown}
38	Zowe	https://jobspring-prod-uploads.s3.amazonaws.com/d2be9005-6566-4bcc-8df7-58d5d7f0fcb6-.svg	{mainframe,java,developer,typescript,python,json,javascript,cobol,c,jit,blockchain,angular,css,react,ibm,sql,ai,documentation,html,programming}
16	All In	https://jobspring-prod-uploads.s3.amazonaws.com/d9757eab-8faa-4d6f-bd80-4b87a1e83e46-.png	{documentation,git}
184	Carvel	https://jobspring-prod-uploads.s3.amazonaws.com/0cbb78ca-1758-470b-bd70-63b6e7daf776-.png	{golang}
28	Cilium	https://jobspring-prod-uploads.s3.amazonaws.com/f7d2860d-a436-48c6-bc45-fb361f2ea111-.png	{go,kubernetes,javascript,docker,security,css}
157	Confedential Computing Fellowshio	https://jobspring-prod-uploads.s3.amazonaws.com/4d84f000-0b5c-48a4-b861-4d6359107e53-.jpg	{rust,git,linux,security,webassembly}
110	Hypereldger	https://jobspring-prod-uploads.s3.amazonaws.com/6f59cb4b-fd09-4ba5-85f7-e15390d6dbeb-.png	{python,docker,javascript,java,kubernetes,git,blockchain,compiler,golang,go,html,postgresql,rust,django,research,protobuf,bash,fabric,linux,gitlab,github,solidity,ansible,molecule,iroha,networking,json,helmcharts,css,gitops,sql,cloud,rocksdb,rocketdb,cmake,programming,vm,openshift,aws,typescript,mininet,teamwork}
1	Hyperledger	https://jobspring-prod-uploads.s3.amazonaws.com/b08449a3-ed50-4361-bed3-055241bcd875-.png	{git,blockchain,javascript,rust,docker,typescript,python,java,go,solidity,fabric,react,research,ansible,html,ethereum,http,bash,linux,chaincode,shell,programming,helm,dlt,markdown,collaborative,css,vscode,kotlin,compiler,yaml,kubernetes,openapi,django,json,quorum,sla,algorithm,angular,communcation,database,c,github,mapping,figma,telecom,remix,wireframes,goquorum,parsers,truffle,postgresql,cryptography,container,devops,protobuf,scouting,besu,testing,cmake,communication}
\.


--
-- Data for Name: projects; Type: TABLE DATA; Schema: public; Owner: admin
--

COPY public.projects (id, lfxprojectid, name, industry, description, skills, programyear, programterm, repository, website, amountraised, organizationid) FROM stdin;
124	f380d879-3fdd-4e09-a3e8-92279806bb41	CodeChecker: Integrate kernel development tools with CodeChecker's Web UI	{"The Linux kernel development uses a number of tools, checkpatch.pl, coccinelle scripts, sparse, etc. and these tools report certain findings. While the valid ones are addressed by the kernel developers, the invalid tool findings are manually assessed and not acted upon. Over time with addressing the valid findings, the proportion of invalid findings increase compared to newly appearing valid findings, as invalid findings of those tools are not marked and tracked over the various versions.\nCodeChecker provides a Web UI to track false positive findings from static analysis tool. Extend CodeChecker to allow to import findings from checkpatch.pl, coccinelle scripts, sparse, etc."}	The Linux kernel development uses a number of tools, checkpatch.pl, coccinelle scripts, sparse, etc. and these tools report certain findings. While the valid ones are addressed by the kernel developers, the invalid tool findings are manually assessed and not acted upon. Over time with addressing the valid findings, the proportion of invalid findings increase compared to newly appearing valid findings, as invalid findings of those tools are not marked and tracked over the various versions.\nCodeChecker provides a Web UI to track false positive findings from static analysis tool. Extend CodeChecker to allow to import findings from checkpatch.pl, coccinelle scripts, sparse, etc.	{python,linux,kernel,docker,c}	2020	Term 3	https://github.com/Ericsson/codechecker	https://docs.google.com/document/d/19sYZP67sepFDEKVBy7PGkdY62ifsmic3S5iic2et2x8/edit?usp=sharing	300000	36
86	fe0f6feb-4571-4dd5-9bfd-8a01c10ed734	KernelCI	{"The mission of the KernelCI project is to ensure the quality, stability and long-term maintenance of the Linux kernel by maintaining an open ecosystem around test automation practices and principles.\n\n"}	The mission of the KernelCI project is to ensure the quality, stability and long-term maintenance of the Linux kernel by maintaining an open ecosystem around test automation practices and principles.	{linux,python,automation,grafana}	2020	Term 1	https://github.com/kernelci/	https://foundation.kernelci.org/	0	56
348	f65481a3-a745-4230-a0cf-61ffdc00876d	CNCF - Kubernetes: Add GPU support to Cluster API Provider GCP (CAPG)	{"The Cluster API is a Kubernetes project to bring declarative, Kubernetes-style APIs to cluster creation, configuration, and management. CAPG provides this Kubernetes-native declarative infrastructure for GCP. \n\nThis project would focus on adding support for attaching GPUs to instances used for worker nodes in a Kubernetes cluster created by CAPG."}	The Cluster API is a Kubernetes project to bring declarative, Kubernetes-style APIs to cluster creation, configuration, and management. CAPG provides this Kubernetes-native declarative infrastructure for GCP. \n\nThis project would focus on adding support for attaching GPUs to instances used for worker nodes in a Kubernetes cluster created by CAPG.	{kubernetes,git,go,google,design}	2022	Term 2	https://github.com/kubernetes-sigs/cluster-api-provider-gcp		300000	75
597	47f53d22-ff5c-4479-b701-3ca3dbc7df0a	CNCF - Kubernetes: Build a Go library and CLI for interacting with OpenBuildService	{"Kubernetes is set to start using [OpenBuildService](http:","","openbuildservice.org) as a platform for building, publishing, and hosting Kubernetes system (Debian and RPM) packages. The current integration with the OpenBuildService platform assumes a lot of manual tasks and depending on `osc` command-line tool written in Python. At SIG Release, we're striving to automate as many tasks as possible. We want to build a library and CLI written in Go for interacting with the OpenBuildService APIs and platform that can be integrated with our existing [release tooling (`krel`)](http:","",github.com,kubernetes,"release).\n- Expected Outcome: Library and CLI tool for interacting with OpenBuildService platform via their publicly available APIs. Both library and CLI tool should be properly tested via unit, integration, and end-to-end tests, and properly documented."}	Kubernetes is set to start using [OpenBuildService](http://openbuildservice.org) as a platform for building, publishing, and hosting Kubernetes system (Debian and RPM) packages. The current integration with the OpenBuildService platform assumes a lot of manual tasks and depending on `osc` command-line tool written in Python. At SIG Release, we're striving to automate as many tasks as possible. We want to build a library and CLI written in Go for interacting with the OpenBuildService APIs and platform that can be integrated with our existing [release tooling (`krel`)](http://github.com/kubernetes/release).\n- Expected Outcome: Library and CLI tool for interacting with OpenBuildService platform via their publicly available APIs. Both library and CLI tool should be properly tested via unit, integration, and end-to-end tests, and properly documented.	{golang}	2023	Term 3	https://github.com/kubernetes/sig-release/issues/2295	https://kubernetes.io	0	75
527	21c95d53-dd75-4a2f-a8fd-92374c54940d	CNCF - Kubevela: Expand multiple database drivers for the API server	{"Now KubeVela's VelaUX uses two kinds of Database to store metadata: Kubernetes ConfigMap and MongoDB. As more users are expecting using different kinds of database. We proposing to expanding multiple database drivers for the VelaUX API server. \n- Expected Outcome: The outcome of this project will be expand two more database driver for KubeVela VelaUX API server:"}	Now KubeVela's VelaUX uses two kinds of Database to store metadata: Kubernetes ConfigMap and MongoDB. As more users are expecting using different kinds of database. We proposing to expanding multiple database drivers for the VelaUX API server. \n- Expected Outcome: The outcome of this project will be expand two more database driver for KubeVela VelaUX API server:	{mysql,postgresql}	2023	Term 2	https://github.com/kubevela/kubevela/issues/5426	https://kubevela.io/	300000	117
142	a06b291e-6cab-4506-8d32-2d15264e48dc	CNCF - Kubernetes SIG Usability: Qualitative analysis of user interview recordings for Jobs-to-Be-do	{"SIG Usability is conducting a Jobs-to-Be-Done study meant to identify the highest impact areas for improving Kubernetes UX. We are in the process of conducting user interviews and need some help going back through the transcribed recordings to annotate and pull out insights from the conversations. Overall, this is a great opportunity for someone who’s studied or engaged in UX",IA,"Usability to get involved in open source."}	SIG Usability is conducting a Jobs-to-Be-Done study meant to identify the highest impact areas for improving Kubernetes UX. We are in the process of conducting user interviews and need some help going back through the transcribed recordings to annotate and pull out insights from the conversations. Overall, this is a great opportunity for someone who’s studied or engaged in UX/IA/Usability to get involved in open source.	{ux,synthesis}	2021	Term 1	https://github.com/kubernetes-sigs/sig-usability/issues/9	kubernetes.io	0	75
9	7fbfee27-5402-4fd6-a36f-388ddf89c0fa	Patchwork tool development Unpaid 2023	{"We are looking for an internship for the LF internship program that can help developing the Patchwork tool which is used by the Linux Kernel Media subsystem and by many other open source projects.\n\nWe're aiming to add an audit trail on patchwork that will allow to identify the \tevents that triggers patch status changes, like:\n- when a new patch is added to Patchwork;\n- when a patch status changed and by whom;\n- when a patch is delegated and by whom.\n\nAs a secondary goal,  identify missing features that exists on patchwork forks (in particular, the one used by Freedesktop) and porting them to upstream.\n\nTechnical qualifications:\n\n- Capable of developing high quality programs in python3;\n- Knowledge on Python Django framework;\n- Some experience with Patchwork is desired (https:","",github.com,getpatchwork,"patchwork)\n\nAt the end of the project, the mentee will learn how to develop high quality Python and submit them using procedures similar to the Linux Kernel submissions.\n\nAfter the end of the project, the applicant will have developed high quality Python and understand the submission process to an upstream open sourc project."}	We are looking for an internship for the LF internship program that can help developing the Patchwork tool which is used by the Linux Kernel Media subsystem and by many other open source projects.\n\nWe're aiming to add an audit trail on patchwork that will allow to identify the \tevents that triggers patch status changes, like:\n- when a new patch is added to Patchwork;\n- when a patch status changed and by whom;\n- when a patch is delegated and by whom.\n\nAs a secondary goal,  identify missing features that exists on patchwork forks (in particular, the one used by Freedesktop) and porting them to upstream.\n\nTechnical qualifications:\n\n- Capable of developing high quality programs in python3;\n- Knowledge on Python Django framework;\n- Some experience with Patchwork is desired (https://github.com/getpatchwork/patchwork)\n\nAt the end of the project, the mentee will learn how to develop high quality Python and submit them using procedures similar to the Linux Kernel submissions.\n\nAfter the end of the project, the applicant will have developed high quality Python and understand the submission process to an upstream open sourc project.	{python,django}	2023	Term 3	https://github.com/getpatchwork/patchwork	https://linuxtv.org/	0	5
367	de2d206e-32cc-45da-bc5a-1fbc7bc1f5c8	CNCF - Thanos: Load balancing of API communication in Thanos	{"Thanos uses gRPC for the majority of network communication. It performs fanouts and sharding of different queries to multiple nodes in a distributed system. Unfortunately, due to the nature of the gRPC, a conventional TCP-based load balancer (e.g. K8s Service) is not enough to distribute requests equally to multiple replicas of the same stateless node. As a result, there is a need to figure out the pragmatic way for Thanos users to load balance requests to multiple backends either by gRPC client load balancing or by guides and integration with popular load balancing proxies like nginx, caddy or envoy.\n\nUpstream Issue (URL):\n- https:","",github.com,thanos-io,thanos,issues,"3016\n- https:","",github.com,thanos-io,thanos,issues,1083}	Thanos uses gRPC for the majority of network communication. It performs fanouts and sharding of different queries to multiple nodes in a distributed system. Unfortunately, due to the nature of the gRPC, a conventional TCP-based load balancer (e.g. K8s Service) is not enough to distribute requests equally to multiple replicas of the same stateless node. As a result, there is a need to figure out the pragmatic way for Thanos users to load balance requests to multiple backends either by gRPC client load balancing or by guides and integration with popular load balancing proxies like nginx, caddy or envoy.\n\nUpstream Issue (URL):\n- https://github.com/thanos-io/thanos/issues/3016\n- https://github.com/thanos-io/thanos/issues/1083	{go,http,grpc}	2022	Term 3	https://github.com/thanos-io/thanos/issues/3016	https://thanos.io/	0	26
12	7f416728-2578-471b-9c7a-2136ebdb1e46	Wrapping proprietary printer drivers into a Printer Application Support for IPP Fax Out.	{"With sandboxed packaging, Linux distributions appear which do not use classic RPM or DEB packages any more, like the all-Snap Ubuntu Core. As a result CUPS will not work with classic PPD","filter-based printer drivers any more but requires all drivers being provided as Printer Applications. The student's task is to add this functionality to the Printer Application framework and create an easy way for users to install proprietary printer drivers into that chroot-equipped Printer Application.\nIPP Fax out is a feature which is currently missing in Linux unlike Print and Scan. The student's task here is to make this functionality easily accessible for users of common desktop Linux distributions. Like printers, faxes should automatically appear on the system and get available in print dialogs and if one prints to a fax, fax-specific options should also appear in the print dialog and the user should be able to pick phone numbers also from contacts.\n"}	With sandboxed packaging, Linux distributions appear which do not use classic RPM or DEB packages any more, like the all-Snap Ubuntu Core. As a result CUPS will not work with classic PPD/filter-based printer drivers any more but requires all drivers being provided as Printer Applications. The student's task is to add this functionality to the Printer Application framework and create an easy way for users to install proprietary printer drivers into that chroot-equipped Printer Application.\nIPP Fax out is a feature which is currently missing in Linux unlike Print and Scan. The student's task here is to make this functionality easily accessible for users of common desktop Linux distributions. Like printers, faxes should automatically appear on the system and get available in print dialogs and if one prints to a fax, fax-specific options should also appear in the print dialog and the user should be able to pick phone numbers also from contacts.	{c,python}	2020	Term 3	https://github.com/OpenPrinting	https://openprinting.github.io/	225000	8
65	ee48108e-0ffb-4106-9502-4f1d55550fef	LF Networking ODL -JSON Examples for ODL User Guides	{"OpenDaylight user guide is extensively referred by users to understand how to interact with the controller. It details the APIs supported by different plugins and provides examples for the same. However, the user guides were originally written a few years ago and have most examples as XML payloads. Over the years though, JSON has turned into a format of choice for most developers, users to interact with such platforms. Hence, this project is to improve the documentation to include JSON examples for most widely used projects. The scope will be limited to the user guides for the following:\n\n- NETCONF\n- BGP\n- PCEP\n- Openflow"}	OpenDaylight user guide is extensively referred by users to understand how to interact with the controller. It details the APIs supported by different plugins and provides examples for the same. However, the user guides were originally written a few years ago and have most examples as XML payloads. Over the years though, JSON has turned into a format of choice for most developers, users to interact with such platforms. Hence, this project is to improve the documentation to include JSON examples for most widely used projects. The scope will be limited to the user guides for the following:\n\n- NETCONF\n- BGP\n- PCEP\n- Openflow	{json,api,git,vim}	2020	Term 2	https://git.opendaylight.org/gerrit/	https://wiki.lfnetworking.org/display/LN/JSON+examples+for+ODL+User+Guides	600000	161
53	79b69ae1-0ce4-46a6-affe-6338022ff40a	CNCF - Volcano: System Stability Enhancement	{"Add more UT","E2E to cover more classical scenarios. Conduct complete stress testing and regression testing, Offer test report, give the improvement plan and put it into practice."}	Add more UT/E2E to cover more classical scenarios. Conduct complete stress testing and regression testing, Offer test report, give the improvement plan and put it into practice.	{go}	2021	Term 1	https://github.com/volcano-sh/volcano/issues/1284	https://volcano.sh/en/	300000	34
54	08d37da9-56cb-42bd-ae17-99fec51c9e1d	CNCF - WasmEdge: Improving the performance of running rustpython	{"Description: WasmEdge is a lightweight, high-performance, and extensible WebAssembly runtime for cloud native, edge, and decentralized applications. WasmEdge is an official sandbox project hosted by the CNCF. WasmEdge is designed for the general purpose wasm runtime. However, when running rustpython, we found the performance is worse than other runtimes such as wasmtime, even after using the ahead-of-time compilation."}	Description: WasmEdge is a lightweight, high-performance, and extensible WebAssembly runtime for cloud native, edge, and decentralized applications. WasmEdge is an official sandbox project hosted by the CNCF. WasmEdge is designed for the general purpose wasm runtime. However, when running rustpython, we found the performance is worse than other runtimes such as wasmtime, even after using the ahead-of-time compilation.	{}	2022	Term 1	https://github.com/WasmEdge/WasmEdge/issues/1061	https://wasmedge.org/	300000	31
84	3cbfb661-bc50-42cd-96f1-4c0b430eba8d	OPNFV Internship	{"OPNFV Internship is for University or College students who are interested in contributing to the OPNFV project. Each intern will be working with a mentor in the OPNFV community to complete a project either in 3 months (full time 40 hours a week) or 6 months (part time 20 hours a week). This is a paid internship program and the internship project can start throughout the year (i.e. not just limited to summer in the northern hemisphere). As an open source project, each intern will work remotely from his or her location of choice. And, we welcome interns from anywhere in the world."}	OPNFV Internship is for University or College students who are interested in contributing to the OPNFV project. Each intern will be working with a mentor in the OPNFV community to complete a project either in 3 months (full time 40 hours a week) or 6 months (part time 20 hours a week). This is a paid internship program and the internship project can start throughout the year (i.e. not just limited to summer in the northern hemisphere). As an open source project, each intern will work remotely from his or her location of choice. And, we welcome interns from anywhere in the world.	{python,containers,docker,ansible,linux}	2019	Term 2	https://github.com/opnfv/	https://wiki.opnfv.org/display/DEV/Intern-projects-page	2400000	42
19	d9968740-faa6-4f6a-a943-dc9f427fd81c	CNCF - LitmusChaos: Add event & alerts infrastructure to the litmus portal	{"LitmusChaos is a Kubernetes native chaos engineering framework that helps SREs & developers find weaknesses in their deployments, with the chaos intent being defined via custom resources. The Litmus portal is a dashboard focused on simplifying the chaos-engineering experience for users and allows the execution of complex \\"chaos workflows\\" that comprise one or more chaos experiments. This portal dashboard needs to be improved to hold more observability information, primarily in the form of an event log & alerts to help users gather important information about the state of the chaos experiments & cluster in general."}	LitmusChaos is a Kubernetes native chaos engineering framework that helps SREs & developers find weaknesses in their deployments, with the chaos intent being defined via custom resources. The Litmus portal is a dashboard focused on simplifying the chaos-engineering experience for users and allows the execution of complex "chaos workflows" that comprise one or more chaos experiments. This portal dashboard needs to be improved to hold more observability information, primarily in the form of an event log & alerts to help users gather important information about the state of the chaos experiments & cluster in general.	{go,typescript}	2021	Term 1	https://github.com/litmuschaos/litmus/issues/2429	https://litmuschaos.io/	300000	13
21	12a9270f-8673-4acb-92ec-fd539fc2b567	Open Policy Agent	{"Policy-based control for cloud native environments."}	Policy-based control for cloud native environments.	{go,python,mongodb}	2020	Term 2	https://github.com/open-policy-agent/opa	https://www.openpolicyagent.org/	550000	14
17	a2cf842b-c4e8-45b0-a69b-113f49d39b19	Software Discovery Tool - Add z/OS support	{"Develop back and front end support for z","OS in the Flask framework (Python), and work with the team to create the initial JSON source file for open source z","OS software"}	Develop back and front end support for z/OS in the Flask framework (Python), and work with the team to create the initial JSON source file for open source z/OS software	{python,flask,json}	2021	Term 2	https://github.com/openmainframeproject/software-discovery-tool	https://github.com/openmainframeproject/software-discovery-tool	600000	11
24	5eb8e708-86e6-4650-b5ed-1614f1cbfc0e	CNCF - Kyverno: e2e tests and CLI tests to cover sample policies	{"Description: Kyverno is a Kubernetes native policy engine that secures and automates Kubernetes configurations. This project will create automated test cases for the samples policies which are missing, and automating execution of tests.The enhancement will involve adding more unit","E2E tests."}	Description: Kyverno is a Kubernetes native policy engine that secures and automates Kubernetes configurations. This project will create automated test cases for the samples policies which are missing, and automating execution of tests.The enhancement will involve adding more unit/E2E tests.	{kubernetes,go,testing,automation}	2022	Term 1	https://github.com/kyverno/kyverno/issues/3121	https://kyverno.io	300000	17
434	0dd36ed5-4c92-4fb3-b809-bb614261a199	CNCF - Linkerd: Linkerd Dashboard Improvements	{"Description: Improve the Linkerd web dashboard with improved topology visualization, support for Linkerd conformance to the Gateway API project, and improved multi-cluster support.\n\nExpected Outcome: A period of focused investment in the Linkerd viz dashboard experience will greatly improve the experience for Linkerd users.\n\nUpstream Issue: https:","",github.com,linkerd,linkerd2,issues,"7865, https:","",github.com,linkerd,linkerd2,issues,"9243, https:","",github.com,linkerd,linkerd2,issues,9554}	Description: Improve the Linkerd web dashboard with improved topology visualization, support for Linkerd conformance to the Gateway API project, and improved multi-cluster support.\n\nExpected Outcome: A period of focused investment in the Linkerd viz dashboard experience will greatly improve the experience for Linkerd users.\n\nUpstream Issue: https://github.com/linkerd/linkerd2/issues/7865, https://github.com/linkerd/linkerd2/issues/9243, https://github.com/linkerd/linkerd2/issues/9554	{javascript,react,kubernetes}	2023	Term 1	https://github.com/linkerd/linkerd2/issues/7865	https://linkerd.io	0	61
74	2145d0aa-4812-4c6b-9108-42d64deea738	Linux kernel PCI project	{"The Linux PCI subsystem includes core code as well as several PCI controller drivers.  This project is to improve both the core code and the drivers to make them more maintainable. Applicants will work on these improvements and gain a much deeper knowledge of the PCI subsystem.\n\nController drivers\nThe Linux PCI subsystem includes 50+ drivers that operate PCI or PCI Express controllers. The controller drivers do similar things, but they have to accommodate the details of a specific device.  This project is to make these drivers more maintainable by making them more consistent in style and structure.  This will be done by many small, independent changes.\n\nError handling\nThe Linux PCI subsystem logs errors reported by PCI devices, such as corrected parity errors, transactions aborted by access control violations, errors caused by defective devices, and errors caused by incorrect configuration by the kernel. This project is to log additional information to make error diagnosis easier and to fix several long-standing defects in this area.  The defects range from fairly simple to quite difficult.\n\nASPM\nASPM is a power-saving feature of PCI Express.  The Linux code that configures this feature is complicated and error-prone. This project is to make this code simpler and easier to maintain.  This will  be done by several incremental changes."}	The Linux PCI subsystem includes core code as well as several PCI controller drivers.  This project is to improve both the core code and the drivers to make them more maintainable. Applicants will work on these improvements and gain a much deeper knowledge of the PCI subsystem.\n\nController drivers\nThe Linux PCI subsystem includes 50+ drivers that operate PCI or PCI Express controllers. The controller drivers do similar things, but they have to accommodate the details of a specific device.  This project is to make these drivers more maintainable by making them more consistent in style and structure.  This will be done by many small, independent changes.\n\nError handling\nThe Linux PCI subsystem logs errors reported by PCI devices, such as corrected parity errors, transactions aborted by access control violations, errors caused by defective devices, and errors caused by incorrect configuration by the kernel. This project is to log additional information to make error diagnosis easier and to fix several long-standing defects in this area.  The defects range from fairly simple to quite difficult.\n\nASPM\nASPM is a power-saving feature of PCI Express.  The Linux code that configures this feature is complicated and error-prone. This project is to make this code simpler and easier to maintain.  This will  be done by several incremental changes.	{c,shell,kernel}	2021	Term 3	https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/	https://wiki.linuxfoundation.org/lkmp	300000	21
22	540036e7-65de-48fe-a958-ccda48374030	LF Networking OPNFV - Log-Analysis and Alerting for OPNFV-VSPERF	{"OPNFV VSPERF already includes comprehensive Monitoring solution. However, it lacks the Log-Analysis and Alert-Management solution. This project aims to include log and alert management solutions based on opensources - ELK Stack. This will enhance the ability of VSPERF to:\n\n1. Perform post-test correlation","causation analysis.\n2. Adapt the Test-Traffic patterns based on the Alerts.\n\nThe first aim is to create custom kibana dashboard analyzing and synthesizing VSPERF logs","events. For every test-run, VSPERF generates following logs:\n\n1. vsperf-overall\n2. vswitchd\n3. trafficgen\n4. host-cmds\n5. guest-cmds\n\nThese logs have useful information that can help user to understand the test behavior, and analyze the performance results.\n\nVSPERF will have Alerting from the Monitoring component, and lacks alerting from the logs. As the next step, this project aims to use the same information - The logs - and create alerts than will be notified to VSPERF. "}	OPNFV VSPERF already includes comprehensive Monitoring solution. However, it lacks the Log-Analysis and Alert-Management solution. This project aims to include log and alert management solutions based on opensources - ELK Stack. This will enhance the ability of VSPERF to:\n\n1. Perform post-test correlation/causation analysis.\n2. Adapt the Test-Traffic patterns based on the Alerts.\n\nThe first aim is to create custom kibana dashboard analyzing and synthesizing VSPERF logs/events. For every test-run, VSPERF generates following logs:\n\n1. vsperf-overall\n2. vswitchd\n3. trafficgen\n4. host-cmds\n5. guest-cmds\n\nThese logs have useful information that can help user to understand the test behavior, and analyze the performance results.\n\nVSPERF will have Alerting from the Monitoring component, and lacks alerting from the logs. As the next step, this project aims to use the same information - The logs - and create alerts than will be notified to VSPERF.	{python,yaml,dashboard}	2020	Term 2	https://git.opnfv.org/	https://wiki.lfnetworking.org/display/LN/Log-Analysis+and+Alerting+for+OPNFV-VSPERF	600000	161
28	187dd140-0a0b-44f0-b5b8-23910692c1cf	CNCF - Tremor: Continuous benchmarking and benchmarking infrastructure	{"Set up infrastructure for running Tremor benchmarks periodically"}	Set up infrastructure for running Tremor benchmarks periodically	{rust}	2021	Term 1	https://github.com/tremor-rs/tremor-runtime/issues/722	https://www.tremor.rs/	300000	20
32	e5f6b5ba-2dd0-4a9f-a959-4a2ccc754f81	Program to test errors- TEST_ DO NOT APPLY	{"test program"}	test program	{}	2023	Term 2	www.example.com		0	24
30	115b9e43-9b50-4342-906a-785407267280	Pasta: Metrics on Linux Kernel Patches, Code Review and Quality	{"The PaStA tool, developed by Ralf Ramsauer, relates patch emails sent on the mailing lists to commits in the git repository for the Linux kernel development process. This allows to analyse the development process on the mailing list, measure some interesting metrics on the development, and identify outliers with respect to some central properties.\n\nThe issue tracker, https:","",github.com,lfd,PaStA,"issues, provides a good overview of possible mentee projects related to PaStA. Start to look into those issues, tasks and goals from that issue tracker and discuss a specific task to pick that can be handled within the timeframe of the mentorship project. Write together with your mentor a project proposal where you breakdown the goals stated in the issue tracker to a more detailed plan of activities, needed extensions of PaStA and the implementation tasks."}	The PaStA tool, developed by Ralf Ramsauer, relates patch emails sent on the mailing lists to commits in the git repository for the Linux kernel development process. This allows to analyse the development process on the mailing list, measure some interesting metrics on the development, and identify outliers with respect to some central properties.\n\nThe issue tracker, https://github.com/lfd/PaStA/issues, provides a good overview of possible mentee projects related to PaStA. Start to look into those issues, tasks and goals from that issue tracker and discuss a specific task to pick that can be handled within the timeframe of the mentorship project. Write together with your mentor a project proposal where you breakdown the goals stated in the issue tracker to a more detailed plan of activities, needed extensions of PaStA and the implementation tasks.	{python,r,statistics}	2020	Term 3	https://github.com/lfd/PaStA	https://github.com/lfd/PaStA/issues	0	21
77	f1275c0e-7152-4e09-8d8b-6b14598afbc3	OpenTelemetry	{"OpenTelemetry is made up of an integrated set of APIs and libraries as well as a collection mechanism via an agent and collector. These components are used to generate, collect, and describe telemetry about distributed systems. This data includes basic context propagation, distributed traces, metrics, and other signals in the future. OpenTelemetry is designed to make it easy to get critical telemetry data out of your services and into your backend(s) of choice. For each supported language it offers a single set of APIs, libraries, and data specifications, and developers can take advantage of whichever components they see fit."}	OpenTelemetry is made up of an integrated set of APIs and libraries as well as a collection mechanism via an agent and collector. These components are used to generate, collect, and describe telemetry about distributed systems. This data includes basic context propagation, distributed traces, metrics, and other signals in the future. OpenTelemetry is designed to make it easy to get critical telemetry data out of your services and into your backend(s) of choice. For each supported language it offers a single set of APIs, libraries, and data specifications, and developers can take advantage of whichever components they see fit.	{go,html,css}	2020	Term 1	https://github.com/open-telemetry	https://opentelemetry.io/	1451000	47
439	7f633ade-64f5-477c-bcbe-7b6693329c63	CNCF - ORAS: Develop ORAS Website	{"Description: [ORAS](https:","",oras.land,") is a tool for working with OCI artifacts and OCI registries. It allows users to distribute OCI artifacts across OCI Registries. ORAS only has a documentation site so far, the project goal is to develop a new website using Hugo framework based on the Figma layout design.\n\nExpected Outcome: Develop a new website using the [Hugo framework](https:","",gohugo.io,") based on the Figma layout design. It will replace the existing [ORAS documentation website](https:","",oras.land,") and provide a better user experience with interactive design."}	Description: [ORAS](https://oras.land/) is a tool for working with OCI artifacts and OCI registries. It allows users to distribute OCI artifacts across OCI Registries. ORAS only has a documentation site so far, the project goal is to develop a new website using Hugo framework based on the Figma layout design.\n\nExpected Outcome: Develop a new website using the [Hugo framework](https://gohugo.io/) based on the Figma layout design. It will replace the existing [ORAS documentation website](https://oras.land/) and provide a better user experience with interactive design.	{html,javascript,css,hugo}	2023	Term 1	https://github.com/oras-project/oras-www/issues/82	https://oras.land	300000	156
115	fa5b2116-3ce5-476f-b1c5-4084159eae1a	Hyperledger Fabric -Ease Endorsement Policies with Hyperledger Fabric Based Products	{"Introduction\n\nEndorsement policy controls the approval of a proposal to be executed at various granular levels.\nIn other words, this is the primary entity which is validated before a transaction is committed. Endorsement policy can operate at two different granularities:\n\nNamespace level: They can be set for the whole namespace.\nThis is the default approach in which endorsement policies are specified in the chaincode definition, which is agreed to by channel members and then committed to a channel\n\nIndividual key level: They can be set for individual state keys called private data collections. You can also specify an endorsement policy at the private data collection level, which would override the chaincode level endorsement policy for any keys in the private data collection. This would further restrict which organizations can write to a private data collection.\n\nProblem Statement\n\nThis becomes complex to handle when operating at scale.\n\nSome use cases\n\n1. Complex endorsement policy resolving for namespace level ones\n2. Tracking Endorsement policies on large quantity Private data collections each having different endorsement policies\n3. Resolving policy for a given object combining namespace level with overriding individual key-level ones."}	Introduction\n\nEndorsement policy controls the approval of a proposal to be executed at various granular levels.\nIn other words, this is the primary entity which is validated before a transaction is committed. Endorsement policy can operate at two different granularities:\n\nNamespace level: They can be set for the whole namespace.\nThis is the default approach in which endorsement policies are specified in the chaincode definition, which is agreed to by channel members and then committed to a channel\n\nIndividual key level: They can be set for individual state keys called private data collections. You can also specify an endorsement policy at the private data collection level, which would override the chaincode level endorsement policy for any keys in the private data collection. This would further restrict which organizations can write to a private data collection.\n\nProblem Statement\n\nThis becomes complex to handle when operating at scale.\n\nSome use cases\n\n1. Complex endorsement policy resolving for namespace level ones\n2. Tracking Endorsement policies on large quantity Private data collections each having different endorsement policies\n3. Resolving policy for a given object combining namespace level with overriding individual key-level ones.	{golang,java,bash,docker}	2020	Term 2	https://github.com/hyperledger/fabric	https://wiki.hyperledger.org/pages/viewpage.action?pageId=29035323	300000	110
37	3fee59ac-7716-4ad5-9282-ea63ccffea9b	CNCF - Thanos: Enhanced Block Viewer UI	{"The Thanos BlockViewer UI has proven to be an essential part of the debuggability story for the Thanos project. It allows administrators to see the exact state of data in Object Storage in a provider-agnostic way. This project is about extending this UI with richer features, context, and actions to improve observability and increase control."}	The Thanos BlockViewer UI has proven to be an essential part of the debuggability story for the Thanos project. It allows administrators to see the exact state of data in Object Storage in a provider-agnostic way. This project is about extending this UI with richer features, context, and actions to improve observability and increase control.	{react,typescript,go}	2021	Term 2	https://github.com/thanos-io/thanos/issues/3112	https://thanos.io/	300000	26
382	7a13b009-0365-4910-8fbf-9088294870fd	CNCF - Thanos: Receive Support for tenant-specific external labels	{"Tenants in Thanos Receivers currently get one external label which indicates their tenant ID. We would like to implement attaching arbitrary external labels to each Thanos Tenant. This functionality is useful for various different use cases, such as improving performance when querying data for tenants which share the same labels."}	Tenants in Thanos Receivers currently get one external label which indicates their tenant ID. We would like to implement attaching arbitrary external labels to each Thanos Tenant. This functionality is useful for various different use cases, such as improving performance when querying data for tenants which share the same labels.	{go}	2022	Term 3	https://github.com/thanos-io/thanos/issues/5434	https://thanos.io/	300000	26
43	ebaff150-3c69-4d3e-8ac1-5268b5d4672e	Hyperledger - making chaincode fault tolerant software	{"The proposed project includes the following key activities:\n\nA systematic review of the applicability of fault-tolerant software patterns to Hyperledger Fabric chaincode as the potentially faulty software\nCreation of programming idioms for applying the applicable patterns to Hyperledger Fabric chaincode\nPrototyping \\"fault tolerance gadgets\\" (libraries) for easy application to chaincode\nThe book \\"Patterns for Fault Tolerant Software\\" from R. Hanmer (Wiley, 2013) is a good reference of the patterns that the project will target.\n\nThis is a research-focused project which is relatively light on programming. At the same time, a rather deep dive into the workings of Hyperledger Fabric is expected."}	The proposed project includes the following key activities:\n\nA systematic review of the applicability of fault-tolerant software patterns to Hyperledger Fabric chaincode as the potentially faulty software\nCreation of programming idioms for applying the applicable patterns to Hyperledger Fabric chaincode\nPrototyping "fault tolerance gadgets" (libraries) for easy application to chaincode\nThe book "Patterns for Fault Tolerant Software" from R. Hanmer (Wiley, 2013) is a good reference of the patterns that the project will target.\n\nThis is a research-focused project which is relatively light on programming. At the same time, a rather deep dive into the workings of Hyperledger Fabric is expected.	{}	2022	Term 2	https://github.com/hyperledger/fabric	https://wiki.hyperledger.org/display/INTERN/Making+chaincode+fault+tolerant+software	600000	1
140	7c510003-5f52-45c2-b5de-b6664851d3de	CNCF - Kubernetes SIG Architecture: Develop tools for evaluating dependency updates to Kubernetes	{"Implement command line utilities that can help Kubernetes developers evaluate new dependencies by capturing statistics","metrics and estimating cost of adding something new. This will involve diving deep into golang dependency chains (transitive","shared dependencies) and coming up with new metrics to estimate how burdensome something new can be or how much we will save by getting rid of something so we can prioritize work and get more efficient from a developer workflow perspective."}	Implement command line utilities that can help Kubernetes developers evaluate new dependencies by capturing statistics/metrics and estimating cost of adding something new. This will involve diving deep into golang dependency chains (transitive/shared dependencies) and coming up with new metrics to estimate how burdensome something new can be or how much we will save by getting rid of something so we can prioritize work and get more efficient from a developer workflow perspective.	{go,cli}	2021	Term 1	https://github.com/kubernetes/kubernetes/issues/98698	kubernetes.io	300000	75
75	3918e3c7-c94e-4ff6-86cf-75affba454a1	Open Service Mesh	{"Open Service Mesh (OSM) is a lightweight, extensible, cloud native service mesh that allows users to uniformly manage, secure, and get out-of-the-box observability features for highly dynamic microservice environments."}	Open Service Mesh (OSM) is a lightweight, extensible, cloud native service mesh that allows users to uniformly manage, secure, and get out-of-the-box observability features for highly dynamic microservice environments.	{go,rust}	2021	Term 1	https://github.com/openservicemesh/osm	https://openservicemesh.io/	300000	45
60	b5b25fd8-5d68-4191-907a-b4802fb13e49	Zowe - Metrics Dashboard for ML services	{"A single dashboard that clearly shows individual services as well as aggregate system information, for transport-level information like request rates, error rates, etc., and lower-level information like CPU usage. This dashboard should have interactions available to fine-tune what metrics are shown."}	A single dashboard that clearly shows individual services as well as aggregate system information, for transport-level information like request rates, error rates, etc., and lower-level information like CPU usage. This dashboard should have interactions available to fine-tune what metrics are shown.	{}	2021	Term 2	https://github.com/zowe/api-layer	https://zowe.org	300000	38
47	b6b2b30f-1663-4902-a6a5-1c3ebd927a38	CNCF - Kubernetes Policy WG: KubeArmor support for Policy Report CRD	{"This project will periodically generate or update a [Policy Report Custom Resource (CR)](https:","",github.com,kubernetes-sigs,wg-policy-prototypes,blob,master,policy-report,"README.md) based on events collected from KubeArmor. This could be implemented as a feature in KubeArmor or developed as an external adapter. The candidate will learn about Kubernetes controllers and various security topics."}	This project will periodically generate or update a [Policy Report Custom Resource (CR)](https://github.com/kubernetes-sigs/wg-policy-prototypes/blob/master/policy-report/README.md) based on events collected from KubeArmor. This could be implemented as a feature in KubeArmor or developed as an external adapter. The candidate will learn about Kubernetes controllers and various security topics.	{linux,go,cli,kubernetes}	2021	Term 3	https://github.com/kubernetes-sigs/wg-policy-prototypes/issues/59	https://github.com/kubernetes-sigs/wg-policy-prototypes/issues/59	300000	75
50	e01ba557-af88-4716-8efe-398ce31a6fb2	CNCF - Kyverno: Monitor Kyverno with Prometheus	{"Publish Kyverno policy execution metrics to Prometheus and Grafana"}	Publish Kyverno policy execution metrics to Prometheus and Grafana	{go,prometheus}	2021	Term 1	https://github.com/kyverno/kyverno/issues/256	https://kyverno.io/	300000	17
51	e2d82cb0-f150-4a25-b96d-8fd4d255fd96	CNCF - Kyverno: Security enhancements	{"Description: Kyverno is a Kubernetes native policy engine that secures and automates Kubernetes configurations. This project improves security posture and processes for Kyverno. Improve OSSF Security Scorecard results, define security processes, and add best practice processes like publishing signed images and build attestations for SLSA compliance."}	Description: Kyverno is a Kubernetes native policy engine that secures and automates Kubernetes configurations. This project improves security posture and processes for Kyverno. Improve OSSF Security Scorecard results, define security processes, and add best practice processes like publishing signed images and build attestations for SLSA compliance.	{security}	2022	Term 1	https://github.com/kyverno/kyverno/issues/2250	https://kyverno.io	300000	17
443	5d331c88-fc2d-4635-a92c-5d25fb42f47d	CNCF - ORAS: Develop .NET SDK for ORAS	{"Description: [ORAS](https:","",oras.land,") is a tool for working with OCI artifacts and OCI registries. It allows users to distribute OCI artifacts across OCI Registries. Users seeking a generic registry client can benefit from the ORAS CLI, while developers can build their own clients on top of one of the ORAS client libraries. ORAS has Python and Golang SDK that allow developers to build their own clients on top of one of the library. Similarly, developing a .NET SDK will enable .Net developers to use ORAS API and enhance the ORAS ecosystem. \n\nExpected Outcome: Develop a .NET SDK in a new repository and write the examples and API document on GoDoc. Write unit test for this SDK and make sure the testing coverage is qualified."}	Description: [ORAS](https://oras.land/) is a tool for working with OCI artifacts and OCI registries. It allows users to distribute OCI artifacts across OCI Registries. Users seeking a generic registry client can benefit from the ORAS CLI, while developers can build their own clients on top of one of the ORAS client libraries. ORAS has Python and Golang SDK that allow developers to build their own clients on top of one of the library. Similarly, developing a .NET SDK will enable .Net developers to use ORAS API and enhance the ORAS ecosystem. \n\nExpected Outcome: Develop a .NET SDK in a new repository and write the examples and API document on GoDoc. Write unit test for this SDK and make sure the testing coverage is qualified.	{oras}	2023	Term 1	https://github.com/oras-project/oras/issues/774	https://oras.land	300000	156
48	2d95e888-ef43-4710-b505-56b3682d5741	CNCF - WasmEdge: Support WASI-Crypto proposal	{"After WasmEdge provides an experimental API, WASI Socket, for supporting Berkeley Sockets API in Wasm. WasmEdge enabled a new way to open a new socket, listen to an existed socket, and send and receive data. Moreover, it will be nice if we can do more things in the related features such as SSL support. To achieve this feature, one possible way is to compile the OpenSSL library to Wasm and link it as a library. However, the performance may be not good, because all the computation jobs are done at the wasm level. Here is an alternative way, instead of the previous one, we can wrap the OpenSSL library to Wasm external functions. For example, binding `ssl_connect` to `(import \\"openssl\\" \\"ssl_connect\\" ... )`. Unfortunately, this is not an easy way to do it. To simply the workload, we decide to implement the WASI-crypto proposal first, and then use this proposal to make the above things happen."}	After WasmEdge provides an experimental API, WASI Socket, for supporting Berkeley Sockets API in Wasm. WasmEdge enabled a new way to open a new socket, listen to an existed socket, and send and receive data. Moreover, it will be nice if we can do more things in the related features such as SSL support. To achieve this feature, one possible way is to compile the OpenSSL library to Wasm and link it as a library. However, the performance may be not good, because all the computation jobs are done at the wasm level. Here is an alternative way, instead of the previous one, we can wrap the OpenSSL library to Wasm external functions. For example, binding `ssl_connect` to `(import "openssl" "ssl_connect" ... )`. Unfortunately, this is not an easy way to do it. To simply the workload, we decide to implement the WASI-crypto proposal first, and then use this proposal to make the above things happen.	{rust}	2021	Term 3	https://github.com/WasmEdge/WasmEdge/issues/345	https://wasmedge.org/	300500	31
389	0e2a5766-9556-4bd6-b3bb-3a7802342485	Hyperledger - Technical Deep Dive Workshop Content Creation for Hyperledger Cactus	{"We are in the inception of interoperability, and Hyperledger Cactus plays a central role. In this internship, you will learn about blockchain interoperability and Hyperledger Cactus.\n\nThe project is about two main goals:\n\n1) becoming knowledgeable in interoperability and an expert in Hyperledger Cactus\n\n2) producing content for a ~4 hour-long interoperability workshop that the Cactus maintainers are planning on creating sometime in the fall of 2022.\n\nApart from creating content, the interns are encouraged to also contribute to Cactus' documentation and fix bugs that may come up during the production of the content itself. An example for the latter would be when the intern is trying to put together a tutorial for something and it turns out that the feature being demonstrated has a problem with it that needs fixing before the content can be presented.\n\nThe opportunity is two-fold:\n\n1) Make valuable contributions to the Hyperleger Cactus codebase - experiencing the OSS workflow end to end\n2) Have the opportunity to present","demonstrate certain parts of the framework in the workshop itself (if the intern is so inclined - this is definitely what the mentors will recommend they do of course)"}	We are in the inception of interoperability, and Hyperledger Cactus plays a central role. In this internship, you will learn about blockchain interoperability and Hyperledger Cactus.\n\nThe project is about two main goals:\n\n1) becoming knowledgeable in interoperability and an expert in Hyperledger Cactus\n\n2) producing content for a ~4 hour-long interoperability workshop that the Cactus maintainers are planning on creating sometime in the fall of 2022.\n\nApart from creating content, the interns are encouraged to also contribute to Cactus' documentation and fix bugs that may come up during the production of the content itself. An example for the latter would be when the intern is trying to put together a tutorial for something and it turns out that the feature being demonstrated has a problem with it that needs fixing before the content can be presented.\n\nThe opportunity is two-fold:\n\n1) Make valuable contributions to the Hyperleger Cactus codebase - experiencing the OSS workflow end to end\n2) Have the opportunity to present/demonstrate certain parts of the framework in the workshop itself (if the intern is so inclined - this is definitely what the mentors will recommend they do of course)	{git,docker,typescript,http,vscode}	2022	Term 2	https://github.com/hyperledger/cactus	https://wiki.hyperledger.org/display/INTERN/Technical+Deep+Dive+Workshop+Content+Creation+for+Hyperledger+Cactus	420000	1
62	de52f04f-f948-41ca-a866-1eaf51c84dcc	Klever: Integrate kernel development tools with Klever's Web UI	{"The Linux kernel development uses a number of tools, checkpatch.pl, coccinelle scripts, sparse, etc. and these tools report certain findings. While the valid ones are addressed by the kernel developers, the invalid tool findings are manually assessed and not acted upon. Over time with addressing the valid findings, the proportion of invalid findings increase compared to newly appearing valid findings, as invalid findings of those tools are not marked and tracked over the various versions.\nKlever provides a Web UI to track false positive findings from static analysis tool. Extend Klever to allow to import findings from checkpatch.pl, coccinelle scripts, sparse, etc."}	The Linux kernel development uses a number of tools, checkpatch.pl, coccinelle scripts, sparse, etc. and these tools report certain findings. While the valid ones are addressed by the kernel developers, the invalid tool findings are manually assessed and not acted upon. Over time with addressing the valid findings, the proportion of invalid findings increase compared to newly appearing valid findings, as invalid findings of those tools are not marked and tracked over the various versions.\nKlever provides a Web UI to track false positive findings from static analysis tool. Extend Klever to allow to import findings from checkpatch.pl, coccinelle scripts, sparse, etc.	{python,linux,kernel,c}	2020	Term 3	https://github.com/ldv-klever/klever	https://forge.ispras.ru/projects/klever	0	21
59	ed8b4d53-115d-41b6-951d-6f72bad7884d	Open Mainframe- Create Static TypeScript Types for Each RMF Report	{"The ZEBRA project is moving to TypeScript from Javascript and would like to enhance the developer experience. With static types of RMF reports, developers can take advantage of intellisense from IDEs like VSCode. They also increase the maintainability of the codebase. We are looking for someone to help go through the RMF Programmer's Guide documentation and help map out the structure of RMF reports and come up with optimized TypeScript types and interfaces."}	The ZEBRA project is moving to TypeScript from Javascript and would like to enhance the developer experience. With static types of RMF reports, developers can take advantage of intellisense from IDEs like VSCode. They also increase the maintainability of the codebase. We are looking for someone to help go through the RMF Programmer's Guide documentation and help map out the structure of RMF reports and come up with optimized TypeScript types and interfaces.	{typescript,javascript,mainframe}	2022	Term 2	https://github.com/openmainframeproject-internship		300000	38
58	a138783e-85b8-425d-ba96-e72d7c9258c8	Open Mainframe- Zowe JavaSDK Continued Development	{"Java SDK is 75% baked. Continue development lets add USS APIs."}	Java SDK is 75% baked. Continue development lets add USS APIs.	{java}	2022	Term 2	https://github.com/openmainframeproject-internship		0	38
64	872be524-7465-4639-be88-1b451c581826	Envoy	{"Envoy is an open source edge and service proxy, designed for cloud-native applications."}	Envoy is an open source edge and service proxy, designed for cloud-native applications.	{}	2020	Term 2	https://github.com/envoyproxy/envoy	https://www.envoyproxy.io/	2550000	40
67	0f038ae3-a36b-4f44-a2d5-c6a94e23a102	FD.io Internship	{"FD.io is accepting student interns during the summer of 2019 to work in the FD.io development community. Each intern will work closely with a mentor for the duration of his","her internship. For a list of suggested intern development projects, please see our Project Ideas Page located here: Project Proposals. Internship development projects do not need to be limited to ideas on this wiki page. If you have a great idea for a development activity for FD.io, you can submit the idea as part of your application and, if chosen, the FD.io Internship Administrator will find a suitable mentor for you."}	FD.io is accepting student interns during the summer of 2019 to work in the FD.io development community. Each intern will work closely with a mentor for the duration of his/her internship. For a list of suggested intern development projects, please see our Project Ideas Page located here: Project Proposals. Internship development projects do not need to be limited to ideas on this wiki page. If you have a great idea for a development activity for FD.io, you can submit the idea as part of your application and, if chosen, the FD.io Internship Administrator will find a suitable mentor for you.	{}	2019	Term 2	https://github.com/fdio		3200000	42
76	40a443f9-cb78-49e6-96ad-26616acb2113	OpenEBS	{"Leading Open Source Container Attached Storage, built using Cloud Native Architecture, simplifies running Stateful Applications on Kubernetes."}	Leading Open Source Container Attached Storage, built using Cloud Native Architecture, simplifies running Stateful Applications on Kubernetes.	{kubernetes,docker,go}	2020	Term 2	https://github.com/openebs/openebs	https://openebs.io/	1400000	46
63	deb57f5a-491f-4b50-8d1a-e0b9afa15d57	Open Mainframe- Zowe Python SDK Enhancements for Zowe v2	{"There is growing interest in the Zowe Python SDK! In this project, the mentee will make various enhancements to this SDK, many of which are listed here -  https:","",github.com,zowe,zowe-client-python-sdk,"issues. This work will include enabling the Python SDK to leverage Zowe team config - a major enhancement in Zowe V2 LTS. This work will be significant in moving the Python SDK toward a Long Term Support Release. We envision the Zowe Python SDK to be used in Python automation scripts, testing frameworks, and applications."}	There is growing interest in the Zowe Python SDK! In this project, the mentee will make various enhancements to this SDK, many of which are listed here -  https://github.com/zowe/zowe-client-python-sdk/issues. This work will include enabling the Python SDK to leverage Zowe team config - a major enhancement in Zowe V2 LTS. This work will be significant in moving the Python SDK toward a Long Term Support Release. We envision the Zowe Python SDK to be used in Python automation scripts, testing frameworks, and applications.	{mainframe,developer}	2022	Term 1	https://github.com/zowe/zowe-client-python-sdk		600000	38
66	3f8e0628-c0ff-43b0-8b80-93e35d1f5c37	Zowe Explorer extension templates	{"The sample VS Code extension should satisfy a generic mainframe use case (search, compare, etc ) and be hosted on github.com","zowe. The extension should demonstrate how to call directly to a backend service and extend Zowe Explorer via a new tree or menu options."}	The sample VS Code extension should satisfy a generic mainframe use case (search, compare, etc ) and be hosted on github.com/zowe. The extension should demonstrate how to call directly to a backend service and extend Zowe Explorer via a new tree or menu options.	{json}	2021	Term 2	https://github.com/zowe/vscode-extension-for-zowe	https://zowe.org	300000	38
68	d323ffd3-161b-4517-a5a6-c69d570e3f95	Zowe toolkit plugin for IntelliJ	{"Here in we propose to develop a set of Zowe plugins for the IntelliJ family of development environments. This will ensure uniform user experience, while adding the possibility to work in their favorite development environment. The plugins will share a common foundation to interface with the IntelliJ platform SDK and to access the Zowe resources through Zowe API ML."}	Here in we propose to develop a set of Zowe plugins for the IntelliJ family of development environments. This will ensure uniform user experience, while adding the possibility to work in their favorite development environment. The plugins will share a common foundation to interface with the IntelliJ platform SDK and to access the Zowe resources through Zowe API ML.	{json}	2021	Term 2	https://github.com/zowe	https://zowe.org	360000	38
70	3f769e95-ee62-47ec-b326-a55304f44853	Slack to Zowe integration	{"Allow a user to create a slack channel to their mainframe and ask questions, e.g. \\"Is my job finished\\" or \\"Is task ABCD still running\\", ... Slack communicates using Zowe CLI commands and a text to command language parser creates the calls, e.g. Watson AI assistant"}	Allow a user to create a slack channel to their mainframe and ask questions, e.g. "Is my job finished" or "Is task ABCD still running", ... Slack communicates using Zowe CLI commands and a text to command language parser creates the calls, e.g. Watson AI assistant	{}	2021	Term 2	https://github.com/zowe	https://zowe.org	300000	38
72	bce45251-1ff4-4131-9699-0a0017b31495	Jenkins	{"Jenkins is an open source automation server. It provides hundreds of plugins to support building, testing, deploying and automating virtually any project at any scale. Your donations help to keep the project going and to accelerate its evolution. More information about how we use donations: https:","",jenkins.io,donate,""}	Jenkins is an open source automation server. It provides hundreds of plugins to support building, testing, deploying and automating virtually any project at any scale. Your donations help to keep the project going and to accelerate its evolution. More information about how we use donations: https://jenkins.io/donate/	{}	2019	Term 2	https://github.com/jenkinsci/	https://jenkins.io/	1340900	44
457	df011bb8-8ce1-4092-bfc6-1e92ce40a17d	CNCF - CNCF Landscape: UX UI improvement	{"In an effort to better the user experience, the CNCF Landscape is actively seeking ways to improve and enhance its features.\n- The aim is for the mentee to carry out a User Research to validate existing user personas, gain a deeper understanding of user needs, and conduct a thorough heuristic evaluation to gain insights into user experiences. Using the results, the mentee will establish a solid foundation to start an iterative process of ideation, prototyping, and testing possible solutions. The ultimate goal is to initiate a continuous cycle of improvement and further development of features that enhance the user experience of the CNCF Landscape.\n\nRecommended skills: Design Thinking, UX research methodology. \n\nIn this stage of the project, we are seeking candidates with a background and","or training in user research. Supporting materials, such as the following recommended deliverables, that demonstrate your understanding and experience in this area are ideal:\n\n1. Proto-Personas\n2. Validated Personas with Supporting Findings\n3. Brief Explanation of the Difference between Proto-Personas and Validated Personas\n4. List of UX Research Techniques for Kickstarting the Discovery of Landscape Users\n5. Figma and Visual Design are a plus."}	In an effort to better the user experience, the CNCF Landscape is actively seeking ways to improve and enhance its features.\n- The aim is for the mentee to carry out a User Research to validate existing user personas, gain a deeper understanding of user needs, and conduct a thorough heuristic evaluation to gain insights into user experiences. Using the results, the mentee will establish a solid foundation to start an iterative process of ideation, prototyping, and testing possible solutions. The ultimate goal is to initiate a continuous cycle of improvement and further development of features that enhance the user experience of the CNCF Landscape.\n\nRecommended skills: Design Thinking, UX research methodology. \n\nIn this stage of the project, we are seeking candidates with a background and/or training in user research. Supporting materials, such as the following recommended deliverables, that demonstrate your understanding and experience in this area are ideal:\n\n1. Proto-Personas\n2. Validated Personas with Supporting Findings\n3. Brief Explanation of the Difference between Proto-Personas and Validated Personas\n4. List of UX Research Techniques for Kickstarting the Discovery of Landscape Users\n5. Figma and Visual Design are a plus.	{}	2023	Term 1	https://github.com/cncf/landscape/issues/2467	https://landscape.cncf.io	300000	160
85	e8362c9e-dd79-47d7-b0ec-c1f7a2a666f4	Open Daylight Internship	{"OpenDaylight is accepting 5 student interns for Data-driven cloud infrastructure cost reduction project.\n\nOpenDaylight runs massive Continuous Integration Testing infrastructure using tools like Jenkins and a public cloud provider. OpenDaylight's 2019 budget requires reducing our cloud infrastructure costs by hundreds of thousands of dollars, or by something like 20% of the current usage. To achieve this ambitious goal, we want to leverage a data-driven approach to identify what jobs are most ripe for optimizations. The intern will do data analytics using OpenDaylight's Bitergia data (number of job runs, duration, VM flavor), in conjunction with cloud hosting costs (cost per VM flavor per unit time). They will then work with the relevant OpenDaylight job-owning projects and the ODL Testing","RelEng community to disable jobs, reduce job frequency, or reduce the size of job VMs."}	OpenDaylight is accepting 5 student interns for Data-driven cloud infrastructure cost reduction project.\n\nOpenDaylight runs massive Continuous Integration Testing infrastructure using tools like Jenkins and a public cloud provider. OpenDaylight's 2019 budget requires reducing our cloud infrastructure costs by hundreds of thousands of dollars, or by something like 20% of the current usage. To achieve this ambitious goal, we want to leverage a data-driven approach to identify what jobs are most ripe for optimizations. The intern will do data analytics using OpenDaylight's Bitergia data (number of job runs, duration, VM flavor), in conjunction with cloud hosting costs (cost per VM flavor per unit time). They will then work with the relevant OpenDaylight job-owning projects and the ODL Testing/RelEng community to disable jobs, reduce job frequency, or reduce the size of job VMs.	{jenkins,bitergia}	2019	Term 2	https://github.com/opendaylight	https://wiki.opendaylight.org/view/Interns/Projects#Infrastructure_Optimization	4001500	42
73	1f3216bc-e266-415c-84c7-a9a728dff3ce	Linux Kernel Mentorship	{"The program is targeted towards adding well trained and educated diverse talent to areas of the project that could benefit from extra help. The primary goal is building a strong, sustainable, and diverse kernel community with focus on improving security and quality of Linux."}	The program is targeted towards adding well trained and educated diverse talent to areas of the project that could benefit from extra help. The primary goal is building a strong, sustainable, and diverse kernel community with focus on improving security and quality of Linux.	{c,shell,kernel}	2019	Term 2	https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/	https://wiki.linuxfoundation.org/lkmp	0	21
78	3237a2e2-1265-4b87-b650-72b7c34a77b6	Polycephaly	{"Polycephaly is an open source project written in Groovy, which enables building z","OS source code using Jenkins and Git"}	Polycephaly is an open source project written in Groovy, which enables building z/OS source code using Jenkins and Git	{groovy,jenkins}	2021	Term 2	https://github.com/openmainframeproject/polycephaly	https://www.openmainframeproject.org/projects/polycephaly	300000	48
71	359dda52-7fb7-4fa8-82cd-a27216757a57	Service Mesh Interface	{"A standard interface for service meshes on Kubernetes."}	A standard interface for service meshes on Kubernetes.	{go,rust}	2020	Term 2	https://smi-spec.io/	https://github.com/servicemeshinterface/smi-spec	850000	43
83	4eb229ab-ebba-486b-a3c0-0b533871ef65	Jaeger Mentorship	{"Monitor and troubleshoot transactions in complex distributed systems"}	Monitor and troubleshoot transactions in complex distributed systems	{golang,javascript,java}	2019	Term 2	https://github.com/jaegertracing/jaeger		550000	178
90	ef31c8f2-5e68-4d76-bc96-399ba2b104a0	Open Mainframe Project 2020 Mentorship - Enhance zvm Prometheus exporter	{"Enhance z","vm Prometheus exporter, as https:","",github.com,zvmexporter,"zvm_exporter is pretty old and some stuffs like xcat is not maintained anymore, we propose to use Feilong as base to enhance Prometheus exporter and to provide more metrics to help ecosystem build up"}	Enhance z/vm Prometheus exporter, as https://github.com/zvmexporter/zvm_exporter is pretty old and some stuffs like xcat is not maintained anymore, we propose to use Feilong as base to enhance Prometheus exporter and to provide more metrics to help ecosystem build up	{java}	2020	Term 2	https://github.com/openmainframeproject-internship/Enhance-zvm-Prometheus-exporter	https://www.openmainframeproject.org/projects/mentorship-program	600000	2
91	2278d94c-c287-4417-a471-70601256e8d6	Open Mainframe Project 2020 Mentorship - Feilong - Ansible Module	{"Create sustainable open source Ansible modules for creating","managing virtual server instances and storage devices connected to IBM Z. The final target state products could be used to communicate to various storage devices connected to mainframe and control HW function to automate storage managements. Intermediate target state would be communicating to IBM DS8K CLIs and","or RestfulAPIs to help initialization of storage device for virtual servers."}	Create sustainable open source Ansible modules for creating/managing virtual server instances and storage devices connected to IBM Z. The final target state products could be used to communicate to various storage devices connected to mainframe and control HW function to automate storage managements. Intermediate target state would be communicating to IBM DS8K CLIs and/or RestfulAPIs to help initialization of storage device for virtual servers.	{ansible}	2020	Term 2	https://github.com/openmainframeproject-internship/Feilong---Ansible-Module	https://www.openmainframeproject.org/projects/mentorship-program	600000	2
93	24f4770b-3344-437d-beb6-77a0c6c47cd5	Open Mainframe Project 2020 Mentorship - Zowe App Framework - File Transfer Application	{"The Zowe Virtual Desktop needs to provide a way to easily transfer files and datasets from the mainframe to a user desktop and vice versa, while dealing with encoding and security."}	The Zowe Virtual Desktop needs to provide a way to easily transfer files and datasets from the mainframe to a user desktop and vice versa, while dealing with encoding and security.	{javascript,typescript}	2020	Term 2	https://github.com/openmainframeproject-internship/Zowe-App-Framework---File-Transfer-Application	https://www.openmainframeproject.org/projects/mentorship-program	600000	2
79	9595fbe7-6a8d-43d4-aebb-a54d57f33fdd	Prometheus	{"Prometheus is an open-source systems monitoring and alerting toolkit originally built at SoundCloud. Since its inception in 2012, many companies and organizations have adopted Prometheus, and the project has a very active developer and user community. It is now a standalone open source project and maintained independently of any company. To emphasize this, and to clarify the project's governance structure, Prometheus joined the Cloud Native Computing Foundation in 2016 as the second hosted project, after Kubernetes."}	Prometheus is an open-source systems monitoring and alerting toolkit originally built at SoundCloud. Since its inception in 2012, many companies and organizations have adopted Prometheus, and the project has a very active developer and user community. It is now a standalone open source project and maintained independently of any company. To emphasize this, and to clarify the project's governance structure, Prometheus joined the Cloud Native Computing Foundation in 2016 as the second hosted project, after Kubernetes.	{go}	2020	Term 1	https://github.com/prometheus/prometheus	https://prometheus.io/	1954000	49
81	c6a0326c-b053-41a3-9bf2-1e7e78481ca6	TiKV	{"Distributed transactional key-value database, originally created to complement TiDB."}	Distributed transactional key-value database, originally created to complement TiDB.	{rust,go,database}	2020	Term 2	https://github.com/tikv/tikv	https://tikv.org/	1700000	51
82	837a970d-64c3-46d1-ade2-5b8b8997a0d4	Volcano	{"A Kubernetes Native Batch System"}	A Kubernetes Native Batch System	{kubernetes,go}	2021	Term 1	https://github.com/volcano-sh/volcano	https://volcano.sh/	900000	34
96	ccf72716-13bb-45be-bfbf-df6997fbf5ef	Open Mainframe Project 2020 Mentorship - Zowe Parsing Engine for SMF or RMF PP Reports	{"Zowe is a great systems operations tool. One of the systems programmers or performance analyzer's job is to decode SMF","RMF reports to check system's health. Having a generic parser for SMF datasets and","or RMF(or CMF) reports for Zowe would open various opportunities to create","re-use many open-source monitoring tools out there. *"}	Zowe is a great systems operations tool. One of the systems programmers or performance analyzer's job is to decode SMF/RMF reports to check system's health. Having a generic parser for SMF datasets and/or RMF(or CMF) reports for Zowe would open various opportunities to create/re-use many open-source monitoring tools out there. *	{}	2020	Term 2	https://github.com/openmainframeproject-internship/Zowe-Parsing-Engine-for-SMF-or-RMF-PP-Reports	https://www.openmainframeproject.org/projects/mentorship-program	600000	2
97	5d5d4357-f340-47c9-9ff2-7b0536291576	Argo	{"Open source Kubernetes native workflows, events, CI and CD"}	Open source Kubernetes native workflows, events, CI and CD	{go,kubernetes}	2020	Term 2	https://github.com/argoproj	https://argoproj.github.io/	550000	58
99	de7ca1c2-2d22-4919-bef8-6cca50a54426	KubeVirt	{"Managing virtualization workloads on Kubernetes."}	Managing virtualization workloads on Kubernetes.	{go,kubernetes}	2020	Term 2	https://github.com/kubevirt	https://kubevirt.io/	1100000	60
100	65742dc0-7217-4c4a-a609-f5f0fcde5c0a	Linkerd	{"Ultralight, security-first service mesh for Kubernetes"}	Ultralight, security-first service mesh for Kubernetes	{go,kubernetes}	2020	Term 2	https://github.com/linkerd/linkerd2	https://linkerd.io/	550000	61
92	8fbb5b59-7466-4303-aa2f-fee1021340b9	Open Mainframe Project 2020 Mentorship - Zowe App Framework – App Generator	{"Creating applications for the Zowe Virtual Desktop requires a significant amount of programming that can be automated. The purpose of this project is to expand on an existing prototype to make stub application generation a breeze for new comers and future exploiters wanting to experiment with Zowe."}	Creating applications for the Zowe Virtual Desktop requires a significant amount of programming that can be automated. The purpose of this project is to expand on an existing prototype to make stub application generation a breeze for new comers and future exploiters wanting to experiment with Zowe.	{javascript,typescript}	2020	Term 2	https://github.com/openmainframeproject-internship/Zowe-App-Framework-App-Generator	https://www.openmainframeproject.org/projects/mentorship-program	600000	2
94	af12b527-5507-41bd-9636-3ee3bcce8846	Open Mainframe Project 2020 Mentorship - Zowe Desktop Application State Persistence Mechanism	{"Web plugins running within the application framework do not have access to a secure state persistence mechanism that would allow a user to restore one of more plugins to an early state when restarting a session. The App Framework needs to provide this capability."}	Web plugins running within the application framework do not have access to a secure state persistence mechanism that would allow a user to restore one of more plugins to an early state when restarting a session. The App Framework needs to provide this capability.	{javascript,typescript}	2020	Term 2	https://github.com/openmainframeproject-internship/Zowe-Desktop-Application-State-Persistence-Mechanism	https://www.openmainframeproject.org/projects/mentorship-program	600000	2
95	4850fcb8-1d20-4996-b519-03be0bf9ef4f	Open Mainframe Project 2020 Mentorship - Zowe Desktop Documentation Viewer	{"Plugins may have a variety of documentation that are shipped with them: readmes, licenses, guides, api documentation and more. As the various types of documentation generally resides in file formats that browsers can visualize, be it PDF or text or markdown, it would be convenient to present this documentation in the same place where the plugins run. A standard viewer & standard packaging scheme is needed so that plugin developers do not instead come up with their own way to solve the same problem, multiple times."}	Plugins may have a variety of documentation that are shipped with them: readmes, licenses, guides, api documentation and more. As the various types of documentation generally resides in file formats that browsers can visualize, be it PDF or text or markdown, it would be convenient to present this documentation in the same place where the plugins run. A standard viewer & standard packaging scheme is needed so that plugin developers do not instead come up with their own way to solve the same problem, multiple times.	{javascript,typescript,html}	2020	Term 2	https://github.com/openmainframeproject-internship/Zowe-Desktop-Documentation-Viewer	https://www.openmainframeproject.org/projects/mentorship-program	600000	2
392	fd61c1c4-1245-4f46-a942-d837333bcaaf	Hyperledger - Chia Connector for Hyperledger Cactus	{"Develop a Cactus Chia connector plugin + test infrastructure (container images, container manager class, etc.) from scratch.\nInclude end to end test cases powered by Jest and the said testing infrastructure that prove that the connector is working as intended with a locally simulated Chia network.\n\nYou will be creating a new package in the Cactus monorepo from scratch under the packages",cactus-plugin-ledger-connector-chia," path.\n\nAll the information presented here can also be found at the GitHub issue tracker of Hyperledger Cactus, filed under issue #1634 => github.com",hyperledger,cactus,issues,1634}	Develop a Cactus Chia connector plugin + test infrastructure (container images, container manager class, etc.) from scratch.\nInclude end to end test cases powered by Jest and the said testing infrastructure that prove that the connector is working as intended with a locally simulated Chia network.\n\nYou will be creating a new package in the Cactus monorepo from scratch under the packages/cactus-plugin-ledger-connector-chia/ path.\n\nAll the information presented here can also be found at the GitHub issue tracker of Hyperledger Cactus, filed under issue #1634 => github.com/hyperledger/cactus/issues/1634	{git,docker,typescript,http,vscode}	2022	Term 2	https://github.com/hyperledger/cactus/issues/1634	https://wiki.hyperledger.org/display/INTERN/Chia+Connector+for+Hyperledger+Cactus	300000	1
144	ca619db0-1b2d-4fbb-a9a7-d4d24c4340c3	CNCF - OpenEBS: An easy to use command-line interface (CLI) for OpenEBS	{"Description: OpenEBS is completely Kubernetes native and is implemented using microservices. OpenEBS can be installed via kubectl or helm chart and managed via Kubernetes custom resources. To improve the usability of OpenEBS, the proposal is to have easy to use OpenEBS CLI (similar to kubectl) to perform operations like:\n- upgrade => Upgrade OpenEBS pools and volumes\n- status => Print the readiness of various components, verify prerequisites are met to run openebs pools and volumes.\n- version => Print the OpenEBS version and associated images\n- describe => Describe OpenEBS component status like component","control plane, pools and volumes.\n- create => Create OpenEBS resources\n- delete => Delete OpenEBS resources"}	Description: OpenEBS is completely Kubernetes native and is implemented using microservices. OpenEBS can be installed via kubectl or helm chart and managed via Kubernetes custom resources. To improve the usability of OpenEBS, the proposal is to have easy to use OpenEBS CLI (similar to kubectl) to perform operations like:\n- upgrade => Upgrade OpenEBS pools and volumes\n- status => Print the readiness of various components, verify prerequisites are met to run openebs pools and volumes.\n- version => Print the OpenEBS version and associated images\n- describe => Describe OpenEBS component status like component/control plane, pools and volumes.\n- create => Create OpenEBS resources\n- delete => Delete OpenEBS resources	{go,kubernetes}	2021	Term 1	https://github.com/openebs/openebs/issues/2946	https://openebs.io/	660000	46
161	32dd1ee4-a927-49ed-99d2-6bd8a38ccd76	My Conservation Life	{"This is the Seneca Park Zoo Society Conservation project. It acts as a front end for an open source database to show a variety of conservation projects (currently in Madagascar) powered by donations from Seneca Park Zoo visitors"}	This is the Seneca Park Zoo Society Conservation project. It acts as a front end for an open source database to show a variety of conservation projects (currently in Madagascar) powered by donations from Seneca Park Zoo visitors	{javascript,react,express}	2021	Term 2	https://github.com/my-conservation-life/Conservation360		0	97
112	aa268a71-bad5-40c3-bd2b-4f50bb3cffca	LF Networking OPNFV - Hardware Delivery Verification Tool Development and Testing	{"Hardware is the base for NFV software deployment. However, it is still difficult to prepare and verify hardware in an automatic way. Based on several Operator's field trail experiences,  there is a desire to verify the hardware underlying the stack "," infrastructure, with this processing being automated.  Initially, this verification would be carried out in labs responsible for the performance of the compliance testing, especially in the future cases examining VNF performance, where hardware will have direct impact.  However, the tooling should also apply directly to larger deployments, verifying the hardware, settings, network wiring, etc., which will save valuable time for operators and users in standing up infrastructure."}	Hardware is the base for NFV software deployment. However, it is still difficult to prepare and verify hardware in an automatic way. Based on several Operator's field trail experiences,  there is a desire to verify the hardware underlying the stack / infrastructure, with this processing being automated.  Initially, this verification would be carried out in labs responsible for the performance of the compliance testing, especially in the future cases examining VNF performance, where hardware will have direct impact.  However, the tooling should also apply directly to larger deployments, verifying the hardware, settings, network wiring, etc., which will save valuable time for operators and users in standing up infrastructure.	{python,git}	2020	Term 2	https://github.com/cntt-n/CNTT/blob/master/doc/ref_impl/cntt-ri/chapters/chapter05.md	https://wiki.lfnetworking.org/display/LN/HDV	300000	161
395	fb6bd579-8034-4492-8bad-af9c82316b2c	Implement Tilelink Uncached Heavyweight (TL-UH) in Caravan Framework	{"\\"Caravan:\nCaravan is a framework that aims to provide CHISEL Designers with an easier way to create and integrate open-source bus protocols in their designs. It is a part of SoC-Now SoC Generator where this framework is the medium for connecting bus interconnects within the design for communication of core and devices. SoC-Now that accepts parameters and generates an entire SoC consisting of a Core with selective extensions and other configuration, devices which are selected via parameters and a Bus Interconnect that is used as the communication medium between the Core and the Devices, which is also selected by means of parameters. Currently SoC-Now contains the implementation of Wishbone and Tilelink Uncached Lightweight (TL-UL).\n\nTilelink Uncached Heavyweight (TL-UH)\nTL-UH is the bus protocol standardized by SiFive that is an extension of TL-UL and contains additional operations such as burst and atomic operations.\\""}	"Caravan:\nCaravan is a framework that aims to provide CHISEL Designers with an easier way to create and integrate open-source bus protocols in their designs. It is a part of SoC-Now SoC Generator where this framework is the medium for connecting bus interconnects within the design for communication of core and devices. SoC-Now that accepts parameters and generates an entire SoC consisting of a Core with selective extensions and other configuration, devices which are selected via parameters and a Bus Interconnect that is used as the communication medium between the Core and the Devices, which is also selected by means of parameters. Currently SoC-Now contains the implementation of Wishbone and Tilelink Uncached Lightweight (TL-UL).\n\nTilelink Uncached Heavyweight (TL-UH)\nTL-UH is the bus protocol standardized by SiFive that is an extension of TL-UL and contains additional operations such as burst and atomic operations."	{hdl,rtl}	2023	Term 1	https://github.com/merledu/caravan		300000	171
146	5c4a200d-b81e-4332-bea6-50120fbf28b4	CNCF - SPIFFE/SPIRE: Design and implement a health/status subsystem in SPIRE	{"SPIRE (https:","","spiffe.io), the SPIFFE Runtime Environment, is an extensible system that implements the principles embodied in the SPIFFE standards. SPIRE manages the platform and workload attestation provides an API for controlling attestation policies and coordinates certificate issuance and rotation. Being a critical system, it is important that operators be able to monitor (and respond to) the current health","state of their SPIRE deployments. To do this, SPIRE needs to grow a full-featured health subsystem that is capable of collecting the status of other subsystems and reporting on it. In this project, you will design and implement this new subsystem with the help and guidance of the SPIRE maintainers."}	SPIRE (https://spiffe.io), the SPIFFE Runtime Environment, is an extensible system that implements the principles embodied in the SPIFFE standards. SPIRE manages the platform and workload attestation provides an API for controlling attestation policies and coordinates certificate issuance and rotation. Being a critical system, it is important that operators be able to monitor (and respond to) the current health/state of their SPIRE deployments. To do this, SPIRE needs to grow a full-featured health subsystem that is capable of collecting the status of other subsystems and reporting on it. In this project, you will design and implement this new subsystem with the help and guidance of the SPIRE maintainers.	{go}	2021	Term 1	https://github.com/spiffe/spire/issues/2047	https://spiffe.io/	300000	89
162	c3c5bb20-0091-4a56-bba1-98a9a6303812	todogroup	{"TODO is an open group of companies who want to collaborate on practices, tools, and other ways to run successful and effective open source projects and programs"}	TODO is an open group of companies who want to collaborate on practices, tools, and other ways to run successful and effective open source projects and programs	{documentation}	2021	Term 2	https://github.com/todogroup	https://todogroup.org	400000	94
114	6573e36c-ad4c-4cbd-86ef-5f6699e0ae69	Hyperledger Besu - Create K8 / Openshift Operators	{"Hyperledger Besu is an Ethereum client written in Java. Operators are a method of packaging, deploying, and managing a Kubernetes or an OpenShift application. This project aims for the mentee to design, build and test operators for Project Besu. The code to this will then become part of the Besu project and added to the code repository."}	Hyperledger Besu is an Ethereum client written in Java. Operators are a method of packaging, deploying, and managing a Kubernetes or an OpenShift application. This project aims for the mentee to design, build and test operators for Project Besu. The code to this will then become part of the Besu project and added to the code repository.	{programming,kubernetes,openshift}	2020	Term 2	https://github.com/hyperledger/besu/	https://wiki.hyperledger.org/pages/viewpage.action?pageId=31195317	300000	110
476	8948ec6e-21ce-44db-a317-3a9b5afb98bb	Hyperledger - Enhance Blockchain Explorer to accommodate new features for Hyperledger Fabric	{"Blockchain Explorer is a GUI tool that allows users to view blocks, transactions, nodes, channels, chaincodes and other associated information about the blockchain network. The aim of this project is to enhance the existing application in-terms of new features and uplift the user-interface with the robust design. Blockchain explorer uses fabric-sdk to get the details from the fabric network and stores the data in the postgres database, which is later used by the explorer UI.\n\nThe developers will get a chance to get involved in the entire software development life cycle with the proposal-implementation of new features, review the peer developed code, write the unit test cases, refining the existing code efficiently to adhere to the coding standards and much more."}	Blockchain Explorer is a GUI tool that allows users to view blocks, transactions, nodes, channels, chaincodes and other associated information about the blockchain network. The aim of this project is to enhance the existing application in-terms of new features and uplift the user-interface with the robust design. Blockchain explorer uses fabric-sdk to get the details from the fabric network and stores the data in the postgres database, which is later used by the explorer UI.\n\nThe developers will get a chance to get involved in the entire software development life cycle with the proposal-implementation of new features, review the peer developed code, write the unit test cases, refining the existing code efficiently to adhere to the coding standards and much more.	{typescript,react,html,postgresql,git,linux,docker}	2023	Term 2	https://github.com/hyperledger-labs/blockchain-explorer	https://wiki.hyperledger.org/display/INTERN/Enhance+Blockchain+Explorer+to+accommodate+new+features+for+Hyperledger+Fabric	300000	1
120	16c0e98d-0b31-4e36-937e-2cbeb7fccebe	improving stress-ng kernel coverage	{"stress-ng is a tool designed to stress test and exercise core components of a kernel.   This project aims to improve the test coverage of stress-ng using data driven test coverage data generated using gcov.   The project will help increase understanding of the Linux kernel interfaces with userspace, usage of tools such as gcov and experience with C programming."}	stress-ng is a tool designed to stress test and exercise core components of a kernel.   This project aims to improve the test coverage of stress-ng using data driven test coverage data generated using gcov.   The project will help increase understanding of the Linux kernel interfaces with userspace, usage of tools such as gcov and experience with C programming.	{c,linux,kernel,gcov,git}	2020	Term 3	https://github.com/ColinIanKing/stress-ng	https://kernel.ubuntu.com/~cking/stress-ng/	300000	66
119	7bb815a7-6be1-43d4-b276-33aeb1cf0d82	Hyperledger - Towards Blockchain Interoperability	{"This project intends to propose a solution to this real-world problem, and it is research-focused. The goal is to give a strong contribution to the Hyperledger ecosystem in terms of the state of the art in blockchain interoperability. Some questions that this project aims to answer are:\n\nHow can we interoperate Hyperledger technologies with other blockchains?,\nHow can Hyperledger initiatives position themselves to tackle blockchain interoperability?,\nWhat standards we require to define middleware between blockchains?\nWhat are the consequences of employing smart contracts with external processes and data (the external part means another connected blockchain)?,\nThe key to the survivability of this technology? \n\nIn other to try to answer those questions, in this project the mentee(s) will start by studying some of the most prominent blockchain interoperability solutions already available or being proposed, and how they can be used to promote and strengthen Hyperledger technologies.  Then, the mentee will design and develop a framework for creating, deploying and maintaining services with several blockchains. The applicability of the solution to be proposed is to be demonstrated through the exploration of a use case, implementing several blockchains (including Hyperledger Fabric) to increase the dependability of blockchain-powered services administrated by several stakeholders, serving as a proof-of-concept."}	This project intends to propose a solution to this real-world problem, and it is research-focused. The goal is to give a strong contribution to the Hyperledger ecosystem in terms of the state of the art in blockchain interoperability. Some questions that this project aims to answer are:\n\nHow can we interoperate Hyperledger technologies with other blockchains?,\nHow can Hyperledger initiatives position themselves to tackle blockchain interoperability?,\nWhat standards we require to define middleware between blockchains?\nWhat are the consequences of employing smart contracts with external processes and data (the external part means another connected blockchain)?,\nThe key to the survivability of this technology? \n\nIn other to try to answer those questions, in this project the mentee(s) will start by studying some of the most prominent blockchain interoperability solutions already available or being proposed, and how they can be used to promote and strengthen Hyperledger technologies.  Then, the mentee will design and develop a framework for creating, deploying and maintaining services with several blockchains. The applicability of the solution to be proposed is to be demonstrated through the exploration of a use case, implementing several blockchains (including Hyperledger Fabric) to increase the dependability of blockchain-powered services administrated by several stakeholders, serving as a proof-of-concept.	{blockchain,javascript,python,rust,go,research}	2020	Term 2	https://github.com/hyperledger-labs/blockchain-integration-framework	https://wiki.hyperledger.org/display/INTERN/Towards+Blockchain+Interoperability+with+Hyperledger	600000	1
129	2d438b9a-c539-46d0-9eed-c6ee4404c88a	Kubernetes	{"Kubernetes (K8s) is an open-source system for automating deployment, scaling, and management of containerized applications.\nIt groups containers that make up an application into logical units for easy management and discovery. Kubernetes builds upon 15 years of experience of running production workloads at Google, combined with best-of-breed ideas and practices from the community."}	Kubernetes (K8s) is an open-source system for automating deployment, scaling, and management of containerized applications.\nIt groups containers that make up an application into logical units for easy management and discovery. Kubernetes builds upon 15 years of experience of running production workloads at Google, combined with best-of-breed ideas and practices from the community.	{kubernetes,go}	2019	Term 3	https://github.com/kubernetes	https://kubernetes.io/	3056500	75
163	c72e6f7c-c7b2-400a-86c2-6893cf58d976	RISC-V Mentorship: Formal verification of the SweRV-EL2 core	{"The RISC-V Mentorship Program enables one or more 12-week internship-style projects per session, funded by RISC-V, to match mentors","project leaders together with mentees","interns. Mentees are guided through a series of milestones by one or more project mentors, with whom the mentees meet on a weekly basis.\n\nWith this mentorship, the mentee will be performing formal verification of the SweRV-EL2 core (https:","",github.com,chipsalliance,"Cores-SweRV-EL2) using riscv-formal (https:","",github.com,SymbioticEDA,"riscv-formal).\n\nRISC-V formal has been applied for formal verification of picorv32 and Vexriscv cores that implement the RV32IMC Instruction set. Since the SweRV EL2 implements RV32IMC, it would be a good idea to formally verify it using RISC-V Formal. This would enable rapid adoption of the SweRV-EL2 core.\n\nDeliverables for this project include an open-source formal testbench for the SweRV-EL2 core that verifies the core correctly implements the RV32IMC instruction set. Additional formal checks can also be added."}	The RISC-V Mentorship Program enables one or more 12-week internship-style projects per session, funded by RISC-V, to match mentors/project leaders together with mentees/interns. Mentees are guided through a series of milestones by one or more project mentors, with whom the mentees meet on a weekly basis.\n\nWith this mentorship, the mentee will be performing formal verification of the SweRV-EL2 core (https://github.com/chipsalliance/Cores-SweRV-EL2) using riscv-formal (https://github.com/SymbioticEDA/riscv-formal).\n\nRISC-V formal has been applied for formal verification of picorv32 and Vexriscv cores that implement the RV32IMC Instruction set. Since the SweRV EL2 implements RV32IMC, it would be a good idea to formally verify it using RISC-V Formal. This would enable rapid adoption of the SweRV-EL2 core.\n\nDeliverables for this project include an open-source formal testbench for the SweRV-EL2 core that verifies the core correctly implements the RV32IMC instruction set. Additional formal checks can also be added.	{sail,systemverilog}	2021	Term 2	https://github.com/riscv	https://riscv.org/community/risc-v-mentorship-program/	300000	171
151	4e30e06e-1e2e-4f0b-b618-834599446c0c	CNCF - Tremor: Google Cloud Connector	{"Enhance tremor with connectors for the Google Cloud Platform"}	Enhance tremor with connectors for the Google Cloud Platform	{rust}	2021	Term 1	https://github.com/tremor-rs/tremor-runtime/issues/724	https://www.tremor.rs/	300500	20
126	d24ab158-e4e5-4042-91ad-b30ae52941d2	Fluentd	{"Fluentd is an open source data collector for building the unified logging layer. Once installed on a server, it runs in the background to collect, parse, transform, analyze and store various types of data."}	Fluentd is an open source data collector for building the unified logging layer. Once installed on a server, it runs in the background to collect, parse, transform, analyze and store various types of data.	{c,linux}	2020	Term 1	https://github.com/fluent/fluentd	https://www.fluentd.org/	1100000	72
479	fb6dcba3-7359-49f0-96f1-7d2ce4dea3ac	Hyperledger - Performance analysis and Benchmarking of Besu using Caliper with complex workloads	{"The objective of this project is to perform a performance and benchmarking analysis of BESU, a Hyperledger project, in private networks (QBFT, IBFT, CLIQUE) using Hyperledger Caliper. This project aims to identify the strengths and weaknesses of BESU in terms of maximum transactions per second, latency, computation power, hard disk space, etc. Additionally, this project aims to add support for more complex and well-known workloads like ERC20, ERC721 in Caliper, which are used by general Ethereum users. This project will provide valuable insights into the performance of BESU and will help developers optimise it for better performance.\n\nLearning Objectives\n\nThrough this project, the mentee will:\n- Develop skills in benchmarking and performance analysis of blockchain networks\n- Gain experience in working with Hyperledger Besu and Hyperledger Caliper\n- Learn to add support for new workloads in Caliper\n- Improve their programming skills by working on a real-world open-source project\n- Learn to write technical research reports and documentation.\n\nExpected Outcome\n\nThe expected deliverables of this project are:\n- A detailed report on the performance of BESU in private networks (QBFT, IBFT, CLIQUE)\n- A set of benchmarks for BESU in private network with different configurations\n- Support for ERC20, ERC721, etc workloads in Hyperledger Caliper\n- Documentation on how to use the benchmarking tool and the new workloads."}	The objective of this project is to perform a performance and benchmarking analysis of BESU, a Hyperledger project, in private networks (QBFT, IBFT, CLIQUE) using Hyperledger Caliper. This project aims to identify the strengths and weaknesses of BESU in terms of maximum transactions per second, latency, computation power, hard disk space, etc. Additionally, this project aims to add support for more complex and well-known workloads like ERC20, ERC721 in Caliper, which are used by general Ethereum users. This project will provide valuable insights into the performance of BESU and will help developers optimise it for better performance.\n\nLearning Objectives\n\nThrough this project, the mentee will:\n- Develop skills in benchmarking and performance analysis of blockchain networks\n- Gain experience in working with Hyperledger Besu and Hyperledger Caliper\n- Learn to add support for new workloads in Caliper\n- Improve their programming skills by working on a real-world open-source project\n- Learn to write technical research reports and documentation.\n\nExpected Outcome\n\nThe expected deliverables of this project are:\n- A detailed report on the performance of BESU in private networks (QBFT, IBFT, CLIQUE)\n- A set of benchmarks for BESU in private network with different configurations\n- Support for ERC20, ERC721, etc workloads in Hyperledger Caliper\n- Documentation on how to use the benchmarking tool and the new workloads.	{java,javascript,blockchain}	2023	Term 2	https://github.com/hyperledger/caliper	https://wiki.hyperledger.org/display/INTERN/Performance+analysis+and+Benchmarking+of+Besu+using+Caliper+with+complex+workloads	300000	1
127	ba41187f-fa8d-47e1-8046-4040e5b35b73	Keptn	{"Keptn is a message-driven control-plane for application delivery and automated operations"}	Keptn is a message-driven control-plane for application delivery and automated operations	{go,kubernetes,angular}	2021	Term 1	https://github.com/keptn/keptn	https://www.keptn.sh/	300000	73
128	1b931913-44a4-43a7-92ed-d7b2089060b1	KubeEdge	{"KubeEdge is an open source system for extending native containerized application orchestration capabilities to hosts at Edge."}	KubeEdge is an open source system for extending native containerized application orchestration capabilities to hosts at Edge.	{go}	2020	Term 2	https://github.com/kubeedge/kubeedge	https://kubeedge.io/	1100000	74
131	eedb7da8-3c05-4859-8877-c182e60a540b	SPDX Online Tools	{"The Software Package Data Exchange (SPDX) is an open source standard for communicating software bills of material information (including components, licenses, copyrights and security references).  To support SPDX, an online tool has been developed which provides an easy all-in-one website to upload and parse SPDX documents for validation, comparison and conversion and search SPDX license list.  This project will provide enhancements and support for the online tools including:\n-\tMore automated and high-quality deployment for software updates\n-\tRemoving some of the dependencies on a Python Java Bridge by moving features like license matching over to a native Python implementation\n-\tImproving performance and quality of the online tools implementation"}	The Software Package Data Exchange (SPDX) is an open source standard for communicating software bills of material information (including components, licenses, copyrights and security references).  To support SPDX, an online tool has been developed which provides an easy all-in-one website to upload and parse SPDX documents for validation, comparison and conversion and search SPDX license list.  This project will provide enhancements and support for the online tools including:\n-\tMore automated and high-quality deployment for software updates\n-\tRemoving some of the dependencies on a Python Java Bridge by moving features like license matching over to a native Python implementation\n-\tImproving performance and quality of the online tools implementation	{django,python}	2020	Term 2	https://github.com/spdx/spdx-online-tools	https://spdx.org/	602300	76
152	04fda726-ab2a-4111-aaf2-3ec7b95771e5	CNCF - Tremor: Property-based tests for tremor-script	{"Extend property-based testing for tremor script"}	Extend property-based testing for tremor script	{erlang,rust}	2021	Term 1	https://github.com/tremor-rs/tremor-runtime/issues/721	https://www.tremor.rs/	300000	20
137	4b711f9e-90f8-486a-90fc-d832e1c852ca	CNCF - Cloud Native Buildpacks: Design and implement Buildpack Registry Search	{"The Buildpack Registry (https:","",github.com,buildpacks,rfcs,blob,main,text,"0022-client-side-buildpack-registry.md) is a place to publish, store, and discover buildpacks. It will provide a centralized service that platforms can use to resolve a buildpack ID and version into a concrete buildpack that can be downloaded and used. The search service will extend the existing [registry index stored in git](https:","",github.com,buildpacks,"registry-index) in a way that can be consumed on the web as defined in this RFC (https:","",github.com,buildpacks,rfcs,blob,main,text,0068-buildpack-registry-search-api.md).}	The Buildpack Registry (https://github.com/buildpacks/rfcs/blob/main/text/0022-client-side-buildpack-registry.md) is a place to publish, store, and discover buildpacks. It will provide a centralized service that platforms can use to resolve a buildpack ID and version into a concrete buildpack that can be downloaded and used. The search service will extend the existing [registry index stored in git](https://github.com/buildpacks/registry-index) in a way that can be consumed on the web as defined in this RFC (https://github.com/buildpacks/rfcs/blob/main/text/0068-buildpack-registry-search-api.md).	{css,react,ruby,rails}	2021	Term 1	https://github.com/buildpacks/registry-api/issues/21	https://buildpacks.io/	600000	82
481	bd483044-1a95-4dc0-8c27-7d37fb9f22ee	Hyperledger - Document, Review, and Implement Hyperledger AnonCreds ZKP Cryptographic Primitives	{"Hyperledger AnonCreds includes specifications and implementations of AnonCreds (Anonymous Credentials) Zero-Knowledge Proof (ZKP)-based verifiable credentials. AnonCreds extends the capabilities of “typical” verifiable credentials by including important privacy-preserving features including selective disclosure, ZKP predicates (e.g., proving I’m older than 21 based on my date of birth without sharing my date of birth), and not revealing any correlatable identifiers in presenting credentials. The version 1.0 specification (based on CL-Signatures as initially implemented in Hyperledger Indy) is near completion, and work has begun on the version 2.0 specification that will retain and extend the privacy-preserving features of AnonCreds v1.0, while improving capabilities, performance, extensibility, and security. An implementation based on the Version 2.0 specification will occur in parallel with the development of the specification.\n\n- For the Version 1.0 specification, the primary task will be to extract the cryptographic primitives from identified sources, confirm alignment with the Rust implementation of AnonCreds, and include the cryptographic primitives in the specification. This is the last step of the specification to make it ready for submission to a standards development organization (SDO).\n- The Version 2.0 specification, the task will be similar, but the work will extend he focus as appropriate to implementation of the cryptographic primitives."}	Hyperledger AnonCreds includes specifications and implementations of AnonCreds (Anonymous Credentials) Zero-Knowledge Proof (ZKP)-based verifiable credentials. AnonCreds extends the capabilities of “typical” verifiable credentials by including important privacy-preserving features including selective disclosure, ZKP predicates (e.g., proving I’m older than 21 based on my date of birth without sharing my date of birth), and not revealing any correlatable identifiers in presenting credentials. The version 1.0 specification (based on CL-Signatures as initially implemented in Hyperledger Indy) is near completion, and work has begun on the version 2.0 specification that will retain and extend the privacy-preserving features of AnonCreds v1.0, while improving capabilities, performance, extensibility, and security. An implementation based on the Version 2.0 specification will occur in parallel with the development of the specification.\n\n- For the Version 1.0 specification, the primary task will be to extract the cryptographic primitives from identified sources, confirm alignment with the Rust implementation of AnonCreds, and include the cryptographic primitives in the specification. This is the last step of the specification to make it ready for submission to a standards development organization (SDO).\n- The Version 2.0 specification, the task will be similar, but the work will extend he focus as appropriate to implementation of the cryptographic primitives.	{cryptography,git}	2023	Term 2	https://www.hyperledger.org/use/anoncreds	https://wiki.hyperledger.org/display/INTERN/Document%2C+Review%2C+and+Implement+Hyperledger+AnonCreds+ZKP+Cryptographic+Primitives	300000	1
173	22ffa754-28f6-42fb-b8eb-9e2577d01ddf	Hyperledger - Support Decentralized Governance for Smart Contracts in Fabric Python SDK	{"With the introduction of Fabric v2.x, a more decentralized way of chaincode management is implemented. There are several improvements over the previous lifecycle and it requires several changes on the sdk. This project aims to support decentralized governance for smart contracts in fabric python sdk and add features such as private data sharing","verifying and external chaincode launcher. The projects will provide a user-friendly and easy-to-use tool for fabric developers and operators.\n\nLearning Objectives\n1) Contributing and collaborating in an open-source project\n2) Advanced understanding for DLT(distributed ledger technology)\n3) Understand the basic workflow of fabric\n4) Being able to implement features for SDK\n5) Writing good documentations\n\nExpected Outcome\n1) update on the chaincode lifecycle management\n--Multiple organizations must agree to the parameters of a chaincode\n--Deliberate chaincode upgrade process\n--Simpler endorsement policy and private data collection updates\n--Inspectable chaincode packages\n--Start multiple chaincodes on a channel using one package\n--Chaincode packages do not need to be identical across channel members\n2) add private data sharing & verifying features\n3) (optional) add features for writing smart contracts in python"}	With the introduction of Fabric v2.x, a more decentralized way of chaincode management is implemented. There are several improvements over the previous lifecycle and it requires several changes on the sdk. This project aims to support decentralized governance for smart contracts in fabric python sdk and add features such as private data sharing/verifying and external chaincode launcher. The projects will provide a user-friendly and easy-to-use tool for fabric developers and operators.\n\nLearning Objectives\n1) Contributing and collaborating in an open-source project\n2) Advanced understanding for DLT(distributed ledger technology)\n3) Understand the basic workflow of fabric\n4) Being able to implement features for SDK\n5) Writing good documentations\n\nExpected Outcome\n1) update on the chaincode lifecycle management\n--Multiple organizations must agree to the parameters of a chaincode\n--Deliberate chaincode upgrade process\n--Simpler endorsement policy and private data collection updates\n--Inspectable chaincode packages\n--Start multiple chaincodes on a channel using one package\n--Chaincode packages do not need to be identical across channel members\n2) add private data sharing & verifying features\n3) (optional) add features for writing smart contracts in python	{python,dlt,fabric,chaincode}	2021	Term 2	https://github.com/hyperledger/fabric-sdk-py/milestone/2	https://wiki.hyperledger.org/display/INTERN/Support+Decentralized+Governance+for+Smart+Contracts+in+Fabric+Python+SDK	540000	1
133	695343c8-2a1a-4d0b-9d4c-e2c7d319c161	SODA API	{"SODA API is an open source implementation of SODA API Standards for Data and Storage Management. The effort is to provide a common unified data and storage interface to store, run any data anywhere.\n\nThis project implements the API Server, Different API services, utilities and more. The implementation will be based on the API standards from The SODA Foundation.\n\nIt connects the application or client to the storage backends to do data","storage management seamlessly providing a platform and storage agnostic framework.\n\nAny application or client platform can implement the interface using the API specification to connect to API or implement northbound plugins to bridge their specifications to SODA API.\n\nCurrently we support block and file APIs for key features of data management (provisioning, migration, fileshare, etc). for on-prem implementation\n\nThis is one of the SODA Core Projects and is maintained by SODA Foundation directly.\n\nProvides the standardization for Data "," Storage Management APIs. Currently we support block and file APIs for key features of data management (provisioning, migration, fileshare, etc). Working to add the storage management APIs.\n\nThis is the key external interface to platforms, which can do a seamless integration with heterogeneous storage backends."}	SODA API is an open source implementation of SODA API Standards for Data and Storage Management. The effort is to provide a common unified data and storage interface to store, run any data anywhere.\n\nThis project implements the API Server, Different API services, utilities and more. The implementation will be based on the API standards from The SODA Foundation.\n\nIt connects the application or client to the storage backends to do data/storage management seamlessly providing a platform and storage agnostic framework.\n\nAny application or client platform can implement the interface using the API specification to connect to API or implement northbound plugins to bridge their specifications to SODA API.\n\nCurrently we support block and file APIs for key features of data management (provisioning, migration, fileshare, etc). for on-prem implementation\n\nThis is one of the SODA Core Projects and is maintained by SODA Foundation directly.\n\nProvides the standardization for Data / Storage Management APIs. Currently we support block and file APIs for key features of data management (provisioning, migration, fileshare, etc). Working to add the storage management APIs.\n\nThis is the key external interface to platforms, which can do a seamless integration with heterogeneous storage backends.	{go,kubernetes,docker,typescript,shell}	2021	Term 1	https://github.com/sodafoundation/api	https://sodafoundation.io/projects/soda-api/	0	78
138	630a656a-74ee-4494-9467-b10201ae9b20	CNCF - Keptn: Generate service skeleton via CLI	{"Provide a CLI command for Keptn CLI that generates a template repository to start developing a Keptn service integration."}	Provide a CLI command for Keptn CLI that generates a template repository to start developing a Keptn service integration.	{go,docker}	2021	Term 1	https://github.com/keptn/keptn/issues/3034	https://keptn.sh/	300000	73
141	51cb21c9-62eb-4e1c-9897-08e554a78b32	CNCF - Kubernetes SIG Storage: Kubernetes working group for CSI driver	{"Container Storage Interface (CSI) is a standard for exposing storage systems to containerized workloads on Kubernetes. The idea is to enhance a few CSI features(e.g. NFS, SMB) and also add e2e, sanity tests to cover those features, e.g. inline volume support, Windows support etc."}	Container Storage Interface (CSI) is a standard for exposing storage systems to containerized workloads on Kubernetes. The idea is to enhance a few CSI features(e.g. NFS, SMB) and also add e2e, sanity tests to cover those features, e.g. inline volume support, Windows support etc.	{go,kubernetes}	2021	Term 1	https://github.com/kubernetes-csi/	kubernetes.io	300000	75
143	d797e3e7-a036-4aa1-831a-fd06b00ed316	CNCF - OpenEBS:  Grafana Dashboards for monitoring OpenEBS	{"OpenEBS is completely Kubernetes native and is implemented using microservices. OpenEBS can be installed via kubectl or helm chart and managed via Kubernetes custom resources. Each of the OpenEBS components","services exposes Prometheus metrics. This proposal is to provide Grafana dashboards for monitoring the OpenEBS services."}	OpenEBS is completely Kubernetes native and is implemented using microservices. OpenEBS can be installed via kubectl or helm chart and managed via Kubernetes custom resources. Each of the OpenEBS components/services exposes Prometheus metrics. This proposal is to provide Grafana dashboards for monitoring the OpenEBS services.	{go,kubernetes}	2021	Term 1	https://github.com/openebs/openebs/issues/3333	https://openebs.io/	300000	46
145	08b86a82-9603-4b17-89b1-cace6b341c16	CNCF - OpenTelemetry: Work through OpenTelemetry User Research Documentation and Implement Fixes	{"Implement the fixes suggested in the user research documentation to make this project more consumable for end users."}	Implement the fixes suggested in the user research documentation to make this project more consumable for end users.	{php}	2021	Term 1	https://github.com/open-telemetry/opentelemetry-php/projects/5	https://opentelemetry.io/	300000	47
148	9920021d-11d2-4c7b-9534-c13e1b5660cc	CNCF - Thanos: Vertical Block Sharding	{"Current Thanos topology is generally horizontally scalable. However, the use cases and approaches of deploying Thanos shifted through time. While initially, Thanos was enabling ingestion through sidecars, now it’s not uncommon to see Thanos receiver usage. This means that the invariant of definite size TSDB block is no longer true. With offline deduplication and arbitrary Receive tenants data can be ingested into huge, often hundreds GB size TSDB blocks. This makes it harder to scale compaction and query operation on top of such blocks. The idea of this work is to vertically split larger blocks into smaller ones with the common scaling technique called sharding. As a mentee, we will guide you to make progress towards this goal by teaming up with experienced developers to deliver transparent automation for vertical block sharding! We are looking forward to working with you! During this mentorship, you will learn a lot about programming in Go, distributed Systems, TimeSeries Database, Prometheus, Thanos!"}	Current Thanos topology is generally horizontally scalable. However, the use cases and approaches of deploying Thanos shifted through time. While initially, Thanos was enabling ingestion through sidecars, now it’s not uncommon to see Thanos receiver usage. This means that the invariant of definite size TSDB block is no longer true. With offline deduplication and arbitrary Receive tenants data can be ingested into huge, often hundreds GB size TSDB blocks. This makes it harder to scale compaction and query operation on top of such blocks. The idea of this work is to vertically split larger blocks into smaller ones with the common scaling technique called sharding. As a mentee, we will guide you to make progress towards this goal by teaming up with experienced developers to deliver transparent automation for vertical block sharding! We are looking forward to working with you! During this mentorship, you will learn a lot about programming in Go, distributed Systems, TimeSeries Database, Prometheus, Thanos!	{go}	2021	Term 1	https://github.com/thanos-io/thanos/issues/3068	https://thanos.io/	300000	26
153	8e98bdb5-a131-4298-a195-31962d1a8564	CNCF - Tremor: Support for Syslog Protocol	{"Enable Tremor to receive and send Syslog Protocol Messages (https:","",tools.ietf.org,html,"rfc5424), supporting as many Syslog implementations as possible that might deviate from the standard"}	Enable Tremor to receive and send Syslog Protocol Messages (https://tools.ietf.org/html/rfc5424), supporting as many Syslog implementations as possible that might deviate from the standard	{rust}	2021	Term 1	https://github.com/tremor-rs/tremor-runtime/issues/12	https://www.tremor.rs/	300000	20
483	fe1a55e2-ee40-406c-af98-3b199250bf21	Hyperledger - Improvement in HLF-Connector, Hardening the production readiness aspects	{"The HLF-Connector is a project within the Hyperledger Labs community that provides a way to connect to and interact with Hyperledger Fabric Blockchain Networks from external applications using REST APIs & Kafka based Topics. This connector acts as a gateway between Fabric and non-Fabric applications, allowing external applications to securely read and write data to the Fabric blockchain through both synchronous and asynchronous modes of interaction. HLF-Connector provides a convenient and flexible way to integrate Fabric blockchain networks into external applications, allowing developers to build robust and secure decentralized applications that can interact with the Fabric network.\n\nLearning Objectives\n- Understand Hyperledger Fabric conceptually.\n- Learn enterprise integrations for practical usage of blockchain technology.\n- Become proficient in micro-service development.\n- Design API interfaces and work to scale by handling asynchronous interfaces.\n- Learn open source processes.\n- Work closely with community experts.\n\nExpected Outcome\n- Design and implement new features in the connector.\n- Updating the usage documentation."}	The HLF-Connector is a project within the Hyperledger Labs community that provides a way to connect to and interact with Hyperledger Fabric Blockchain Networks from external applications using REST APIs & Kafka based Topics. This connector acts as a gateway between Fabric and non-Fabric applications, allowing external applications to securely read and write data to the Fabric blockchain through both synchronous and asynchronous modes of interaction. HLF-Connector provides a convenient and flexible way to integrate Fabric blockchain networks into external applications, allowing developers to build robust and secure decentralized applications that can interact with the Fabric network.\n\nLearning Objectives\n- Understand Hyperledger Fabric conceptually.\n- Learn enterprise integrations for practical usage of blockchain technology.\n- Become proficient in micro-service development.\n- Design API interfaces and work to scale by handling asynchronous interfaces.\n- Learn open source processes.\n- Work closely with community experts.\n\nExpected Outcome\n- Design and implement new features in the connector.\n- Updating the usage documentation.	{java,blockchain}	2023	Term 2	https://github.com/hyperledger-labs/hlf-connector	https://wiki.hyperledger.org/display/INTERN/Improvements+in+HLF-Connector%2C+Hardening+the+production+readiness+aspects	300000	1
155	523735c3-9cb9-466f-977b-5719635a5ce7	CNCF - Kyverno: Integration of Kyverno with Litmus for chaos testing	{"Integrate Litmus Chaos testing with Kyevrno."}	Integrate Litmus Chaos testing with Kyevrno.	{go}	2021	Term 1	https://github.com/kyverno/kyverno/issues/1622	https://kyverno.io	300000	17
195	622bbf07-b001-470c-8da8-b714bb127183	CNCF - Racklet: Open source scale-model of Data Centers using commodity compute like Raspberry Pis	{"The future is already here - it's just not evenly distributed” - William Gibson\n\n  We’d like to introduce an idea for a new open-source project: Racklet. It’s a fully-integrated, Raspberry Pi form-factor\n  server rack and software stack that aims to be a scale model of hyperscaler datacenters. All layers of the stack\n  are 100% OSS","OSH, and will be developed together with the community. It’s reproducible through open PCB designs,\n  3D printed casing, and commodity, off-the-shelf hardware.\n\n  We want to lower the barrier of entry for becoming cloud native. Racklet aims to inspire users to explore how\n  modern server architectures work, in a tangible and educational way. Emphasis is put on security, knowledge\n  sharing, extensibility, and portability.\n\n  The goal is to conceptually map to real environments and provide an accessible and well-documented path to welcome\n  future talents to the world of cloud native."}	The future is already here - it's just not evenly distributed” - William Gibson\n\n  We’d like to introduce an idea for a new open-source project: Racklet. It’s a fully-integrated, Raspberry Pi form-factor\n  server rack and software stack that aims to be a scale model of hyperscaler datacenters. All layers of the stack\n  are 100% OSS/OSH, and will be developed together with the community. It’s reproducible through open PCB designs,\n  3D printed casing, and commodity, off-the-shelf hardware.\n\n  We want to lower the barrier of entry for becoming cloud native. Racklet aims to inspire users to explore how\n  modern server architectures work, in a tangible and educational way. Emphasis is put on security, knowledge\n  sharing, extensibility, and portability.\n\n  The goal is to conceptually map to real environments and provide an accessible and well-documented path to welcome\n  future talents to the world of cloud native.	{go,rust,kubernetes,linux}	2021	Term 2	https://github.com/racklet/racklet	https://docs.racklet.io/rfcs/0001-high-level-architecture.html	2400000	106
149	f0bb4ba5-e7e9-4678-8cd1-75f7a7977381	CNCF - TiKV: Coprocessor plugin	{"Implement a basic coprocessor plugin runtime on top of Wasmer."}	Implement a basic coprocessor plugin runtime on top of Wasmer.	{rust}	2021	Term 1	https://github.com/tikv/tikv/issues/8036	https://tikv.org/	540000	90
150	1417be44-d2e2-4cdd-8ba7-9de70b87e1a2	CNCF - TiKV: Implement Jepsen test for TiKV	{"Build an integration test framework with Jepsen for TiKV, using the TiKV Rust client."}	Build an integration test framework with Jepsen for TiKV, using the TiKV Rust client.	{rust,clojure}	2021	Term 1	https://github.com/tikv/tikv/issues/9588	https://tikv.org/	300000	90
25	e5101a74-a177-4f65-b882-8c8ac2a2d65d	CICS and Laptop Option for COBOL Programming Course	{"Create a version of the OMP COBOL Course that includes new modules to cover CICS COBOL programming basics. Additionally, to enable a version of the course that can be run on a laptop using the free version of Enterprise Developer for IBM Z."}	Create a version of the OMP COBOL Course that includes new modules to cover CICS COBOL programming basics. Additionally, to enable a version of the course that can be run on a laptop using the free version of Enterprise Developer for IBM Z.	{cobol}	2021	Term 2	https://github.com/openmainframeproject/wg-cobol	https://www.openmainframeproject.org/projects/cobol-working-group	600000	18
164	e6884a4f-c8bd-4a31-ac6b-577f6db507bc	OSPO Playbook	{"A summarization of various resources and building a documentation playbook detailing the specifics and resources needed for running Open Programs offices within a University."}	A summarization of various resources and building a documentation playbook detailing the specifics and resources needed for running Open Programs offices within a University.	{deployment,documentation,git,markdown}	2021	Term 2	https://opensource.ieee.org/rit/ospo-playbook		0	97
166	7ce99708-8ead-40df-ba12-6ed02f639c04	CNCF - Thanos: gRPC Exemplars API	{"Exemplars are an amazing solution that allows linking metrics to logs, traces, and more! Recently Prometheus added support to Exemplars as defined by OpenMetrics API. In Thanos with our powerful deployment flexibility, we can allow federating Exemplars up to multi-cluster, global level! During this task mentee will develop together with mentors a new gRPC API that allows to access Prometheus exemplars on Thanos level. This is a work item bringing novel and edge technology to the open-source, which will enormously help Thanos users. During this mentorship, you will learn a lot about programming in Go, distributed Systems, gRPC Observability, Prometheus, Thanos!"}	Exemplars are an amazing solution that allows linking metrics to logs, traces, and more! Recently Prometheus added support to Exemplars as defined by OpenMetrics API. In Thanos with our powerful deployment flexibility, we can allow federating Exemplars up to multi-cluster, global level! During this task mentee will develop together with mentors a new gRPC API that allows to access Prometheus exemplars on Thanos level. This is a work item bringing novel and edge technology to the open-source, which will enormously help Thanos users. During this mentorship, you will learn a lot about programming in Go, distributed Systems, gRPC Observability, Prometheus, Thanos!	{go,grpc}	2021	Term 1	https://github.com/thanos-io/thanos/issues/3435	https://thanos.io/	300000	26
159	74a898a5-d741-4c92-87c9-456541214395	CNCF - Thanos: Stateless Ruler	{"Thanos Ruler is a critical component in Thanos that is responsible for the alert evaluation and recording rules. However, a few extensive rules can create a significant amount of resulting time-series, limiting the scalability of Thanos Rule, as it uses a single embedded TSDB. Recording","Alerting Rules are a substantial piece of monitoring infrastructure, so we want to ensure users can operate Rulers and scale them in an easy way. There is no way to scale rule evaluation and storage today except functionally sharding rules onto multiple instances of the Thanos Ruler component. Luckily, we have already solved scaling storage of time-series across various processes using Thanos Receiver. To scale rule evaluations and storage, during this mentorship, you will have a chance to implement the proposal that allows the Thanos rule component to have a stateless mode, storing results of queries by sending them to a Thanos receive hash-ring instead of storing them locally. You will learn about Go, Time-series databases, distributed system design, Prometheus, and of course Thanos."}	Thanos Ruler is a critical component in Thanos that is responsible for the alert evaluation and recording rules. However, a few extensive rules can create a significant amount of resulting time-series, limiting the scalability of Thanos Rule, as it uses a single embedded TSDB. Recording/Alerting Rules are a substantial piece of monitoring infrastructure, so we want to ensure users can operate Rulers and scale them in an easy way. There is no way to scale rule evaluation and storage today except functionally sharding rules onto multiple instances of the Thanos Ruler component. Luckily, we have already solved scaling storage of time-series across various processes using Thanos Receiver. To scale rule evaluations and storage, during this mentorship, you will have a chance to implement the proposal that allows the Thanos rule component to have a stateless mode, storing results of queries by sending them to a Thanos receive hash-ring instead of storing them locally. You will learn about Go, Time-series databases, distributed system design, Prometheus, and of course Thanos.	{go}	2021	Term 1	https://github.com/thanos-io/thanos/issues/3761	https://thanos.io/	300000	26
169	6b88c848-0fa6-4751-9c76-7fbcd067b50a	Hyperledger Iroha - Blueprint-like interface for Iroha Special Instructions	{"The task will be to implement a visual blueprint frontend similar to Blockly, Scratch, or Unreal Engine Blueprints for the Iroha Special Instructions smart contract language.\n\nIroha Special Instructions (or ISI) are a composable smart contract language for Iroha v2 providing an opportunity to execute logic on-chain. In their raw form they represent a syntactical tree, which is not very convenient to write. We think that the structure of the ISI will perfectly match a visual blueprint language and therefore make developing Iroha v2 smart contracts more comfortable for everyone.\n\nThe app to be developed should allow the user to compose ISI, potentially validating the correspondent types of outputs of each of the blocks. After the program of blueprints is composed, the app should provide a way to transpile the blocks into the file of raw ISI in their byte format. Potentially the app can also act like a client and submit the ISI directly to a Hyperledger Iroha network to be executed right away."}	The task will be to implement a visual blueprint frontend similar to Blockly, Scratch, or Unreal Engine Blueprints for the Iroha Special Instructions smart contract language.\n\nIroha Special Instructions (or ISI) are a composable smart contract language for Iroha v2 providing an opportunity to execute logic on-chain. In their raw form they represent a syntactical tree, which is not very convenient to write. We think that the structure of the ISI will perfectly match a visual blueprint language and therefore make developing Iroha v2 smart contracts more comfortable for everyone.\n\nThe app to be developed should allow the user to compose ISI, potentially validating the correspondent types of outputs of each of the blocks. After the program of blueprints is composed, the app should provide a way to transpile the blocks into the file of raw ISI in their byte format. Potentially the app can also act like a client and submit the ISI directly to a Hyperledger Iroha network to be executed right away.	{}	2021	Term 2	https://github.com/hyperledger/iroha/tree/iroha2-dev	https://wiki.hyperledger.org/display/INTERN/Blueprint-like+interface+for+Iroha+Special+Instructions	600000	110
486	428e6fb1-587d-4ba1-8ceb-fef7d00814ac	Hyperledger - Runtime-checked automated programming for chaincode development	{"Can we make ChatGPT write chaincode for us?\n\nAutomated programming for other blockchain platforms is an already emerging topic, but Fabric has not been really addressed yet. This is an experimental mentorship with equal weight in research and hands-on coding, with the following goals.\n\n- To review existing examples and approaches of automated programming for smart contracts of other platforms (i.e., Solidity).\n- To explore the technically feasible options of automated chaincode programming for Hyperledger Fabric (incl. ChatGPT) and select one for further work.\n- To formulate a chaincode specification style which “seems to work well enough” on a set of representative examples with automated programming.\n(We don’t know yet what will work; One-shot or iterative? Requirement set or Behavior-Driven Development (BDD)? Conversational or formal specification? And so on.)\n- To create support for translating the specification partially or fully to runtime verification code, which can wrap the chaincode implementation.\n\nOn the last point: starting from a specification is a tried and tested way to create verifier code either manually or automatically and is much easier to do correctly than creating the implementation correctly from a specification. We plan to keep this last point “classic”: i.e., no ChatGPT here. Specification-based verification criteria are usually amenable to development time verification, too (with static analysis, model checking, etc.)."}	Can we make ChatGPT write chaincode for us?\n\nAutomated programming for other blockchain platforms is an already emerging topic, but Fabric has not been really addressed yet. This is an experimental mentorship with equal weight in research and hands-on coding, with the following goals.\n\n- To review existing examples and approaches of automated programming for smart contracts of other platforms (i.e., Solidity).\n- To explore the technically feasible options of automated chaincode programming for Hyperledger Fabric (incl. ChatGPT) and select one for further work.\n- To formulate a chaincode specification style which “seems to work well enough” on a set of representative examples with automated programming.\n(We don’t know yet what will work; One-shot or iterative? Requirement set or Behavior-Driven Development (BDD)? Conversational or formal specification? And so on.)\n- To create support for translating the specification partially or fully to runtime verification code, which can wrap the chaincode implementation.\n\nOn the last point: starting from a specification is a tried and tested way to create verifier code either manually or automatically and is much easier to do correctly than creating the implementation correctly from a specification. We plan to keep this last point “classic”: i.e., no ChatGPT here. Specification-based verification criteria are usually amenable to development time verification, too (with static analysis, model checking, etc.).	{chaincode}	2023	Term 2	https://github.com/hyperledger	https://wiki.hyperledger.org/display/INTERN/Runtime-checked+automated+programming+for+chaincode+development	600000	1
158	fc674c5f-3429-4591-9cae-8f58d347560f	CNCF - KubeEdge: Integration and verification of third-party CNI/CSI based on the edge side list-wat	{"We need to develop integration and verification of third-party CNI","CSI for the edge applications."}	We need to develop integration and verification of third-party CNI/CSI for the edge applications.	{go}	2021	Term 1	https://github.com/kubeedge/kubeedge/issues/2545	https://kubeedge.io/en/	540000	74
174	d155aa68-9ddf-4c83-a4cd-dbc3a563c84d	Hyperledger - Implement Client Side Security for Climate SIG Fabric Application	{"The Carbon Accounting and Certification Working Group is developing an Operating System for Climate Action, where there is a Hyperledger Fabric Utility Emissions Channel with utility data.  Currently, the security certificates for accessing Fabric are held server side.  A client application authenticates the user through standard username","password authentication and then access the Fabric chain code on behalf of the user. \n\nWe would like to explore a client driven authentication for the Fabric application, similar to how Metamask is used with Ethereum dApps such as the Emissions Tokens Network.  In such a use case, the user would sign into a web portal, enter and upload their information, and then sign the transaction with a local security key or a dApp wallet such as Metamask.\n\nAs part of this project, you will learn about\n1) Hyperledger Fabric chain code development\n2) REST APIs\n3) dApp security and wallets\n4) Open source development and project management\n\nExpected Outcome\nImplementation of an application with client side authentication for Hyperledger Fabric utility emissions data channel.  Documentation and tutorials showing how this is done."}	The Carbon Accounting and Certification Working Group is developing an Operating System for Climate Action, where there is a Hyperledger Fabric Utility Emissions Channel with utility data.  Currently, the security certificates for accessing Fabric are held server side.  A client application authenticates the user through standard username/password authentication and then access the Fabric chain code on behalf of the user. \n\nWe would like to explore a client driven authentication for the Fabric application, similar to how Metamask is used with Ethereum dApps such as the Emissions Tokens Network.  In such a use case, the user would sign into a web portal, enter and upload their information, and then sign the transaction with a local security key or a dApp wallet such as Metamask.\n\nAs part of this project, you will learn about\n1) Hyperledger Fabric chain code development\n2) REST APIs\n3) dApp security and wallets\n4) Open source development and project management\n\nExpected Outcome\nImplementation of an application with client side authentication for Hyperledger Fabric utility emissions data channel.  Documentation and tutorials showing how this is done.	{}	2021	Term 2	https://github.com/hyperledger-labs/blockchain-carbon-accounting	https://wiki.hyperledger.org/display/INTERN/Implement+Client+Side+Security+for+Climate+SIG+Fabric+Application	300000	1
491	a1e7cb12-cfca-4798-858c-48a32bd4a9c0	Hyperledger - Decentralized Identity Management for Trusted Interoperation	{"The goal of this project will be to build a template or sample Interoperation Identity Network (IIN) by minimally augmenting existing decentralized identity registries built on Hyperledger Indy and Aries, adding support for group Decentralized Identifiers (DIDs), and optionally proposing changes or additions to W3C standards for DIDs and Verifiable Credentials (VCs). To demonstrate the working of identity plane protocols, existing Cacti IIN agents for Hyperledger Fabric must be augmented, and IIN Agents for Corda must be built, enabling data sharing and asset transfer across networks built on either Fabric or Corda. Also, a sample trust anchor must be designed and implemented as a service to issue VCs to network members."}	The goal of this project will be to build a template or sample Interoperation Identity Network (IIN) by minimally augmenting existing decentralized identity registries built on Hyperledger Indy and Aries, adding support for group Decentralized Identifiers (DIDs), and optionally proposing changes or additions to W3C standards for DIDs and Verifiable Credentials (VCs). To demonstrate the working of identity plane protocols, existing Cacti IIN agents for Hyperledger Fabric must be augmented, and IIN Agents for Corda must be built, enabling data sharing and asset transfer across networks built on either Fabric or Corda. Also, a sample trust anchor must be designed and implemented as a service to issue VCs to network members.	{javascript,python,http}	2023	Term 2	https://github.com/hyperledger/cacti	https://wiki.hyperledger.org/display/INTERN/Cacti%3A+Decentralized+Identity+Management+for+Trusted+Interoperation	300000	1
176	d2c4a0c3-1a66-4a1f-8595-ac1ef747eb6e	Hyperledger - Extend secure DID Registry for Hyperledger frameworks on Github/Gitlab	{"A DID registry is a type of verifiable data registry that can be simply referred to as a role, a system performs to mediate the functionalities like create, verify, update, and deactivate the decentralized identifiers.\n\nThe mentee will have an opportunity to learn\n-- The application and automation of the Decentralized Identity \nHyperledger Aries and Ursa code base\n-- Public key cryptography\n-- Application of Hyperledger Ursa in enabling Zero-Knowledge proof\n-- Git, Shell Scripting, RUST\n-- Open source contribution, documentation, and sense of ownership\nExpected Outcome\n-- Command-Line utility to automate the process of creating a DID Registry for the members of organizations utilizing Github","Gitlab as Verifiable Credential Registry, On-boarding organizations, and features for easy management\n-- Proper test cases and documentations\n-- Codebase maintained with proper read me document"}	A DID registry is a type of verifiable data registry that can be simply referred to as a role, a system performs to mediate the functionalities like create, verify, update, and deactivate the decentralized identifiers.\n\nThe mentee will have an opportunity to learn\n-- The application and automation of the Decentralized Identity \nHyperledger Aries and Ursa code base\n-- Public key cryptography\n-- Application of Hyperledger Ursa in enabling Zero-Knowledge proof\n-- Git, Shell Scripting, RUST\n-- Open source contribution, documentation, and sense of ownership\nExpected Outcome\n-- Command-Line utility to automate the process of creating a DID Registry for the members of organizations utilizing Github/Gitlab as Verifiable Credential Registry, On-boarding organizations, and features for easy management\n-- Proper test cases and documentations\n-- Codebase maintained with proper read me document	{shell,git,github}	2021	Term 2	https://github.com/DIDman/DRman	https://wiki.hyperledger.org/pages/viewpage.action?pageId=41594660	600000	1
157	cf597b73-56fd-45b8-a605-5bb73f6b838d	CNCF - KubeEdge: Design more tests for specific scenarios of edge computing	{"We need to do some designs for adding more tests especially for the specific scenarios of edge computing, eg:\n- Application migration when the network is disconnected\n- System stability when the network is unstable\n- Run large-scale cluster tests periodically"}	We need to do some designs for adding more tests especially for the specific scenarios of edge computing, eg:\n- Application migration when the network is disconnected\n- System stability when the network is unstable\n- Run large-scale cluster tests periodically	{go}	2021	Term 1	https://github.com/kubeedge/kubeedge/issues/2544	https://kubeedge.io/en/	300000	74
279	83b6042d-25e9-4e02-b9b7-e17e7f6fbf1b	CNCF - KubeEdge: Move edge native k8s api interface GA	{"Description: Now we have add the edge native k8s api interface, apps like operator running in edgeside can access the apiserver and obtain resources. We still need to fix bug and improve the stability."}	Description: Now we have add the edge native k8s api interface, apps like operator running in edgeside can access the apiserver and obtain resources. We still need to fix bug and improve the stability.	{kubernetes,kubeedge}	2022	Term 1	https://github.com/kubeedge/kubeedge/issues/3596	https://kubeedge.io/en/	300000	74
175	e62df6a8-66ca-4861-8690-7d0398523be6	Hyperledger - Documentation and Use Cases for Climate Action	{"The Carbon Accounting and Certification Working Group is developing an Operating System for Climate Action, which is a set of applications for obtaining and certifying data, creating tradeable tokens for emissions and offsets, and using DAO's to coordinate collective action.\n\nAs part of this project, we're looking for technical writers who could help both document the technologies and how they could be applied to actual use cases.  We've started with a few ideas and would like your help refining them with actual or potential projects.  We'd also like help reaching out to and interviewing potential users of the applications to get their feedback on how they could be used.\n\nAs part of this project, you will learn about\n1) Blockchain technologies including Hyperledger Fabric and Ethereum\n2) Applications such as Distributed Autonomous Organizations (DAO's)\n3) Interviewing skills\n4) Technical writing and documentation\n\nExpected Outcome\nImproved documentation of the blockchain carbon accounting applications.  Tutorials on how to develop and enhance the applications.  Use cases for the applications."}	The Carbon Accounting and Certification Working Group is developing an Operating System for Climate Action, which is a set of applications for obtaining and certifying data, creating tradeable tokens for emissions and offsets, and using DAO's to coordinate collective action.\n\nAs part of this project, we're looking for technical writers who could help both document the technologies and how they could be applied to actual use cases.  We've started with a few ideas and would like your help refining them with actual or potential projects.  We'd also like help reaching out to and interviewing potential users of the applications to get their feedback on how they could be used.\n\nAs part of this project, you will learn about\n1) Blockchain technologies including Hyperledger Fabric and Ethereum\n2) Applications such as Distributed Autonomous Organizations (DAO's)\n3) Interviewing skills\n4) Technical writing and documentation\n\nExpected Outcome\nImproved documentation of the blockchain carbon accounting applications.  Tutorials on how to develop and enhance the applications.  Use cases for the applications.	{programming}	2021	Term 2	https://github.com/hyperledger-labs/blockchain-carbon-accounting	https://wiki.hyperledger.org/display/INTERN/Documentation+and+Use+Cases+for+Climate+Action	360000	1
495	239f8808-d7de-4a80-a6b6-becc18ab7980	Hyperledger - Make Solidity Language Server Functional	{"A language server is what makes it possible to rich editing like to go to definition or code completions in source code editors. The Solang Solidity Compiler does have a language server, but it is very rudimentary. The aim of this mentorship is to make the language server much more complete, will make developing Solidity code using this language server will be a much greater developer experience.\n\nDuring this mentorship you will learn how language servers work, and you'll need to walk the parsed Solidity abstract syntax tree in various ways to for example find where a variable is defined (Go to definition), or to find all usages of a variable (Find all references), or find the type definition (Go to type definition). This means walking a complex data structure, and understanding how source code is represented as a abstract syntax tree, which is the result of parsing and semantic analysis, the first stage of an compiler."}	A language server is what makes it possible to rich editing like to go to definition or code completions in source code editors. The Solang Solidity Compiler does have a language server, but it is very rudimentary. The aim of this mentorship is to make the language server much more complete, will make developing Solidity code using this language server will be a much greater developer experience.\n\nDuring this mentorship you will learn how language servers work, and you'll need to walk the parsed Solidity abstract syntax tree in various ways to for example find where a variable is defined (Go to definition), or to find all usages of a variable (Find all references), or find the type definition (Go to type definition). This means walking a complex data structure, and understanding how source code is represented as a abstract syntax tree, which is the result of parsing and semantic analysis, the first stage of an compiler.	{parsers,rust,typescript}	2023	Term 2	https://github.com/hyperledger/solang	https://wiki.hyperledger.org/display/INTERN/Make+the+Solidity+Language+Server+functional	300000	1
517	55c226fe-d119-4b2c-aba0-e7415867f6e5	CNCF - WasmEdge: A stream log processing framework for WasmEdge	{"In this project, we aim to build a Rust-based log processing framework. Applications built on this framework will be compiled into WebAssembly and run in WasmEdge containers side by side with Linux containers and apps. The WasmEdge app collects logs from other containerized apps and then sends them to a streaming database or processing pipeline.\n- Expected outcome:\n  * Create a Rust framework with 3 traits similar to the [`Transformer`](https:","",github.com,second-state,MEGA,blob,main,mega_etl,src,"lib.rs#L99) trait in the [MEGA framework](https:","",github.com,second-state,"MEGA).\n    * The `Collector` trait abstracts operations needed for a log collector.\n    * The `Transformer` trait abstracts the transformation algorithms that can be applied to the logs.\n    * The `Destination` trait abstracts operations needed to send transformed to a streaming data pipeline or database.\n  * Implement at least two `Collector`s. One for MySQL database binlog and the other for a generic log file in a Linux container in the same Kubernetes pod.\n  * Implement at least two `Transformer` algorithms supported by [FileBeat](https:","",www.elastic.co,guide,en,beats,filebeat,current,"filebeat-overview.html).\n  * Implement at least three `Destination`s. One for a Kafka queue, one for a Redis database, and the other for ElasticSearch.\n  * Provide CI and demo test cases.\n  * Provide documentation and tutorials."}	In this project, we aim to build a Rust-based log processing framework. Applications built on this framework will be compiled into WebAssembly and run in WasmEdge containers side by side with Linux containers and apps. The WasmEdge app collects logs from other containerized apps and then sends them to a streaming database or processing pipeline.\n- Expected outcome:\n  * Create a Rust framework with 3 traits similar to the [`Transformer`](https://github.com/second-state/MEGA/blob/main/mega_etl/src/lib.rs#L99) trait in the [MEGA framework](https://github.com/second-state/MEGA).\n    * The `Collector` trait abstracts operations needed for a log collector.\n    * The `Transformer` trait abstracts the transformation algorithms that can be applied to the logs.\n    * The `Destination` trait abstracts operations needed to send transformed to a streaming data pipeline or database.\n  * Implement at least two `Collector`s. One for MySQL database binlog and the other for a generic log file in a Linux container in the same Kubernetes pod.\n  * Implement at least two `Transformer` algorithms supported by [FileBeat](https://www.elastic.co/guide/en/beats/filebeat/current/filebeat-overview.html).\n  * Implement at least three `Destination`s. One for a Kafka queue, one for a Redis database, and the other for ElasticSearch.\n  * Provide CI and demo test cases.\n  * Provide documentation and tutorials.	{kafka,filebeat,rust,elasticsearch,wasmedge,wasm}	2023	Term 2	https://github.com/WasmEdge/WasmEdge/issues/2470	https://wasmedge.org/	300000	31
179	a2672e89-28a2-4091-a5d7-768f96ddce1d	ZOROW - BYO z/OSMF Workflow	{"Develop z","OSMF workflow for some common tasks such as copying one dataset to other. Learn various capabilities provided by z","OSMF workflow. Contribute the z","OSMF workflows to the ZOROW community."}	Develop z/OSMF workflow for some common tasks such as copying one dataset to other. Learn various capabilities provided by z/OSMF workflow. Contribute the z/OSMF workflows to the ZOROW community.	{xml}	2021	Term 2	https://github.com/openmainframeproject/zorow	https://www.openmainframeproject.org/projects/zorow	0	101
180	78234cf3-5ff2-41ab-972e-a1688f1a197c	Hyperledger Cello - Operate Blockchain Network in an Efficient Way	{"Today, most users adopt Blockchain in order to collaborate with each other, however, it is difficult to manage a large blockchain network because of multiple organizations and nodes. \n\nHyperledger Cello is aiming to resolve the following challenges, 1) Facilitate creation of blockchain network, can help user without blockchain background to setup their network nodes quickly. 2) Cross organization communication, can connect blockchain networks among multiple organizations. This project targets to design and implement a practical operational system equipping with decentralized functionalities to solve above challenges, based on Hyperledger Cello code base.\n\nLearning Objectives\n1) Work closely with community experts and developers to learn the open-source culture and skills;\n2) Learn the advanced knowledge inside the blockchain and distributed ledgers;\n3) Practice hand-on experience with web application design and implement.\n\nExpected Outcome\nHelp design and implement the blockchain operation dashboard and api server."}	Today, most users adopt Blockchain in order to collaborate with each other, however, it is difficult to manage a large blockchain network because of multiple organizations and nodes. \n\nHyperledger Cello is aiming to resolve the following challenges, 1) Facilitate creation of blockchain network, can help user without blockchain background to setup their network nodes quickly. 2) Cross organization communication, can connect blockchain networks among multiple organizations. This project targets to design and implement a practical operational system equipping with decentralized functionalities to solve above challenges, based on Hyperledger Cello code base.\n\nLearning Objectives\n1) Work closely with community experts and developers to learn the open-source culture and skills;\n2) Learn the advanced knowledge inside the blockchain and distributed ledgers;\n3) Practice hand-on experience with web application design and implement.\n\nExpected Outcome\nHelp design and implement the blockchain operation dashboard and api server.	{fabric,python,html,javascript,django,docker,kubernetes}	2021	Term 2	https://github.com/hyperledger/cello	https://wiki.hyperledger.org/display/INTERN/Operate+Blockchain+Network+in+an+Efficient+Way	600000	110
499	f33e952d-3624-4673-8e9d-27232da6b0a0	Hyperledger - aries-vcx next-gen mobile wrapper	{"Implement rust wrapper around aries-vcx  to enable consuming the library in mobile setting using modern FFI approach using uniffi library. Additionally implement aries mediator client to unlock full mobile support.\n\nLearning Objectives\n- Learn about self sovereign identity and Aries protocols\n- Learn Rust language\n- Learn basics of Kotlin language, basics of Android development\n- Learn Github Actions and CI process in general\n- Learn about FFI (Foreign Function Interface) by using uniffi library \n\nExpected Outcome\n- uniffi_aries_vcx  rust crate which generate mobile bindings for aries-vcx library\n- implement mediator client with aries message pick-up protocol support\n- simple android demo application which can be run in simulator\n- CI job to build Kotlin and iOS bindings, CI job to test the wrapper"}	Implement rust wrapper around aries-vcx  to enable consuming the library in mobile setting using modern FFI approach using uniffi library. Additionally implement aries mediator client to unlock full mobile support.\n\nLearning Objectives\n- Learn about self sovereign identity and Aries protocols\n- Learn Rust language\n- Learn basics of Kotlin language, basics of Android development\n- Learn Github Actions and CI process in general\n- Learn about FFI (Foreign Function Interface) by using uniffi library \n\nExpected Outcome\n- uniffi_aries_vcx  rust crate which generate mobile bindings for aries-vcx library\n- implement mediator client with aries message pick-up protocol support\n- simple android demo application which can be run in simulator\n- CI job to build Kotlin and iOS bindings, CI job to test the wrapper	{programming}	2023	Term 2	https://github.com/hyperledger/aries-vcx/tree/main/uniffi_aries_vcx	https://wiki.hyperledger.org/display/INTERN/aries-vcx+next-gen+mobile+wrapper	300000	1
187	e978885c-fe45-48b1-9a67-9cc9ffd68e82	CNCF - Keptn: Provide a hub for Keptn integrations	{"Currently, Keptn services and integrations can be found on an overview page at https:","",keptn.sh,docs,integrations," While this served fine as a central overview of all currently supported integrations, a more sophisticated \\"integrations hub\\" is desired. The hub should list all available integrations including their name, status, install numbers","github stars, description, and installation instructions. The project includes a research task of other hubs and how they are built."}	Currently, Keptn services and integrations can be found on an overview page at https://keptn.sh/docs/integrations/ While this served fine as a central overview of all currently supported integrations, a more sophisticated "integrations hub" is desired. The hub should list all available integrations including their name, status, install numbers/github stars, description, and installation instructions. The project includes a research task of other hubs and how they are built.	{go,javascript}	2021	Term 2	https://github.com/keptn/keptn/issues/3406	https://keptn.sh/	0	73
188	38cafd48-6ffb-44e8-9ed1-5c467bc69fd2	CNCF - Keptn: Support for generic webhook execution	{"As a user, I want to be able to call arbitrary URLs via webhooks that are registered on Keptn events to interact with systems outside of Keptn. Therefore, I would like to use a templating mechanism to define payloads to be able to interact with external systems."}	As a user, I want to be able to call arbitrary URLs via webhooks that are registered on Keptn events to interact with systems outside of Keptn. Therefore, I would like to use a templating mechanism to define payloads to be able to interact with external systems.	{go}	2021	Term 2	https://github.com/keptn/keptn/issues/3822	https://keptn.sh/	0	73
189	5ad92659-907e-45b6-8cac-e5531c4696bd	CNCF - KubeEdge: Improve the KubeEdge website	{"Improve the design and content of the kubeedge website."}	Improve the design and content of the kubeedge website.	{javascript,kubeedge}	2021	Term 2	https://github.com/kubeedge/website/issues/70	https://kubeedge.io/	300000	74
502	66c83532-84ba-473e-8764-6309481ff9ea	RISC-V Engagement Apprenticeship	{"Unlock your creative potential and make a global impact! RISC-V International invites you to join our vibrant, international community and play a pivotal role in boosting our online presence. RISC-V International is looking for an individual willing to help the community and technical program managers in a variety of tasks. A good candidate will be an independent self starter who can assist with graphic design, program management and video editing. Got a flair for podcasts or started one yourself? That's a fantastic bonus! See what its like to work in different departments of a Open Standard Project. Don't miss this extraordinary opportunity to grow your skills and make a difference with this 30 hour a week, remote role. Apply now!"}	Unlock your creative potential and make a global impact! RISC-V International invites you to join our vibrant, international community and play a pivotal role in boosting our online presence. RISC-V International is looking for an individual willing to help the community and technical program managers in a variety of tasks. A good candidate will be an independent self starter who can assist with graphic design, program management and video editing. Got a flair for podcasts or started one yourself? That's a fantastic bonus! See what its like to work in different departments of a Open Standard Project. Don't miss this extraordinary opportunity to grow your skills and make a difference with this 30 hour a week, remote role. Apply now!	{}	2023	Term 2	https://github.com/riscv	https://riscv.org/	1140000	171
23	31957b4f-1b8c-4877-aa94-b1c3997bc32b	All In	{"All In is a community dedicated to advancing diversity, equity, and inclusion within open source.\n\nAll In for Students is a paid, 12-month program for select college students from underrepresented backgrounds. During the program, students will receive professional development training, open source education, a structured 12-week open source project, and ultimately have the opportunity to join one of our corporate partners for a summer internship experience."}	All In is a community dedicated to advancing diversity, equity, and inclusion within open source.\n\nAll In for Students is a paid, 12-month program for select college students from underrepresented backgrounds. During the program, students will receive professional development training, open source education, a structured 12-week open source project, and ultimately have the opportunity to join one of our corporate partners for a summer internship experience.	{documentation,git}	2022	Term 1	https://github.com/AllInOpenSource/All-In	https://allinopensource.org/	6522500	16
190	446813c7-f2a6-4d81-ac64-ba407333fabe	CNCF - KubeEdge: Refactor the cloudStream to pass-through the request instead of parsing the web pat	{"Edgestream is used to handle the request from apiserver, then forward the request to edged through tunnel. We will find a way to pass-through the request, through the hijack stuff, instead of parsing the web path manually."}	Edgestream is used to handle the request from apiserver, then forward the request to edged through tunnel. We will find a way to pass-through the request, through the hijack stuff, instead of parsing the web path manually.	{go,kubernetes,kubeedge}	2021	Term 2	https://github.com/kubeedge/kubeedge/issues/2756	https://kubeedge.io/	300000	74
193	35b9d57a-fc2c-4b49-a5b3-9a5cf74af66c	CNCF - OpenEBS: Default Kyverno policies for OpenEBS	{"Kyverno is a Kubernetes native policy manager that can be used in place of PodSecurityPolicies. OpenEBS helm charts currently set up PSPs for many of its Storage engines. This project is to convert PSPs into corresponding Kyverno policies. The OpenEBS storage engines also uses a custom admission webhook validator. The scope of the project can extend to replacing the custom validators with Kyverno policies."}	Kyverno is a Kubernetes native policy manager that can be used in place of PodSecurityPolicies. OpenEBS helm charts currently set up PSPs for many of its Storage engines. This project is to convert PSPs into corresponding Kyverno policies. The OpenEBS storage engines also uses a custom admission webhook validator. The scope of the project can extend to replacing the custom validators with Kyverno policies.	{go,testing}	2021	Term 2	https://github.com/openebs/openebs/issues/3385	https://openebs.io/	300000	46
201	94852601-29c2-480a-9f26-8a2c850e1d66	COBOL Modernization	{"Using tried and true processes and tools, modernize a legacy COBOL application to take advantage of cloud, APIs and micro services.  Document the process and results, enabling others to modernize their legacy COBOL applications."}	Using tried and true processes and tools, modernize a legacy COBOL application to take advantage of cloud, APIs and micro services.  Document the process and results, enabling others to modernize their legacy COBOL applications.	{cobol}	2021	Term 2	https://github.com/openmainframeproject/wg-cobol	https://www.openmainframeproject.org/projects/cobol-working-group	300000	18
211	3d2a6deb-cfbc-423d-9bb4-98f72aab16f6	FINOS Marketing	{"FINOS (FinTech Open Source Foundation - part of the Linux Foundation) is looking for a summer marketing intern to assist with:\n- Marketing research (members, projects, BD)\n- Social media (research, operations)\n- Content for website (research, operations)\n- Podcast production (research, operations)"}	FINOS (FinTech Open Source Foundation - part of the Linux Foundation) is looking for a summer marketing intern to assist with:\n- Marketing research (members, projects, BD)\n- Social media (research, operations)\n- Content for website (research, operations)\n- Podcast production (research, operations)	{marketing,hubspot,wordpress}	2021	Term 3	https://github.com/finos/finos-marketing	https://www.finos.org	1473800	113
200	96d22656-ad60-4de2-9b1d-02c32173c40c	Mainframe Open Education Core Team	{"Help us get the Mainframe Open Education off the ground - Assist with helping identify topics, governance structure, establish processes, personas, etc."}	Help us get the Mainframe Open Education off the ground - Assist with helping identify topics, governance structure, establish processes, personas, etc.	{documentation}	2021	Term 2	https://github.com/openmainframeproject/omp-education	https://github.com/openmainframeproject/omp-education	601000	109
198	336820e5-b46e-4184-a7eb-8f0cb4dd18b4	CNCF - Tremor: Tremor Web Redesign - Make tremor’s web presence awesome	{"As an early stage project we’ve biased in favour of documenting the essentials and getting content in place as fast as possible. This has worked well but a side-effect is 3 or 4 different sources of content ( www, docs, rfcs and courseware ).\n\n  In concert with CNCF technical writing and learning best practices use your UX","web design and technical writing expertise for tremor where we as a team are unskilled - make our content awesome and the user experience exceptional.\n\n  These are some improvements we did think of, but these are neither complete nor required, more suggestions are welcome:\n\n- Unify the different content forms under a single consolidated theme and design\n- Ease of navigation ( breadcrumbs )\n- Preserve markdown for data entry ( we’re programmers ) and keep design separate ( we’re not designers and find this stuff super hard )\n- A clean, easy to navigate theme with a focus on user experience\n- Well integrated with our CI and doc generation tooling ( think gitops for docs and content )\n\n  This task would suit a candidate who is interested in `full stack` engineering and the complete software development lifecycle with a specific focus or interest in engineering documentation, web design and communicating well designed content to others with a good user experience - exploiting principles of good technical writing and web design of content management systems for technical content consumers."}	As an early stage project we’ve biased in favour of documenting the essentials and getting content in place as fast as possible. This has worked well but a side-effect is 3 or 4 different sources of content ( www, docs, rfcs and courseware ).\n\n  In concert with CNCF technical writing and learning best practices use your UX/web design and technical writing expertise for tremor where we as a team are unskilled - make our content awesome and the user experience exceptional.\n\n  These are some improvements we did think of, but these are neither complete nor required, more suggestions are welcome:\n\n- Unify the different content forms under a single consolidated theme and design\n- Ease of navigation ( breadcrumbs )\n- Preserve markdown for data entry ( we’re programmers ) and keep design separate ( we’re not designers and find this stuff super hard )\n- A clean, easy to navigate theme with a focus on user experience\n- Well integrated with our CI and doc generation tooling ( think gitops for docs and content )\n\n  This task would suit a candidate who is interested in `full stack` engineering and the complete software development lifecycle with a specific focus or interest in engineering documentation, web design and communicating well designed content to others with a good user experience - exploiting principles of good technical writing and web design of content management systems for technical content consumers.	{documentation}	2021	Term 2	https://github.com/tremor-rs/tremor-www-docs/issues/121	https://www.tremor.rs/	300000	20
197	76f2edd5-0f1c-4755-90a1-41ee09fb4d4d	CNCF - Vitess: Add testing framework for Django to ensure compatibility with Vitess	{"Vitess is a database clustering system for horizontal scaling of MySQL. One of the key goals of Vitess is to emulate MySQL behavior even while running multiple MySQL instances so that ORMs and frameworks work seamlessly. To this end, we would like to add a comprehensive test suite to ensure compatibility with [Django](https:","",www.djangoproject.com,") framework. The mentee would be introduced to the world of distributed databases and how everything comes together without the user realizing the difference. They would learn how to run Vitess and about comprehensive testing techniques."}	Vitess is a database clustering system for horizontal scaling of MySQL. One of the key goals of Vitess is to emulate MySQL behavior even while running multiple MySQL instances so that ORMs and frameworks work seamlessly. To this end, we would like to add a comprehensive test suite to ensure compatibility with [Django](https://www.djangoproject.com/) framework. The mentee would be introduced to the world of distributed databases and how everything comes together without the user realizing the difference. They would learn how to run Vitess and about comprehensive testing techniques.	{python,django,bash}	2021	Term 2	https://github.com/vitessio/vitess/issues/7905	https://vitess.io/	300000	107
504	24e1907b-c8e0-488c-aa24-1fbddb9df886	Hyperledger - Iroha 2 blockchain explorer update	{"The Iroha 2 blockchain explorer is a web-based UI that inspects the entire state of an Iroha v2-based blockchain. In the scope of this internship, the mentee shall learn how to construct a frontend service to a blockchain, organise the UI and cater to unique challenges associated with the specific needs of specific blockchain ledgers such as Hyperledger Iroha v2. \n\nLearning Objectives\n\nThe mentee shall learn how to:\n- Build a web-based UI in Vue.\n- Connect said Web-based UI to a backend service.\n- Implement UTF-8-based encoding of public keys (optional).\n- Learn about UI","UX challenges associated with making things work cross-platform, cross-form-factor.\n\nExpected Outcome\n- The state of the Iroha v2 blockchain explorer shall be improved."}	The Iroha 2 blockchain explorer is a web-based UI that inspects the entire state of an Iroha v2-based blockchain. In the scope of this internship, the mentee shall learn how to construct a frontend service to a blockchain, organise the UI and cater to unique challenges associated with the specific needs of specific blockchain ledgers such as Hyperledger Iroha v2. \n\nLearning Objectives\n\nThe mentee shall learn how to:\n- Build a web-based UI in Vue.\n- Connect said Web-based UI to a backend service.\n- Implement UTF-8-based encoding of public keys (optional).\n- Learn about UI/UX challenges associated with making things work cross-platform, cross-form-factor.\n\nExpected Outcome\n- The state of the Iroha v2 blockchain explorer shall be improved.	{rust,javascript,typescript}	2023	Term 2	https://github.com/hyperledger/iroha	https://wiki.hyperledger.org/display/INTERN/Iroha+2%3A+blockchain+explorer+update	300000	1
204	84227d0f-1183-44f5-af4c-4c4138e40e70	Hyperledger - Visualization and Analysis of Cross-chain Transactions	{"The emergence of blockchain interoperability is reducing the risk of investing in blockchain by avoiding vendor lock-in, leveraging interoperation with off-chain systems, and providing a truly open ecosystem, enabling a network of blockchains.  However, there are very limited efforts on the value","semantic level of interoperability - which soon will become a bottleneck for interoperability.\n\nIn particular, the human-challenges related to the adoption of such technology have received little attention, with few significant contributions to date. Yet, real innovation can be achieved only by adopting a holistic approach. Designing for blockchain interoperability poses several socio-technical challenges thus offering a unique opportunity to link the underlying technology with human experience and values.\n\nThis project aims to advance the state of the art on understanding blockchain interoperability on the value","semantic layers - In particular, how can someone conjugate human-centric design, service design, and sustainability to blockchain interoperability? How can one make informed choices on the best cross-chain synergies? How can someone visualize cross-blockchain transactions?\n\nExpected Outcome\n1 ) Implementation, testing, and documenting the set of technological artifacts required to answer the research questions\n2) Scientific paper (or technical report) on the achieved results, that can be used to disseminate the knowledge created on this internship"}	The emergence of blockchain interoperability is reducing the risk of investing in blockchain by avoiding vendor lock-in, leveraging interoperation with off-chain systems, and providing a truly open ecosystem, enabling a network of blockchains.  However, there are very limited efforts on the value/semantic level of interoperability - which soon will become a bottleneck for interoperability.\n\nIn particular, the human-challenges related to the adoption of such technology have received little attention, with few significant contributions to date. Yet, real innovation can be achieved only by adopting a holistic approach. Designing for blockchain interoperability poses several socio-technical challenges thus offering a unique opportunity to link the underlying technology with human experience and values.\n\nThis project aims to advance the state of the art on understanding blockchain interoperability on the value/semantic layers - In particular, how can someone conjugate human-centric design, service design, and sustainability to blockchain interoperability? How can one make informed choices on the best cross-chain synergies? How can someone visualize cross-blockchain transactions?\n\nExpected Outcome\n1 ) Implementation, testing, and documenting the set of technological artifacts required to answer the research questions\n2) Scientific paper (or technical report) on the achieved results, that can be used to disseminate the knowledge created on this internship	{blockchain,angular,react,typescript}	2021	Term 2	https://github.com/hyperledger/cactus	https://wiki.hyperledger.org/display/INTERN/Visualization+and+Analysis+of+Cross-chain+Transactions	300000	1
206	eb96b91a-85c5-4ee5-aae4-46c9f10981e0	CNCF - TiKV: Implement Node client	{"TiKV is a distributed KV database. It support using clients in Rust, Golang, Java, C++ and Python, and the Node client is the last missing piece. This program is going to implement Node client on top of Rust client just like Python client and C++ client."}	TiKV is a distributed KV database. It support using clients in Rust, Golang, Java, C++ and Python, and the Node client is the last missing piece. This program is going to implement Node client on top of Rust client just like Python client and C++ client.	{javascript,typescript,rust}	2021	Term 2	https://github.com/tikv/tikv/issues/10054	https://tikv.org/	600000	90
214	e77be644-affc-4096-b27b-261de2c11f2a	mixcore	{"Fully Open Source UI Tools to create web apps. n nCMS and Dashboards built on top of .Net Core, Angular.JS and Bootstrap. n nCreating your web from scratch with the dedicated experience development team can be very expensive. Using our solutions you don't have to worry about tech."}	Fully Open Source UI Tools to create web apps. n nCMS and Dashboards built on top of .Net Core, Angular.JS and Bootstrap. n nCreating your web from scratch with the dedicated experience development team can be very expensive. Using our solutions you don't have to worry about tech.	{}	2021	Term 3	https://github.com/mixcore/mix.core	https://mixcore.org/	0	114
473	4df8fab8-e11a-4877-8140-c3633099ea24	Alpha-Omega: Security Researching	{"To accelerate the mission of the Omega in Alpha-Omega project, four entry-level individual contributors (IC) are being requested for a Summer internship type program. Two of these individuals will assist with the software development and research on the Omega toolchain, led by Yesenia. While the remaining two will assist with developing automated remediation of security vulnerabilities, and open source security research, led by Jonathan. The overall goal will be to provide a 12-week program for the four ICs during the Summer semester term for undergraduate and graduate students. The targeted students will be newcomers to the open source community, software development, or security researcher assisting to expand knowledge and talent to underrepresented majorities, DEI groups, and increase the diverse talent pool for future employers."}	To accelerate the mission of the Omega in Alpha-Omega project, four entry-level individual contributors (IC) are being requested for a Summer internship type program. Two of these individuals will assist with the software development and research on the Omega toolchain, led by Yesenia. While the remaining two will assist with developing automated remediation of security vulnerabilities, and open source security research, led by Jonathan. The overall goal will be to provide a 12-week program for the four ICs during the Summer semester term for undergraduate and graduate students. The targeted students will be newcomers to the open source community, software development, or security researcher assisting to expand knowledge and talent to underrepresented majorities, DEI groups, and increase the diverse talent pool for future employers.	{api,azure,compiler,java,python,docker,security,git}	2023	Term 2	https://github.com/ossf/alpha-omega	https://openssf.org/community/alpha-omega/	0	167
208	d8a154c6-41fb-4733-b3c8-df37796e7fa3	Hyperledger - The Use of NLP and DLT to Enable the Digitalization of Telecom Roaming Agreements	{"The project looks at how to facilitate the process of Telecom roaming agreements drafting and negotiation. The project looks at first constructing a library of drafting articles with a set of variables that will be extracted from the available templates and previous roaming agreements using Natural Language Processing process NLP. The second part will be towards translating the drafting and negotiation process as a chaincode on Blockchain to digitalize the process and provide a maintainable and actionable copy of the agreement. The project focus on how to be able to digitalize the legal process and make it automated through smart contracts to be more efficient and less error prone.\n\nThe expected outcome of this project, would include:\n1. Building a drafting library and an NPL model that will include a digital version of the most relevant and important articles in a the GSMA AA.12, AA.13 and AA-14 roaming agreement templates.\n2. Developing a chaincode that will map all steps of the drafting and negotiation business processes for roaming agreements.\n3. Building a PoC for UI to handle the agreement construction\n4. Writing a solution document summarizing the implementation details"}	The project looks at how to facilitate the process of Telecom roaming agreements drafting and negotiation. The project looks at first constructing a library of drafting articles with a set of variables that will be extracted from the available templates and previous roaming agreements using Natural Language Processing process NLP. The second part will be towards translating the drafting and negotiation process as a chaincode on Blockchain to digitalize the process and provide a maintainable and actionable copy of the agreement. The project focus on how to be able to digitalize the legal process and make it automated through smart contracts to be more efficient and less error prone.\n\nThe expected outcome of this project, would include:\n1. Building a drafting library and an NPL model that will include a digital version of the most relevant and important articles in a the GSMA AA.12, AA.13 and AA-14 roaming agreement templates.\n2. Developing a chaincode that will map all steps of the drafting and negotiation business processes for roaming agreements.\n3. Building a PoC for UI to handle the agreement construction\n4. Writing a solution document summarizing the implementation details	{fabric,javascript,go,telecom}	2021	Term 2	https://github.com/hyperledger/fabric	https://wiki.hyperledger.org/display/INTERN/The+Use+of+NLP+and+DLT+to+Enable+the+Digitalization+of+Telecom+Roaming+Agreements	515900	1
212	87b2f82c-0783-4962-8931-70e76fd4e93d	CNCF - Tremor: Add gRPC client and server support to tremor	{"gRPC is an industry-standard API abstraction and runtime. Currently, tremor supports interfacing with the outside world over WebSockets, HTTP","1.1, or target-specific connectors. Adding support for gRPC will allow generalizing a whole lot of client and server connections and making interfacing with other cloud-native applications easier. The goal of this project is to add support for such generic gRPC based services."}	gRPC is an industry-standard API abstraction and runtime. Currently, tremor supports interfacing with the outside world over WebSockets, HTTP/1.1, or target-specific connectors. Adding support for gRPC will allow generalizing a whole lot of client and server connections and making interfacing with other cloud-native applications easier. The goal of this project is to add support for such generic gRPC based services.	{rust,linux,http}	2021	Term 3	https://github.com/tremor-rs/tremor-runtime/issues/790	https://www.tremor.rs/	300000	20
213	b90f7174-fc53-40bc-b9e2-9905f88c38ff	CNCF - Tremor: Add plugin support for tremor (PDK)	{"The PDK or (plugin development kit) aims to allow loading artifacts into tremor at runtime instead of requiring a full recompilation. This streamlines the development process of extending tremor with custom functionality. The goal is to allow live loading of a number of artifacts so start with: Pipeline Operators, Codecs, Pre- and Postprocessors, Custom Functions."}	The PDK or (plugin development kit) aims to allow loading artifacts into tremor at runtime instead of requiring a full recompilation. This streamlines the development process of extending tremor with custom functionality. The goal is to allow live loading of a number of artifacts so start with: Pipeline Operators, Codecs, Pre- and Postprocessors, Custom Functions.	{rust,linux}	2021	Term 3	https://github.com/tremor-rs/tremor-runtime/issues/791	https://www.tremor.rs/	420000	20
237	1c387c2c-8c17-49e2-82b2-429c012472e6	CNCF - Kubevela: Support remote Terraform HCL (Git repo or ConfigMap) in Terraform Controller	{"Currently, Terraform Controller supports inline HCL, but community users normally already stored Terraform HCLs remotely (in Git repo or ConfigMap). Terraform Controller should support remote HCLs."}	Currently, Terraform Controller supports inline HCL, but community users normally already stored Terraform HCLs remotely (in Git repo or ConfigMap). Terraform Controller should support remote HCLs.	{go,terraform}	2021	Term 3	https://github.com/oam-dev/terraform-controller/issues/46	https://kubevela.io/	0	117
217	8f69e012-08d0-4e2b-baa7-9143b5f98823	Linux Kernel: Mining for Maintainers	{"Jonathan Corbet identified in his article MAINTAINERS truth and fiction [https:","",lwn.net,Articles,842415,"] that about 2,800 files in the kernel repository have no dedicated maintainer in the MAINTAINERS file (see https:","",lwn.net,Articles,842606," for the full list of files). Jonathan Corbet sets out the call for action:\n\n“the vast majority are header files under include",", most of which probably do have maintainers and should be added to the appropriate entries.”\n\nThe task in this mentorship is to follow this call for action and add the header files under include"," to the appropriate entries."}	Jonathan Corbet identified in his article MAINTAINERS truth and fiction [https://lwn.net/Articles/842415/] that about 2,800 files in the kernel repository have no dedicated maintainer in the MAINTAINERS file (see https://lwn.net/Articles/842606/ for the full list of files). Jonathan Corbet sets out the call for action:\n\n“the vast majority are header files under include/, most of which probably do have maintainers and should be added to the appropriate entries.”\n\nThe task in this mentorship is to follow this call for action and add the header files under include/ to the appropriate entries.	{linux,bash,perl,python}	2021	Term 2	https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git		0	116
469	fb9e1ba6-d6ed-40b5-82b5-ee1089ef050a	RISC-V Mentorship: Porting Spidermonkey to RISC-V	{"The RISC-V Mentorship Program enables one or more 12-week internship-style projects per session, funded by RISC-V, to match mentors","project leaders together with mentees","interns . Mentees are guided through a series of milestones by one or more project mentors, with whom the mentees meet on a weekly basis.\n\nSpidermonkey is the JavaScript Engine inside Firefox. It has JIT compilers for generating native binary codes on the fly. This project aims to porting Spidermonkey to RV64GC platform.\n\nBasic knowledge of compilers and language virtual machines are needed.\n \nDeliverables:\n- Cross-compile Spidermonkey to RV64GC Linux (Fedora) platform.\n- Patches that let Spidermonkey running on RV64GC Linux under interpreter mode.\n- Porting the baseline compilers so that Spidermonkey can enable at least one JIT compiler on RV64GC platform\n- Submit all patches to upstream for code review (merging into upstream is encouraged but not required)\n \nAcceptance criteria:\n- Pass the regression tests in the Spidermonkey.\n- Get performance data by running SunSpider, Octane and Kraken benchmarks on the RISC-V machine (RV64GC) or software simulator (QEMU)."}	The RISC-V Mentorship Program enables one or more 12-week internship-style projects per session, funded by RISC-V, to match mentors/project leaders together with mentees/interns . Mentees are guided through a series of milestones by one or more project mentors, with whom the mentees meet on a weekly basis.\n\nSpidermonkey is the JavaScript Engine inside Firefox. It has JIT compilers for generating native binary codes on the fly. This project aims to porting Spidermonkey to RV64GC platform.\n\nBasic knowledge of compilers and language virtual machines are needed.\n \nDeliverables:\n- Cross-compile Spidermonkey to RV64GC Linux (Fedora) platform.\n- Patches that let Spidermonkey running on RV64GC Linux under interpreter mode.\n- Porting the baseline compilers so that Spidermonkey can enable at least one JIT compiler on RV64GC platform\n- Submit all patches to upstream for code review (merging into upstream is encouraged but not required)\n \nAcceptance criteria:\n- Pass the regression tests in the Spidermonkey.\n- Get performance data by running SunSpider, Octane and Kraken benchmarks on the RISC-V machine (RV64GC) or software simulator (QEMU).	{spidermonkey}	2021	Term 2	https://github.com/riscv/	https://riscv.org/community/risc-v-mentorship-program/	600000	171
222	57c710e5-4498-48f1-a2ab-b920183b4982	CNCF - Vitess: Add support for Upgrade/Downgrade Testing	{"Currently Vitess has a rich set of functional tests that are run as part of every commit to catch regressions early. However, they are not sufficient to assess the quality of the product for production rollout. We currently do not test for incompatibilities introduced via upgrade, and ensuring that users can downgrade one level if they need to backout of a failed upgrade. The scenarios will need to be written down, and then tests can be written using GitHub actions:\n  - Document Supported Upgrade","Downgrade Scenario"}	Currently Vitess has a rich set of functional tests that are run as part of every commit to catch regressions early. However, they are not sufficient to assess the quality of the product for production rollout. We currently do not test for incompatibilities introduced via upgrade, and ensuring that users can downgrade one level if they need to backout of a failed upgrade. The scenarios will need to be written down, and then tests can be written using GitHub actions:\n  - Document Supported Upgrade/Downgrade Scenario	{go,docker}	2021	Term 3	https://github.com/vitessio/vitess/issues/4989	https://vitess.io/	300000	107
227	6f1497c6-f583-462e-bd67-b3f0bb3df5cb	CNCF - LitmusChaos: Develop E2E dashboard with CI/CD pipeline details and enhance litmus e2e website	{"his project aims to build an E2E Web dashboard that will display the CI","CD pipeline details of [scheduled](https:","",litmuschaos.github.io,litmus-e2e,generic-pipeline,pipeline-runs,"pod-level-run.html) and [manual](https:","",github.com,litmuschaos,litmus-e2e,tree,master,.github,"workflows) runs. It should contain all the litmus backend and portal pipelines and can be easily switched or add more pipelines. Currently, [these](https:","",github.com,litmuschaos,litmus-e2e,tree,master,.github,"workflows) are the pipeline present in the e2e."}	his project aims to build an E2E Web dashboard that will display the CI/CD pipeline details of [scheduled](https://litmuschaos.github.io/litmus-e2e/generic-pipeline/pipeline-runs/pod-level-run.html) and [manual](https://github.com/litmuschaos/litmus-e2e/tree/master/.github/workflows) runs. It should contain all the litmus backend and portal pipelines and can be easily switched or add more pipelines. Currently, [these](https://github.com/litmuschaos/litmus-e2e/tree/master/.github/workflows) are the pipeline present in the e2e.	{go,javascript,react}	2021	Term 3	https://github.com/litmuschaos/litmus/issues/3112	https://litmuschaos.io/	300000	13
233	e2496a2f-3068-4f1f-b22a-850f1081d126	CNCF - Tremor: AWS (S3) connectors	{"Tremor supports a number of other systems to connect one, yet one of the most widespread API's in the cloud world (S3) isn't yet supported. This is a chance to shine and build something many users will enjoy."}	Tremor supports a number of other systems to connect one, yet one of the most widespread API's in the cloud world (S3) isn't yet supported. This is a chance to shine and build something many users will enjoy.	{rust,linux}	2021	Term 3	https://github.com/tremor-rs/tremor-runtime/issues/1176	https://www.tremor.rs/	0	20
228	8e429646-09ee-4579-8ed8-0acae0a53d9c	CNCF - Tremor: Solidify and generalize error handling code in the runtime	{"The runtimes error handling code stems from ancient times where rust was younger, and we were more naive. As such, it is more grown than designed in many places. With the knowledge of the past three years and stabilization of the codebase, now is a good time to polish it and make errors an exciting user experience."}	The runtimes error handling code stems from ancient times where rust was younger, and we were more naive. As such, it is more grown than designed in many places. With the knowledge of the past three years and stabilization of the codebase, now is a good time to polish it and make errors an exciting user experience.	{rust}	2021	Term 3	https://github.com/tremor-rs/tremor-runtime/issues/1175	https://www.tremor.rs/	300000	20
225	bc94a28d-ca38-4df1-9a19-7bee42efa08c	CNCF - Vitess: Add complete parsing support for MySQL constructs	{"Vitess is a database clustering system for horizontal scaling of MySQL. One of the key goals of Vitess is to emulate MySQL behavior even while running multiple MySQL instances so that ORMs and frameworks work seamlessly. Vitess has its own in-built SQL-parser which it uses to understand the query and represent as structs for further processing. As of now, a lot of MySQL constructs are not parsed and result in syntax errors. For example, we do not have complete support to parse [partition constructs](https:","",dev.mysql.com,doc,refman,5.7,en,"partitioning-overview.html). Parsing for a lot of the newer features in MySQL 8.0 is also missing. The task of the mentee would be to add parsing support for such constructs."}	Vitess is a database clustering system for horizontal scaling of MySQL. One of the key goals of Vitess is to emulate MySQL behavior even while running multiple MySQL instances so that ORMs and frameworks work seamlessly. Vitess has its own in-built SQL-parser which it uses to understand the query and represent as structs for further processing. As of now, a lot of MySQL constructs are not parsed and result in syntax errors. For example, we do not have complete support to parse [partition constructs](https://dev.mysql.com/doc/refman/5.7/en/partitioning-overview.html). Parsing for a lot of the newer features in MySQL 8.0 is also missing. The task of the mentee would be to add parsing support for such constructs.	{go,sql,yacc}	2021	Term 3	https://github.com/vitessio/vitess/issues/8604	https://vitess.io/	300000	107
232	90e18b5c-0ac5-4b24-aa06-ca7affc4da7e	CNCF - WasmEdge: Support Wasm-C-API proposal	{"he wasm-c-api proposal provides the C and C++ API for WASM runtimes. Even though WasmEdge already provided the C API, it's proper to implement the wasm-c-API proposal for the general C","C++ API. In the current status, we've already implemented the non-runtime data structures on the branch. Then, we need to finish the runtime implementation."}	he wasm-c-api proposal provides the C and C++ API for WASM runtimes. Even though WasmEdge already provided the C API, it's proper to implement the wasm-c-API proposal for the general C/C++ API. In the current status, we've already implemented the non-runtime data structures on the branch. Then, we need to finish the runtime implementation.	{rust}	2021	Term 3	https://github.com/WasmEdge/WasmEdge/issues/306	https://wasmedge.org/	300000	31
236	45d3c78d-79bd-4bdf-9fb2-1d136aee2367	CNCF - etcd: Etcd.io Docs/SEO Improvement Plan Continuation	{"This project is the continuation of the [etcd.io website](https:","","etcd.io) docs improvement work. While the relocation of the primary documentation, SEO, and site look and feel work has already been done, there is still more work to be done to implement the new information architecture that has been proposed. This includes working with the mentors and project maintainers to write new pages, adapt existing ones, and write blog posts."}	This project is the continuation of the [etcd.io website](https://etcd.io) docs improvement work. While the relocation of the primary documentation, SEO, and site look and feel work has already been done, there is still more work to be done to implement the new information architecture that has been proposed. This includes working with the mentors and project maintainers to write new pages, adapt existing ones, and write blog posts.	{documentation,git}	2021	Term 3	https://github.com/etcd-io/website/issues/65	etcd.id	300000	118
234	76c1d526-6369-4094-babf-90cf6af1c187	CNCF - Kubevela: Build Gitops continuous deployment tools on kubevela	{"Kubevela is like Lego, you can build any platform you need based on kubevela. And GitOps is very popular and user-friendly. In this project, we will build Gitops continuous deployment tools on kubevela"}	Kubevela is like Lego, you can build any platform you need based on kubevela. And GitOps is very popular and user-friendly. In this project, we will build Gitops continuous deployment tools on kubevela	{go,kubernetes}	2021	Term 3	https://github.com/oam-dev/kubevela/issues/2062	https://kubevela.io/	0	117
235	15bb5d46-4d54-4a81-a5e4-016450cc780c	CNCF - Kubevela: Integration with developing time to provide consistent experience	{"Users can use KubeVela to do application delivery and management. In this project, we hope to integrate KubeVela with developing time. So developers can have a consitent experience between local development and production deploy. There're multiple developing tools such as Tilt or Nocalhost, both of them can integrate with KubeVela by supporting KubeVela Application YAML."}	Users can use KubeVela to do application delivery and management. In this project, we hope to integrate KubeVela with developing time. So developers can have a consitent experience between local development and production deploy. There're multiple developing tools such as Tilt or Nocalhost, both of them can integrate with KubeVela by supporting KubeVela Application YAML.	{go,kubernetes}	2021	Term 3	https://github.com/oam-dev/kubevela/issues/795	https://kubevela.io/	0	117
239	8b8276b1-cb02-48e5-885b-31c59592ee1b	CNCF - Kyverno: Scalability testing for Kyverno	{"Define and execute scalability tests for Kyverno on large Kubernetes clusters with several namespaces and resources. The candidate has to propose and execute a performance test plan and help optimize resource usage of Kyverno for different loads for large Kubernetes clusters."}	Define and execute scalability tests for Kyverno on large Kubernetes clusters with several namespaces and resources. The candidate has to propose and execute a performance test plan and help optimize resource usage of Kyverno for different loads for large Kubernetes clusters.	{go,testing,kubernetes}	2021	Term 3	https://github.com/kyverno/kyverno/issues/2248	https://kyverno.io	300000	17
242	8e9537fe-fdea-4f92-941d-e86d2fcb48ba	CNCF - LitmusChaos: Develop/Enhance E2E test-cases for ChaosCenter	{"This project aims to develop","enhance the E2E test cases for ChaosCenter. The ChaosCenter is a single source of truth to control all the different Chaos Activities happening around Litmus. From the ChaosCenter you get the freedom to manage every single part of Litmus and shape your workflows exactly the way you want them."}	This project aims to develop/enhance the E2E test cases for ChaosCenter. The ChaosCenter is a single source of truth to control all the different Chaos Activities happening around Litmus. From the ChaosCenter you get the freedom to manage every single part of Litmus and shape your workflows exactly the way you want them.	{go,javascript,kubernetes}	2021	Term 3	https://github.com/litmuschaos/litmus/issues/3114	https://litmuschaos.io/	300000	13
248	0e34db3f-99c3-471d-a114-e3c89f57eab5	CNCF - Kyverno: Extend Kyverno test command to cover generate policies & improve test coverage	{"Extend the Kyverno CLI to cover generate policies and improve tests coverage for Kyverno. Based on the test results, the candidate has to add more unit","E2E tests."}	Extend the Kyverno CLI to cover generate policies and improve tests coverage for Kyverno. Based on the test results, the candidate has to add more unit/E2E tests.	{go,testing}	2021	Term 3	https://github.com/kyverno/kyverno/issues/2249	https://kyverno.io	300000	17
525	06774504-da91-469e-89f9-14fb18b6e0d8	CNCF - Notary: Design and implement the new Notary website	{"Design the new Notary website using the Figma tool and develop it based on the design layout. We redesigned the Notary website and finished the first phase development work with CNCF employee @thisisobate in [#PR 139](https:","",github.com,notaryproject,notaryproject.dev,pull,"139). This project is to continue to design and implement the new Notary website and ensure deliver a developer-friendly experience.\n- Expected Outcome: \n   - Design and implement the Adopters page\n   - Redesign a Community page\n   - Improve the landing page design; add an installation section with a terminal effect design\n   - Support mobile responsive design\n   - Add a pop-up window on the landing page to tell users how to join the community\n   - Add Algolia search for the website\n   - Design and implement a video page to list Youtube videos\n   - Refine the content on the website\n   - Add Broken link check to Netlify CI"}	Design the new Notary website using the Figma tool and develop it based on the design layout. We redesigned the Notary website and finished the first phase development work with CNCF employee @thisisobate in [#PR 139](https://github.com/notaryproject/notaryproject.dev/pull/139). This project is to continue to design and implement the new Notary website and ensure deliver a developer-friendly experience.\n- Expected Outcome: \n   - Design and implement the Adopters page\n   - Redesign a Community page\n   - Improve the landing page design; add an installation section with a terminal effect design\n   - Support mobile responsive design\n   - Add a pop-up window on the landing page to tell users how to join the community\n   - Add Algolia search for the website\n   - Design and implement a video page to list Youtube videos\n   - Refine the content on the website\n   - Add Broken link check to Netlify CI	{html,hugo,css,javascript}	2023	Term 2	https://github.com/notaryproject/notaryproject.dev/issues/194	https://notaryproject.dev	300000	155
220	64e3add3-060b-4ffa-9408-1289e2f2fdc5	CNCF - OpenEBS: Enhance OpenEBS CLI with a sub-command to upgrade Jiva Volumes	{"After upgrading the OpenEBS control plane, user","admin is required to upgrade the Jiva Volumes by launching a Kubernetes Job. This enhancement request is to improve the existing OpenEBS CLI jiva sub-commands that will help users with required information to create Kubernetes Jobs, as well as to add a sub-command that will automatically launch the upgrade jobs."}	After upgrading the OpenEBS control plane, user/admin is required to upgrade the Jiva Volumes by launching a Kubernetes Job. This enhancement request is to improve the existing OpenEBS CLI jiva sub-commands that will help users with required information to create Kubernetes Jobs, as well as to add a sub-command that will automatically launch the upgrade jobs.	{go,kubernetes,openebs}	2021	Term 3	https://github.com/openebs/openebs/issues/81	https://openebs.io/	300000	46
249	6db29eff-87ef-4eb7-93cb-de7b560d478d	Open@RIT - University OSPO Methodologies Research	{"This program will be building upon the work detailed in \n\nhttps:","",mentorship.lfx.linuxfoundation.org,project,"6bfb3b5f-0819-482f-b90a-a5ebf458c69b\n\nWith the completion of our first version of the playbook, we have a structure and detailed consulting metholodologies section.\n\nFor the coming semester, the work will require the mentee to begin to construct and visualize examples for each step in our methodologies in order to help explain each step of our process using real-world, things that our team has done. This work will include observing and participating in our fellowship consulting process to understand how other parts of our playbook connect and integrate with our developed methodologies.\n\nThe mentee will also continue work on finalizing the visual design, as much of this document is just copy at this point. The mentee will finally draft a number of articles that we will publish at places such as opensource.com (where Open@RIT has published a few articles so far, and has more in their questions to be published, or in draft on our side) to spread knowledge of the playbook and help ensure that those who are interested in it, end up finding it."}	This program will be building upon the work detailed in \n\nhttps://mentorship.lfx.linuxfoundation.org/project/6bfb3b5f-0819-482f-b90a-a5ebf458c69b\n\nWith the completion of our first version of the playbook, we have a structure and detailed consulting metholodologies section.\n\nFor the coming semester, the work will require the mentee to begin to construct and visualize examples for each step in our methodologies in order to help explain each step of our process using real-world, things that our team has done. This work will include observing and participating in our fellowship consulting process to understand how other parts of our playbook connect and integrate with our developed methodologies.\n\nThe mentee will also continue work on finalizing the visual design, as much of this document is just copy at this point. The mentee will finally draft a number of articles that we will publish at places such as opensource.com (where Open@RIT has published a few articles so far, and has more in their questions to be published, or in draft on our side) to spread knowledge of the playbook and help ensure that those who are interested in it, end up finding it.	{documentation,design}	2022	Term 1	https://opensource.ieee.org/rit/ospo-playbook		0	97
254	4fe23ece-633c-488a-93a4-0501cae5a6f5	CNCF - WasmEdge: Improving the performance of running miniruby	{"Description: WasmEdge is a lightweight, high-performance, and extensible WebAssembly runtime for cloud native, edge, and decentralized applications. WasmEdge is an official sandbox project hosted by the CNCF. WasmEdge is designed for the general purpose wasm runtime. However, when running rustpython, we found the performance is worse than other runtimes such as wasmtime, even after using the ahead-of-time compilation."}	Description: WasmEdge is a lightweight, high-performance, and extensible WebAssembly runtime for cloud native, edge, and decentralized applications. WasmEdge is an official sandbox project hosted by the CNCF. WasmEdge is designed for the general purpose wasm runtime. However, when running rustpython, we found the performance is worse than other runtimes such as wasmtime, even after using the ahead-of-time compilation.	{}	2022	Term 1	https://github.com/WasmEdge/WasmEdge/issues/1061	https://wasmedge.org/	300000	31
263	81f0d863-30ca-43b5-a368-fd422ec681a6	CNCF - KubeArmor: Using mutating webhooks for applying pod/container kubearmor annotations	{"KubeArmor is Cloud Native Runtime Security Enforcement System that restricts the behavior of containers and nodes at the system level using Linux Kernel LSMs (Linux Security Modules) and eBPF. KubeArmor applies container","pod annotations to achieve some of the goals. Currently, these annotations are applied using k8s rolling update feature that results in pod getting terminated and recreated. Mutating webhooks could achieve the same functionality in a much cleaner way without having to terminate the pods."}	KubeArmor is Cloud Native Runtime Security Enforcement System that restricts the behavior of containers and nodes at the system level using Linux Kernel LSMs (Linux Security Modules) and eBPF. KubeArmor applies container/pod annotations to achieve some of the goals. Currently, these annotations are applied using k8s rolling update feature that results in pod getting terminated and recreated. Mutating webhooks could achieve the same functionality in a much cleaner way without having to terminate the pods.	{go,kubernetes}	2022	Term 1	https://github.com/kubearmor/KubeArmor/issues/360	https://kubearmor.com/	300000	9
253	0afab514-1094-4363-b700-d5116b2d62de	CNCF - KubeEdge: Updating the kubeedge docs	{"Description: Now we have lots of new features, we need add more docs for them."}	Description: Now we have lots of new features, we need add more docs for them.	{kubernetes,kubeedge}	2022	Term 1	https://github.com/kubeedge/website/issues/189	https://kubeedge.io/en/	300000	74
223	ea9d68a3-7201-4395-a6bc-66ccd939f9ee	CNCF - Thanos: Add groupcache (and improve) backend for the caching bucket in Thanos Store	{"This project is about implementing the [groupcache](https:","",github.com,golang,"groupcache) back-end for the caching bucket. Caching bucket is a generic mechanism for caching requests to remote object storage. All of the other caching mechanisms currently suffer from the [cache stampede](https:","",en.wikipedia.org,wiki,"Cache_stampede) problem and it is impossible to \\"properly\\" solve this problem with them. This is where `groupcache` comes in. Some work has already been [done](https:","",github.com,GiedriusS,thanos,commit,"d269ce25b744b02c6d4d99f0e2e72af812e45f37) so you will have to something work off of. `Groupcache` also needs some improvements with regards to fetching multiple keys at once to improve the performance even more. In the end your work will make not just Thanos faster but also cheaper because less requests will need to be made to the remote object storage."}	This project is about implementing the [groupcache](https://github.com/golang/groupcache) back-end for the caching bucket. Caching bucket is a generic mechanism for caching requests to remote object storage. All of the other caching mechanisms currently suffer from the [cache stampede](https://en.wikipedia.org/wiki/Cache_stampede) problem and it is impossible to "properly" solve this problem with them. This is where `groupcache` comes in. Some work has already been [done](https://github.com/GiedriusS/thanos/commit/d269ce25b744b02c6d4d99f0e2e72af812e45f37) so you will have to something work off of. `Groupcache` also needs some improvements with regards to fetching multiple keys at once to improve the performance even more. In the end your work will make not just Thanos faster but also cheaper because less requests will need to be made to the remote object storage.	{go,yaml}	2021	Term 3	https://github.com/thanos-io/thanos/issues/2962	https://thanos.io/	300000	26
261	dfd7b7d9-7f80-48f8-91f4-bada2ceb416a	Mailu	{"Mailu is a simple yet full-featured mail server as a set of Docker images. It is free software (both as in free beer and as in free speech), open to suggestions and external contributions. The project aims at providing people with an easily setup, easily maintained and full-featured mail server while not shipping proprietary software nor unrelated features often found in popular groupware.\n\nMain features include:\n  - Standard email server, IMAP and IMAP+, SMTP and Submission\n  - Advanced email features, aliases, domain aliases, custom routing\n  - Web access, multiple Webmails and administration interface\n  - User features, aliases, auto-reply, auto-forward, fetched accounts\n  - Admin features, global admins, announcements, per-domain delegation, quotas\n  - Security, enforced TLS, Letsencrypt!, outgoing DKIM, anti-virus scanner\n- Antispam, auto-learn, greylisting, DMARC and SPF\n- Freedom, all FOSS components, no tracker included"}	Mailu is a simple yet full-featured mail server as a set of Docker images. It is free software (both as in free beer and as in free speech), open to suggestions and external contributions. The project aims at providing people with an easily setup, easily maintained and full-featured mail server while not shipping proprietary software nor unrelated features often found in popular groupware.\n\nMain features include:\n  - Standard email server, IMAP and IMAP+, SMTP and Submission\n  - Advanced email features, aliases, domain aliases, custom routing\n  - Web access, multiple Webmails and administration interface\n  - User features, aliases, auto-reply, auto-forward, fetched accounts\n  - Admin features, global admins, announcements, per-domain delegation, quotas\n  - Security, enforced TLS, Letsencrypt!, outgoing DKIM, anti-virus scanner\n- Antispam, auto-learn, greylisting, DMARC and SPF\n- Freedom, all FOSS components, no tracker included	{}	2022	Term 1	https://github.com/Mailu	https://mailu.io	164600	128
226	4ac88eda-21e8-4b15-97dd-1cd3caa045c5	CNCF - Kubevela: Building An Machine Learning Platform on KubeVela	{"Data scientists need a ML platform to develop, test, and deploy ML models easily. In this project, we will design and build a self-service ML platform on top of KubeVela. We will use KubeVela to provide high level workflow and APIs to glue and simplify deployment pipelines. We will also use Cloud resources to support deployment and operations tasks like domain routing, monitoring, health checking, etc."}	Data scientists need a ML platform to develop, test, and deploy ML models easily. In this project, we will design and build a self-service ML platform on top of KubeVela. We will use KubeVela to provide high level workflow and APIs to glue and simplify deployment pipelines. We will also use Cloud resources to support deployment and operations tasks like domain routing, monitoring, health checking, etc.	{go,kubernetes}	2021	Term 3	https://github.com/oam-dev/kubevela/issues/2061	https://kubevela.io/	300000	117
266	2a182d3b-f5cd-4ca7-9ede-4e8b5158c6a2	CNCF - Kubevela: Management of Terraform state	{"Description: To some extent, Terraform state is the most essential component for cloud resources provisioned by Terraform Controller. We need to better manage the state."}	Description: To some extent, Terraform state is the most essential component for cloud resources provisioned by Terraform Controller. We need to better manage the state.	{go,terraform}	2022	Term 1	https://github.com/oam-dev/terraform-controller/issues/239	https://kubevela.io/	300000	117
262	b86dbdc2-6bef-41f1-9fc4-c0a354ef3702	CNCF - LitmusChaos: Develop new feature and add integration tests for LitmusCTL	{"LitmusChaos is an open source Chaos Engineering platform that enables teams to identify weaknesses & potential outages in infrastructures by inducing chaos tests in a controlled way. This project aims to develop new commands","features for litmusctl along with integration tests for it."}	LitmusChaos is an open source Chaos Engineering platform that enables teams to identify weaknesses & potential outages in infrastructures by inducing chaos tests in a controlled way. This project aims to develop new commands/features for litmusctl along with integration tests for it.	{go,kubernetes,cli}	2022	Term 1	https://github.com/litmuschaos/litmus/issues/3440	https://litmuschaos.io/	300000	13
259	45320aba-558a-4660-ab0a-0770f06b856a	Compressed instructions for SERV	{"The award-winning SERV (https:","",github.com,olofk,"serv) is the world's smallest RISC-V CPU and is suitable for deeply embedded applications where chip area is at a premium. In a real system however, not only the CPU sets the system size and the smaller the CPU, the more important to also minimize the memory size. Adding support for the C ISA extension (compressed instructions) to SERV would enable RISC-V to reach even more use-cases on the deeply embedded side.\n\nSERV is an open source bit-serial RISC-V implementation written in Verilog with an emphasis on resource efficiency. The addition of the C ISA extension is expected to consist of changes to the instructions fetcher, an extra optional decoder stage for converting the compressed instructions into their non-compressed equivalents as well as other changes related to the PC not necessarily being word-aligned. On the infrastructure side, the relevant test suites (riscv-formal and riscv-compliance) are expected to pass and it should run the Zephyr operation OS with compressed instructions enabled. The SERV FuseSoC core description file should also be updated to allow users to conditionally select the C ISA extension at compile-time for simulation and FPGA","ASIC targets."}	The award-winning SERV (https://github.com/olofk/serv) is the world's smallest RISC-V CPU and is suitable for deeply embedded applications where chip area is at a premium. In a real system however, not only the CPU sets the system size and the smaller the CPU, the more important to also minimize the memory size. Adding support for the C ISA extension (compressed instructions) to SERV would enable RISC-V to reach even more use-cases on the deeply embedded side.\n\nSERV is an open source bit-serial RISC-V implementation written in Verilog with an emphasis on resource efficiency. The addition of the C ISA extension is expected to consist of changes to the instructions fetcher, an extra optional decoder stage for converting the compressed instructions into their non-compressed equivalents as well as other changes related to the PC not necessarily being word-aligned. On the infrastructure side, the relevant test suites (riscv-formal and riscv-compliance) are expected to pass and it should run the Zephyr operation OS with compressed instructions enabled. The SERV FuseSoC core description file should also be updated to allow users to conditionally select the C ISA extension at compile-time for simulation and FPGA/ASIC targets.	{verilog,scripting}	2022	Term 1	https://github.com/olofk/serv		300000	126
270	2d65b3c2-bf6c-4290-bb9b-4edf181a4d97	CNCF - Kyverno: Automate Performance Testing	{"Description: Kyverno is a Kubernetes native policy engine that secures and automates Kubernetes configurations. This project automates scalability tests for Kyverno on large Kubernetes clusters with several namespaces and resources. The candidate has to propose an automation plan to create clusters and resources and help optimize resource usage of Kyverno for different loads for large Kubernetes clusters."}	Description: Kyverno is a Kubernetes native policy engine that secures and automates Kubernetes configurations. This project automates scalability tests for Kyverno on large Kubernetes clusters with several namespaces and resources. The candidate has to propose an automation plan to create clusters and resources and help optimize resource usage of Kyverno for different loads for large Kubernetes clusters.	{kubernetes,go,automation,testing}	2022	Term 1	https://github.com/kyverno/kyverno/issues/3113	https://kyverno.io	300000	17
269	6b79b7b7-7f30-4891-bdb1-5798ea207bef	CNCF - Kyverno: Extend Kyverno CLI test command for Generate policy rules	{"Description: Kyverno is a Kubernetes native policy engine that secures and automates Kubernetes configurations. This project extends the Kyverno CLI to cover generate policies and improve tests coverage for Kyverno, based on the test results. The enhancement will involve extending the test command for generate policy rules, adding more test cases for the samples, and automating execution of tests."}	Description: Kyverno is a Kubernetes native policy engine that secures and automates Kubernetes configurations. This project extends the Kyverno CLI to cover generate policies and improve tests coverage for Kyverno, based on the test results. The enhancement will involve extending the test command for generate policy rules, adding more test cases for the samples, and automating execution of tests.	{kubernetes,security,documentation}	2022	Term 1	https://github.com/kyverno/kyverno/issues/3114	https://kyverno.io	300000	17
230	32c7f43b-b580-4236-bc8f-dd934a6ea940	CNCF - Thanos: Migrate Thanos to the New Protocol Buffers v2 API	{"This project is about updating and optimizing the [protocol buffers](https:","",developers.google.com,"protocol-buffers) implementation used for communication throughout the Thanos project. Thanos relies heavily on gRPC and protocol buffers for communication between its many components, and now with the release of the [new Golang API for protocol buffers](https:","",blog.golang.org,"protobuf-apiv2) and the deprecation of the old API, Thanos should be updated to benefit from the improvements in the ecosystem. As part of this project, the protocol buffers generator used in Thanos will be migrated to one that supports the new API and brings other improvements, such as reducing memory allocations. This project will help make Thanos faster and ensure that we stay up to date with the latest libraries."}	This project is about updating and optimizing the [protocol buffers](https://developers.google.com/protocol-buffers) implementation used for communication throughout the Thanos project. Thanos relies heavily on gRPC and protocol buffers for communication between its many components, and now with the release of the [new Golang API for protocol buffers](https://blog.golang.org/protobuf-apiv2) and the deprecation of the old API, Thanos should be updated to benefit from the improvements in the ecosystem. As part of this project, the protocol buffers generator used in Thanos will be migrated to one that supports the new API and brings other improvements, such as reducing memory allocations. This project will help make Thanos faster and ensure that we stay up to date with the latest libraries.	{go,yaml}	2021	Term 3	https://github.com/thanos-io/thanos/issues/4557	https://thanos.io/	300000	26
275	74840708-0989-4983-88f6-9f43034ae351	CNCF - Karmada: Enhancement for controllers scalability	{"Description: Ensures the controllers are suitable for large-scale deployment in production cases."}	Description: Ensures the controllers are suitable for large-scale deployment in production cases.	{go,kubernetes}	2022	Term 1	https://github.com/karmada-io/karmada/issues/1331	https://karmada.io/	300000	124
277	3d992848-6dfe-4760-8b30-6a2cd4a5895d	CNCF - KubeEdge: Design and add more e2e tests especially for edge scenarios	{"Description: We have many features for edge scenarios, as edge autonomy, reliable message transmission, etc. We need to add e2e tests for them."}	Description: We have many features for edge scenarios, as edge autonomy, reliable message transmission, etc. We need to add e2e tests for them.	{kubernetes,kubeedge}	2022	Term 1	https://github.com/kubeedge/kubeedge/issues/3595	https://kubeedge.io/en/	0	74
271	e5515661-956e-49f2-8cef-8e9a14d52052	CNCF - Kyverno: OpenTelemetry exporter for Kyverno	{"Description: Kyverno is a Kubernetes native policy engine that secures and automates Kubernetes configurations. This project will instrument Kyverno to export OpenTelemetry data for metrics, logs, flows, and policy reports. The project will include testing with OpenTelemetry collectors and documenting the integration steps."}	Description: Kyverno is a Kubernetes native policy engine that secures and automates Kubernetes configurations. This project will instrument Kyverno to export OpenTelemetry data for metrics, logs, flows, and policy reports. The project will include testing with OpenTelemetry collectors and documenting the integration steps.	{observability,prometheus,opentelemetry,golang}	2022	Term 1	https://github.com/kyverno/kyverno/issues/3120	https://kyverno.io	300000	17
538	4a8a4f26-0ca9-4517-8cce-582c92092e33	CNCF - WasmEdge: Serialization Completion	{"WasmEdge is a WebAssembly runtime in both interpreter and ahead-of-time mode. However, WasmEdge only supports the binary format for the input WebAssembly file. To help the text format WebAssembly loader feature in the future, the implementation of serializing a WebAssembly module is necessary. In this mentorship, we hope the mentee should complete the serialization functions already in [the `dev","serialize` branch](https:","",github.com,WasmEdge,WasmEdge,tree,dev,"serialize) of the `WasmEdge` repo.\n- Expected outcome: Complete the serialization functions of WebAssembly modules, such as the element segment and data segment encoding. Complete the WebAssembly instructions encoding. Generate the unit test data and pass the unit tests. >80% of code coverage for serialization."}	WasmEdge is a WebAssembly runtime in both interpreter and ahead-of-time mode. However, WasmEdge only supports the binary format for the input WebAssembly file. To help the text format WebAssembly loader feature in the future, the implementation of serializing a WebAssembly module is necessary. In this mentorship, we hope the mentee should complete the serialization functions already in [the `dev/serialize` branch](https://github.com/WasmEdge/WasmEdge/tree/dev/serialize) of the `WasmEdge` repo.\n- Expected outcome: Complete the serialization functions of WebAssembly modules, such as the element segment and data segment encoding. Complete the WebAssembly instructions encoding. Generate the unit test data and pass the unit tests. >80% of code coverage for serialization.	{wasm}	2023	Term 2	https://github.com/WasmEdge/WasmEdge/issues/2262	https://wasmedge.org/	300000	31
238	9101857e-88c7-4fd9-b166-a18cf698003a	CNCF - WasmEdge: Support Wasm-Signature proposal	{"The wasm-Signature proposal is specifically about embedded digital signatures in WebAssembly modules, not about package","OCI signatures. When distributing WebAssembly modules, it will be nice if we can have a way to verify. To achieve this target, we choose a Wasm-Signature proposal as our implementation standard. With this proposal, WasmEdge can provide `sign` and `verify` features."}	The wasm-Signature proposal is specifically about embedded digital signatures in WebAssembly modules, not about package/OCI signatures. When distributing WebAssembly modules, it will be nice if we can have a way to verify. To achieve this target, we choose a Wasm-Signature proposal as our implementation standard. With this proposal, WasmEdge can provide `sign` and `verify` features.	{rust}	2021	Term 3	https://github.com/WasmEdge/WasmEdge/issues/344	https://wasmedge.org/	300000	31
310	c7d6faa6-0db3-40e2-a87a-2826c7fe08ba	Open Mainframe- Additional Log Support for ADE	{"ADE currently support logs generated by the Linux kernel and Spark.  This project is to add additional log suport for logs.  An example would be logs generated by NGINX"}	ADE currently support logs generated by the Linux kernel and Spark.  This project is to add additional log suport for logs.  An example would be logs generated by NGINX	{mainframe,developer}	2022	Term 1	https://github.com/openmainframeproject/ade		300000	38
286	61da37e0-a485-4b8e-aaa0-7e5b32578572	CNCF - Pixie: Add support for new protocols in protocol tracer	{"Description: Pixie performs automatic tracing and parsing of various protocols (e.g. HTTP, MySQL, Kafka), but there are many others that still need to be added. You can help add protocol parsers for technologies such as Mongo, AMQP (used by RabbitMQ and other message queues), or another protocol of your choice."}	Description: Pixie performs automatic tracing and parsing of various protocols (e.g. HTTP, MySQL, Kafka), but there are many others that still need to be added. You can help add protocol parsers for technologies such as Mongo, AMQP (used by RabbitMQ and other message queues), or another protocol of your choice.	{}	2022	Term 1	https://github.com/pixie-io/pixie/issues/341	https://px.dev/	300000	131
285	fcf265aa-57d4-4ec4-aa2d-365ca2d97349	CNCF - Kubernetes: Improving unit test coverage(CAPV)	{"Description: Cluster API (CAPI) is a Kubernetes sub-project focused on providing declarative APIs and tooling to simplify lifecycle management of Kubernetes clusters. CAPV is the infrastructure provider that extends Cluster API to manage Kubernetes clusters on vSphere. As a mentee, you will start with learning CAPI","CAPV concepts and then, will work on the main project which is to improve unit test coverage."}	Description: Cluster API (CAPI) is a Kubernetes sub-project focused on providing declarative APIs and tooling to simplify lifecycle management of Kubernetes clusters. CAPV is the infrastructure provider that extends Cluster API to manage Kubernetes clusters on vSphere. As a mentee, you will start with learning CAPI/CAPV concepts and then, will work on the main project which is to improve unit test coverage.	{go,git,testing,automation,ci,cd}	2022	Term 1	https://github.com/kubernetes-sigs/cluster-api-provider-vsphere/issues/1392	http://kubernetes.io/	300000	75
462	73202d21-d4ca-4435-9a73-f326c9b3e796	CNCF - Meshery: Distributed workflow engine	{"Integrate a new architectural component into Meshery: a workflow engine. This project involves shifting Meshery off of bitcask and off of sqlite over to postgres using gorm (golang). Interns will familiarize with concepts of orchestration engines, including chaining workflows, and content lifecycle management."}	Integrate a new architectural component into Meshery: a workflow engine. This project involves shifting Meshery off of bitcask and off of sqlite over to postgres using gorm (golang). Interns will familiarize with concepts of orchestration engines, including chaining workflows, and content lifecycle management.	{go,react,temporal}	2023	Term 1	https://github.com/meshery/meshery/issues/3934	https://meshery.io/	300000	186
288	0fa2c3b1-f9e5-4f87-bdac-adb72d6a0752	CNCF - Meshery: Service mesh playground (extended)	{"Description: Create the world’s service mesh playground. Meshery’s genesis is that of helping teach people about service mesh technology and enabling to operate this type of cloud native infrastructure confidently. The proposed project is aimed at furthering this mission with interactive API documentation connected to a service mesh learning playground (a running instance of Meshery)."}	Description: Create the world’s service mesh playground. Meshery’s genesis is that of helping teach people about service mesh technology and enabling to operate this type of cloud native infrastructure confidently. The proposed project is aimed at furthering this mission with interactive API documentation connected to a service mesh learning playground (a running instance of Meshery).	{go,react}	2022	Term 1	https://github.com/layer5io/meshery/issues/2931	https://meshery.io/	300000	186
241	058a41fd-79e8-43f7-a5f7-1840c1f759be	CNCF - Vitess: Add support for comparing strings using collations and character-sets	{"Vitess does not yet have support for collations and character-sets. So, to compare varchar strings Vitess needs to rely on [WEIGHT_STRING](https:","",dev.mysql.com,doc,refman,5.7,en,"string-functions.html#function_weight-string) function for now. As per MySQL documentation, WEIGHT_STRING is a debugging function, meant only for internal use. Having the ability to compare strings using collation and character set support we will be able to better implement ORDER BY, GROUP BY, JOIN. It will also allow us to leverage more advanced join techniques than what we currently implement."}	Vitess does not yet have support for collations and character-sets. So, to compare varchar strings Vitess needs to rely on [WEIGHT_STRING](https://dev.mysql.com/doc/refman/5.7/en/string-functions.html#function_weight-string) function for now. As per MySQL documentation, WEIGHT_STRING is a debugging function, meant only for internal use. Having the ability to compare strings using collation and character set support we will be able to better implement ORDER BY, GROUP BY, JOIN. It will also allow us to leverage more advanced join techniques than what we currently implement.	{go,sql}	2021	Term 3	https://github.com/vitessio/vitess/issues/8606	https://vitess.io/	300000	107
296	98abee3b-13e0-44b3-b9a4-fae5d34bb075	Hyperledger - Optimising pipelines using Github Actions for Caliper and Caliper-Benchmarks	{"Performance benchmarking distributed systems (such as DLTs) is a challenging task, comprising of multiple aspects, such as scalable workload generation, representative workload definitions, and comprehensive data analysis.\n\nHyperledger Caliper is a general-purpose benchmarking tool with the goal of mitigating the aforementioned aspects of performance benchmarking.\n\nHyperledger Caliper-benchmarks provides a set of example benchmarks for use with Hyperledger Caliper.\n\nIn order to be able to assist maintainers","contributors to Caliper and Caliper-Benchmarks, a build system is utilised to run tests to verify a PR doesn't break anything plus it provides a continual deploy mechanism to publish Hyperledger Caliper artifacts to DockerHub and NPM so end users can have access to the Caliper tool.\n\nThese build systems use different build pipeline implementations.\n\nThe goal of this project is to optimise how Caliper and Caliper-Benchmarks verify PRs and publish artifacts using Github Actions and to improve how testing is done to ensure that the artifacts to be published actually work as expected."}	Performance benchmarking distributed systems (such as DLTs) is a challenging task, comprising of multiple aspects, such as scalable workload generation, representative workload definitions, and comprehensive data analysis.\n\nHyperledger Caliper is a general-purpose benchmarking tool with the goal of mitigating the aforementioned aspects of performance benchmarking.\n\nHyperledger Caliper-benchmarks provides a set of example benchmarks for use with Hyperledger Caliper.\n\nIn order to be able to assist maintainers/contributors to Caliper and Caliper-Benchmarks, a build system is utilised to run tests to verify a PR doesn't break anything plus it provides a continual deploy mechanism to publish Hyperledger Caliper artifacts to DockerHub and NPM so end users can have access to the Caliper tool.\n\nThese build systems use different build pipeline implementations.\n\nThe goal of this project is to optimise how Caliper and Caliper-Benchmarks verify PRs and publish artifacts using Github Actions and to improve how testing is done to ensure that the artifacts to be published actually work as expected.	{git,bash,javascript,docker}	2022	Term 2	https://github.com/hyperledger/caliper	https://wiki.hyperledger.org/display/INTERN/Optimising+the+pipelines+using+Github+Actions+for+Caliper+and+Caliper-Benchmarks	300000	1
294	35992fa0-df01-4d47-a7cd-4fd869490dda	LFX Engineering Mentorship	{"CommunityBridge is a platform created by the Linux Foundation to empower developers — and the individuals and organizations who support them to advance sustainability, security, and diversity in open source technology. CommunityBridge brings together maintainers, contributors, aspiring developers, and enterprises to help solve these critical challenges and drive innovation."}	CommunityBridge is a platform created by the Linux Foundation to empower developers — and the individuals and organizations who support them to advance sustainability, security, and diversity in open source technology. CommunityBridge brings together maintainers, contributors, aspiring developers, and enterprises to help solve these critical challenges and drive innovation.	{go,api,angular}	2020	Term 1	https://github.com/communitybridge/dev-analytics	https://communitybridge.org/	2854000	132
246	1d6304a1-12a5-449a-8fd3-436be9756b09	CNCF - Meshery: Workflow engine	{"Integrate a new architectural component into Meshery: a workflow engine. This project involves shifting Meshery off of bitcask and off of sqlite over to postgres using gorm (golang). Interns will familiarize with concepts of orchestration engines, including chaining workflows, and content lifecycle management."}	Integrate a new architectural component into Meshery: a workflow engine. This project involves shifting Meshery off of bitcask and off of sqlite over to postgres using gorm (golang). Interns will familiarize with concepts of orchestration engines, including chaining workflows, and content lifecycle management.	{go,react}	2021	Term 3	https://github.com/meshery/meshery/issues/3934	https://meshery.io/	300000	186
398	e4e6d486-e6df-475d-8074-a363d0361076	CNCF - WasmEdge: A Rust library crate for mediapipe models for WasmEdge NN	{"Description: WasmEdge would like to build a Rust library crate that enables easy integration of Mediapipe models in WasmEdge applications. Each Mediapipe model has [a description page](https:","",google.github.io,mediapipe,solutions,"face_detection.html) that describes its input and output tensors. The [models](https:","",google.github.io,mediapipe,solutions,"models.html) are available in Tensorflow Lite format, which is supported by the WasmEdge Tensorflow Lite plugin.\n\nExpected Outcome: We need at least one set of library functions for each model in Mediapipe. Each library function takes in a media object and returns the inference result."}	Description: WasmEdge would like to build a Rust library crate that enables easy integration of Mediapipe models in WasmEdge applications. Each Mediapipe model has [a description page](https://google.github.io/mediapipe/solutions/face_detection.html) that describes its input and output tensors. The [models](https://google.github.io/mediapipe/solutions/models.html) are available in Tensorflow Lite format, which is supported by the WasmEdge Tensorflow Lite plugin.\n\nExpected Outcome: We need at least one set of library functions for each model in Mediapipe. Each library function takes in a media object and returns the inference result.	{rust}	2023	Term 1	https://github.com/WasmEdge/WasmEdge/issues/2229	https://wasmedge.org/	300000	31
550	0fc6c44b-5ddf-467f-8016-72cc35b4e3ff	CNCF - Jaeger: Implement Critical Path analysis	{"Description: Jaeger (https:","","jaegertracing.io) is a popular platform for distributed tracing. Critical path analysis is an important tool in the latency investigations. This project aims to add support for critical path analysis to Jaeger UI.\n- Expected outcomes:\n  - Implement critical path determination algorithm (maybe in the backend)\n  - Enhance Trace Timeline view to overlay critical path on top of the trace.\n  - Add relevant documentation to the Jaeger website\n  - Author a blog post on Jaeger blog explaining the new feature\n- Stretch goals:\n  - Add critical path visualization to other trace views (graph, table, flamechart)"}	Description: Jaeger (https://jaegertracing.io) is a popular platform for distributed tracing. Critical path analysis is an important tool in the latency investigations. This project aims to add support for critical path analysis to Jaeger UI.\n- Expected outcomes:\n  - Implement critical path determination algorithm (maybe in the backend)\n  - Enhance Trace Timeline view to overlay critical path on top of the trace.\n  - Add relevant documentation to the Jaeger website\n  - Author a blog post on Jaeger blog explaining the new feature\n- Stretch goals:\n  - Add critical path visualization to other trace views (graph, table, flamechart)	{go,typescript,javascript}	2023	Term 2	https://github.com/jaegertracing/jaeger-ui/issues/1288	https://www.jaegertracing.io/	300000	178
466	d2eff030-2e28-4a6d-b89b-7f3e2084ea96	Accelerating RISC-V vector instructions by using a bionic network design	{"Software acceleration utilizing specifically designed hardware components is a widely used technique for speeding up different kinds of kernels found in the targeted applications. An example of such kernels in High-Performance Computing (HPC) are algorithms involving the computation of sparse data structures. Sparse applications are difficult to accelerate due to their memory-bound nature, which means that the data is not accessed contiguously in memory. That implies many scatter and gather accesses to the comparatively slow memory. Consequently, dedicated hardware components capable of accelerating the scatter","gather instructions, are a key to improve the performance of sparse applications. \nThe goal of this project is to achieve an FPGA-based acceleration of the vrgather RISC-V vector instruction (RISC-V ISA vector extension v1.0) using a bitonic network. The bitonic network  is originally a sorting network that implements the merge-sort algorithm (add reference here). A number of inputs of the network can end up in any output after traversing it. When this concept is applied to a Vector Register File, the bitonic network can be used not only to sort a set of vector elements, but also to move data internally from one register to any other register. The scope of this project is limited to the execution and acceleration of the gather vector instruction, in which the variability of the vector length needs to be considered."}	Software acceleration utilizing specifically designed hardware components is a widely used technique for speeding up different kinds of kernels found in the targeted applications. An example of such kernels in High-Performance Computing (HPC) are algorithms involving the computation of sparse data structures. Sparse applications are difficult to accelerate due to their memory-bound nature, which means that the data is not accessed contiguously in memory. That implies many scatter and gather accesses to the comparatively slow memory. Consequently, dedicated hardware components capable of accelerating the scatter/gather instructions, are a key to improve the performance of sparse applications. \nThe goal of this project is to achieve an FPGA-based acceleration of the vrgather RISC-V vector instruction (RISC-V ISA vector extension v1.0) using a bitonic network. The bitonic network  is originally a sorting network that implements the merge-sort algorithm (add reference here). A number of inputs of the network can end up in any output after traversing it. When this concept is applied to a Vector Register File, the bitonic network can be used not only to sort a set of vector elements, but also to move data internally from one register to any other register. The scope of this project is limited to the execution and acceleration of the gather vector instruction, in which the variability of the vector length needs to be considered.	{tcl}	2022	Term 1	https://github.com/riscv/riscv-v-spec/releases/download/v0.10/riscv-v-spec-0.10.pdf		300000	171
247	278ad0b0-ec8a-474a-863b-a8a01956d99c	CNCF - Service Mesh Performance: Definition of MeshMark	{"Create MeshMark provides a universal performance index to gauge your mesh’s efficiency against deployments in other organizations’ environments. MeshMark functions as a service mesh performance index (a scale) to provide people the ability to weigh the value of their service mesh versus the overhead of their service mesh and assess whether they are getting out of the mesh what they are “paying” for in it. Work with maintainers from Layer5, Intel, Red Hat, and HashiCorp on researching cloud native infrastructure performance. Internship involves: machine learning, adaptive algorithms, running and analyzing performance statistics."}	Create MeshMark provides a universal performance index to gauge your mesh’s efficiency against deployments in other organizations’ environments. MeshMark functions as a service mesh performance index (a scale) to provide people the ability to weigh the value of their service mesh versus the overhead of their service mesh and assess whether they are getting out of the mesh what they are “paying” for in it. Work with maintainers from Layer5, Intel, Red Hat, and HashiCorp on researching cloud native infrastructure performance. Internship involves: machine learning, adaptive algorithms, running and analyzing performance statistics.	{analytics,algorithm,go}	2021	Term 3	https://github.com/service-mesh-performance/service-mesh-performance/issues/227	https://smp-spec.io/	360000	121
305	2de90ff3-3eea-4f47-b3c7-a452338fb33c	Hyperledger - Learning Tokens	{"We propose to use the composable Interwork Alliance Token Taxonomy Framework (IWA TTF) to produce a Learning Token that recognizes, registers, and rewards community engagement in teaching and learning experiences, certifying, at the same time, the acquisition of skills and competencies.\n\nCommunities that agree upon valuable behaviors can turn those behaviors into unique token representations of their value and set up clear rules for its creation and supply, able to resist modification or tampering, and, most important, capable of being transferred per-to-peer. Our Learning Token is proof of behaviors that benefit learning communities, individual members, and collectivity.\n\nWith IWA TTF, we shall define common token concepts with a clear understanding of our requirements. We shall specify properties for particular behaviors. We shall link their base Token Classification Hierarchy with our Taxonomy of DLT "," Blockchain Educational Opportunities as initial meta-data for standard artifacts and control message descriptions."}	We propose to use the composable Interwork Alliance Token Taxonomy Framework (IWA TTF) to produce a Learning Token that recognizes, registers, and rewards community engagement in teaching and learning experiences, certifying, at the same time, the acquisition of skills and competencies.\n\nCommunities that agree upon valuable behaviors can turn those behaviors into unique token representations of their value and set up clear rules for its creation and supply, able to resist modification or tampering, and, most important, capable of being transferred per-to-peer. Our Learning Token is proof of behaviors that benefit learning communities, individual members, and collectivity.\n\nWith IWA TTF, we shall define common token concepts with a clear understanding of our requirements. We shall specify properties for particular behaviors. We shall link their base Token Classification Hierarchy with our Taxonomy of DLT / Blockchain Educational Opportunities as initial meta-data for standard artifacts and control message descriptions.	{chaincode}	2022	Term 2	https://github.com/hyperledger	https://wiki.hyperledger.org/display/INTERN/Learning+Tokens	300000	1
303	d66a01c0-9e5e-4590-a61c-fff3d1192ef7	Hyperledger - SLA Self-Assessment with Hyperledger Fabric	{"The overall project plan is divided into three phases while the mentee is properly guided by their mentor in each phase. \n\n1. In the first phase, the development of the Fabric chaincodes for the SLA operations and their integration points occurs in the context of an entire decentralized architecture. \n2. In the second phase, building on the required interactions with on- and off-chain data is addressed with details as understood and designed on the previous phase. \n3. Finally, the holistic approach is developed in the third phase, where permissioned ecosystem is formed end-to-end and its related documentation is produced."}	The overall project plan is divided into three phases while the mentee is properly guided by their mentor in each phase. \n\n1. In the first phase, the development of the Fabric chaincodes for the SLA operations and their integration points occurs in the context of an entire decentralized architecture. \n2. In the second phase, building on the required interactions with on- and off-chain data is addressed with details as understood and designed on the previous phase. \n3. Finally, the holistic approach is developed in the third phase, where permissioned ecosystem is formed end-to-end and its related documentation is produced.	{sla,collaborative}	2022	Term 2	https://github.com/hyperledger/fabric	https://wiki.hyperledger.org/display/INTERN/SLA+Self-Assessment+with+Hyperledger+Fabric	0	1
302	c9ad16d3-d79f-4e78-8775-06e238b75914	Hyperledger - Visual Studio Code support for Hyperledger Caliper artifacts	{"Performance benchmarking distributed systems (such as DLTs) is a challenging task, comprising of multiple aspects, such as scalable workload generation, representative workload definitions, and comprehensive data analysis.\n\nHyperledger Caliper is a general-purpose benchmarking tool with the goal of mitigating the aforementioned aspects of performance benchmarking:\n\n1. It provides a flexible architecture to allow scalable workload generation.\n2. It collects and reports results based on detailed client-observable execution traces.\n3. It allows the implementation","plug-in of custom workload behaviors to meet the diverse criteria of a wide range of business scenarios.\n\nThe flexible support of the aforementioned performance benchmarking aspects inherently makes the setup and configuration of a Caliper-based project cumbersome and error-prone:\n\n- The user must provide information about the system under test (SUT) in the form of a network configuration file.\n- The user must also specify the flow of the benchmark run in the form of a benchmark configuration file.\n- The runtime behavior of Caliper components can be influenced in detail through a configuration file.\n- As with most programming-related projects, a Caliper-based project should employ best practices to make its structure flexible and maintainable.\n\nThe goal of the mentorship project is to deliver a Visual Studio Code extension that can help users perform the aforementioned tasks in an assisted and possibly automated manner."}	Performance benchmarking distributed systems (such as DLTs) is a challenging task, comprising of multiple aspects, such as scalable workload generation, representative workload definitions, and comprehensive data analysis.\n\nHyperledger Caliper is a general-purpose benchmarking tool with the goal of mitigating the aforementioned aspects of performance benchmarking:\n\n1. It provides a flexible architecture to allow scalable workload generation.\n2. It collects and reports results based on detailed client-observable execution traces.\n3. It allows the implementation/plug-in of custom workload behaviors to meet the diverse criteria of a wide range of business scenarios.\n\nThe flexible support of the aforementioned performance benchmarking aspects inherently makes the setup and configuration of a Caliper-based project cumbersome and error-prone:\n\n- The user must provide information about the system under test (SUT) in the form of a network configuration file.\n- The user must also specify the flow of the benchmark run in the form of a benchmark configuration file.\n- The runtime behavior of Caliper components can be influenced in detail through a configuration file.\n- As with most programming-related projects, a Caliper-based project should employ best practices to make its structure flexible and maintainable.\n\nThe goal of the mentorship project is to deliver a Visual Studio Code extension that can help users perform the aforementioned tasks in an assisted and possibly automated manner.	{git,testing,markdown,research}	2022	Term 2	https://github.com/hyperledger/caliper	https://wiki.hyperledger.org/display/INTERN/Visual+Studio+Code+support+for+Hyperledger+Caliper+artifacts	600000	1
401	d8a7022f-8c62-4776-9e7c-4cc12f306177	CNCF - Konveyor: Move2Kube Consume Move2Kube through a plugin on VSCode	{"Move2Kube is a command-line tool for automating creation of Infrastructure as code (IaC) artifacts. It has inbuilt support for creating IaC artifacts for replatforming to Kubernetes","OpenShift. Users currently have to use move2kube command line tool or UI to access move2kube. Allow Move2Kube to be accessible from VSCode as a plugin. It can start with simple functionality like right clicking on a docker-compose file, and generating all Kubernetes artifacts. A VSCode plugin for Move2kube will promote fast integration in replatforming workflows.\n\nExpected Outcome: An end to end working VSCode plugin with a demo video showcasing the functionality."}	Move2Kube is a command-line tool for automating creation of Infrastructure as code (IaC) artifacts. It has inbuilt support for creating IaC artifacts for replatforming to Kubernetes/OpenShift. Users currently have to use move2kube command line tool or UI to access move2kube. Allow Move2Kube to be accessible from VSCode as a plugin. It can start with simple functionality like right clicking on a docker-compose file, and generating all Kubernetes artifacts. A VSCode plugin for Move2kube will promote fast integration in replatforming workflows.\n\nExpected Outcome: An end to end working VSCode plugin with a demo video showcasing the functionality.	{golang,typescript}	2023	Term 1	https://github.com/konveyor/move2kube/issues/395	https://www.konveyor.io/	300000	148
309	cd5508de-1d5b-47a4-b3c8-823e6efee5cb	Hyperledger - Ecosystem Analyst	{"Implementations of Hyperledger technologies span multiple industries, including finance, supply chain, trade, health, media, telecom and more. The beauty of our open source technology community is that anyone can use and build upon it for their specific needs and use cases. There are hundreds of implementations and capturing them in an easy to find and useful manner is a key enabling tool. Helping our community and the public to better understand use cases is a key part of helping advance the impact of distributed ledger technology. It validates ideas others have, provides avenues for reaching out and sharing lessons learned, and is a key part of building the Hyperledger community. In addition, we would also like to track academic papers and courses related to Hyperledger projects in order to develop an understanding of the academic landscape. \n\nIn gathering and organizing the above information, the mentee will leverage their broad view of Hyperledger use cases as well as understanding of the overall blockchain space to identify potential opportunities for growth in uses or applications of Hyperledger technology. This can include use cases where our community isn’t as present, where we can explore doing further outreach and education. In addition, the mentee will make recommendations and potentially also work on developing ways for the use case and academic information to be leveraged for educational and ecosystem-building content and activities."}	Implementations of Hyperledger technologies span multiple industries, including finance, supply chain, trade, health, media, telecom and more. The beauty of our open source technology community is that anyone can use and build upon it for their specific needs and use cases. There are hundreds of implementations and capturing them in an easy to find and useful manner is a key enabling tool. Helping our community and the public to better understand use cases is a key part of helping advance the impact of distributed ledger technology. It validates ideas others have, provides avenues for reaching out and sharing lessons learned, and is a key part of building the Hyperledger community. In addition, we would also like to track academic papers and courses related to Hyperledger projects in order to develop an understanding of the academic landscape. \n\nIn gathering and organizing the above information, the mentee will leverage their broad view of Hyperledger use cases as well as understanding of the overall blockchain space to identify potential opportunities for growth in uses or applications of Hyperledger technology. This can include use cases where our community isn’t as present, where we can explore doing further outreach and education. In addition, the mentee will make recommendations and potentially also work on developing ways for the use case and academic information to be leveraged for educational and ecosystem-building content and activities.	{database,research}	2022	Term 2	https://github.com/hyperledger	https://wiki.hyperledger.org/display/INTERN/Ecosystem+Analyst	0	1
339	457cabc9-90d3-4519-bfad-92966501dbd4	CNCF - Kyverno: Kyverno SLSA 3	{"Implement software supply chain security best practices to achieve SLSA Level 3 compliance (https:","",slsa.dev,"). This includes generation of build provenance data for Kyverno. Kyverno - SLSA."}	Implement software supply chain security best practices to achieve SLSA Level 3 compliance (https://slsa.dev/). This includes generation of build provenance data for Kyverno. Kyverno - SLSA.	{go,security}	2022	Term 2	https://github.com/kyverno/kyverno/issues/3119	https://kyverno.io/	300000	17
556	26377c30-9ffd-41e3-bfea-839bf126f8f6	CNCF - Meshery: Adopt OCI as the packaging and distribution format for Meshery MeshModels	{"Meshery MeshModels represent a schema-based description of cloud native infratructure. MeshModels need to be portable between Meshery deployments as well as easily versionable in external repositories.\n- Expected outcome:\n  - Meshery clients (mesheryctl and Meshery UI) should be able to import","export MeshModels as OCI images.\n  - Meshery clients (mesheryctl and Meshery UI) should be able to push","pull from OCI-compatible registries.\n  - Stretch Goal: OCI image signing; Verify the authenticity of MeshModels using [cosign](https:","",github.com,sigstore,"cosign).\n  - Target registries: Meshery Catalog (https:","",meshery.io,"catalog), Artifact Hub."}	Meshery MeshModels represent a schema-based description of cloud native infratructure. MeshModels need to be portable between Meshery deployments as well as easily versionable in external repositories.\n- Expected outcome:\n  - Meshery clients (mesheryctl and Meshery UI) should be able to import/export MeshModels as OCI images.\n  - Meshery clients (mesheryctl and Meshery UI) should be able to push/pull from OCI-compatible registries.\n  - Stretch Goal: OCI image signing; Verify the authenticity of MeshModels using [cosign](https://github.com/sigstore/cosign).\n  - Target registries: Meshery Catalog (https://meshery.io/catalog), Artifact Hub.	{go,react,graphql}	2023	Term 2	https://github.com/meshery/meshery/issues/6447	https://meshery.io/	300000	186
318	b78003c5-933e-4885-b576-a0eb55e65b49	Hyperledger - Improve User Experience from Getting Started to Continuous Engagement	{"Hyperledger India Chapter started with the https:","","start-here.hyperledger.org to help new developers look for contribution opportunities. The tool provides an option to see through a list of all \\"Good First Issues\\", \\"Open PRs\\" and \\"Releases\\".\n\nThe requirement, in this proposal, is to extend or improve the website to include - role-based navigation (eg. Developer, Business User, Contributor, etc) to information. The process will involve adding easy to access links or information for project documentation, community contribution guidelines, guiding (new & experienced) users towards how to start & what tasks require contribution, informing the latest happenings within the community (i.e. figuring out available events and meetups), and Calendar(s). The cherry on top of it would be to have navigations defined for different users and search capability.\n\nThe task would start with the survey on what somebody looks for within the community, understand what troubles them. The project is aimed at providing an improved user experience."}	Hyperledger India Chapter started with the https://start-here.hyperledger.org to help new developers look for contribution opportunities. The tool provides an option to see through a list of all "Good First Issues", "Open PRs" and "Releases".\n\nThe requirement, in this proposal, is to extend or improve the website to include - role-based navigation (eg. Developer, Business User, Contributor, etc) to information. The process will involve adding easy to access links or information for project documentation, community contribution guidelines, guiding (new & experienced) users towards how to start & what tasks require contribution, informing the latest happenings within the community (i.e. figuring out available events and meetups), and Calendar(s). The cherry on top of it would be to have navigations defined for different users and search capability.\n\nThe task would start with the survey on what somebody looks for within the community, understand what troubles them. The project is aimed at providing an improved user experience.	{javascript,git,react,html,css}	2022	Term 2	https://github.com/hyperledger-tooling/start-here-hyperledger	https://wiki.hyperledger.org/display/INTERN/One-stop+shop+for+all-things+Hyperledger	300000	1
284	63faaa29-00a2-43af-b874-fa1b90630318	CNCF - Kubernetes and Elekto: Elections Security Improvements	{"Description: Elekto is a project for running preference elections for open source projects hosted by the CNCF. It is used for elections for the Kubernetes and Knative projects, and will soon be used by others. During the 2021 elections, a security audit identified several areas of improvement in both in the security and privacy of Elekto, and in the security of the Kubernetes deployment. The mentee for this project would be implementing those recommendations in order to make Elekto and Kubernetes elections more secure."}	Description: Elekto is a project for running preference elections for open source projects hosted by the CNCF. It is used for elections for the Kubernetes and Knative projects, and will soon be used by others. During the 2021 elections, a security audit identified several areas of improvement in both in the security and privacy of Elekto, and in the security of the Kubernetes deployment. The mentee for this project would be implementing those recommendations in order to make Elekto and Kubernetes elections more secure.	{python,flask,html,css,sql,cryptographic}	2022	Term 1	https://github.com/elekto-io/elekto/issues/51	https://elekto.dev/	300000	75
557	f4a9804f-2e46-42a4-b2ae-ad3ea7b29734	CNCF - Meshery: Meshery UI Permissions Framework	{"Meshery UI lacks a permissions framework. The existing internal implementation is simple, fragile and must be completely rewritten. The approach to implemention a permmissions framework includes using React.js and CASL.js. Meshery UI's approach needs to be extensible given that this framework will be an extension point for Remote Providers to supply their own permissions.\n- Expected outcome: Definition of permissions and their enforcement in Meshery with an aim for deep granularity and extensibility with each user interface input component carrying a unique permission key id. Each key is then put into a group of keys in a keychain, keychains assigned to user roles, in turn, roles assigned to users. With users having the ability to create own custom roles, the framework will be dynamic based on the associated server-side permissions for the currently auth’ed user.\n\n- Upstream Issue (URL): https:","",github.com,meshery,meshery,issues,"7436, https:","",github.com,meshery,meshery,issues,7382}	Meshery UI lacks a permissions framework. The existing internal implementation is simple, fragile and must be completely rewritten. The approach to implemention a permmissions framework includes using React.js and CASL.js. Meshery UI's approach needs to be extensible given that this framework will be an extension point for Remote Providers to supply their own permissions.\n- Expected outcome: Definition of permissions and their enforcement in Meshery with an aim for deep granularity and extensibility with each user interface input component carrying a unique permission key id. Each key is then put into a group of keys in a keychain, keychains assigned to user roles, in turn, roles assigned to users. With users having the ability to create own custom roles, the framework will be dynamic based on the associated server-side permissions for the currently auth’ed user.\n\n- Upstream Issue (URL): https://github.com/meshery/meshery/issues/7436, https://github.com/meshery/meshery/issues/7382	{go,react,rego}	2023	Term 2	https://github.com/meshery/meshery/issues/7436	https://meshery.io/	300000	186
408	fc06da19-fadd-499f-ae71-3da2caba5aea	CNCF - Konveyor: Move2Kube Allow customizations be added as remote git repo path	{"Move2Kube is a command-line tool for automating creation of Infrastructure as code (IaC) artifacts. It has inbuilt support for creating IaC artifacts for replatforming to Kubernetes","OpenShift. Currently, in the CLI we can use the -c flag to point to the folder containing customizations and in UI we could upload a zip file containing the customizatoins. It would be better to consume customizations when specified as a git repo path. The use case can also be extended to take source code input taken directory from a remote git repository.\n\nExpected Outcome: Move2Kube should be able to consume git repo path as input."}	Move2Kube is a command-line tool for automating creation of Infrastructure as code (IaC) artifacts. It has inbuilt support for creating IaC artifacts for replatforming to Kubernetes/OpenShift. Currently, in the CLI we can use the -c flag to point to the folder containing customizations and in UI we could upload a zip file containing the customizatoins. It would be better to consume customizations when specified as a git repo path. The use case can also be extended to take source code input taken directory from a remote git repository.\n\nExpected Outcome: Move2Kube should be able to consume git repo path as input.	{golang}	2023	Term 1	https://github.com/konveyor/move2kube/issues/604	https://www.konveyor.io/	0	148
322	cf992af8-6102-4c2d-8394-c3462112e39f	CNCF - Service Mesh Performance: Implementation of MeshMark	{"MeshMark functions as a service mesh performance index (a scale) to provide people the ability to weigh the value of their service mesh versus the overhead of their service mesh and assess whether they are getting out of the mesh what they are “paying” for in it. Implementation of this new performance index involves integration with Meshery, Prometheus, and more."}	MeshMark functions as a service mesh performance index (a scale) to provide people the ability to weigh the value of their service mesh versus the overhead of their service mesh and assess whether they are getting out of the mesh what they are “paying” for in it. Implementation of this new performance index involves integration with Meshery, Prometheus, and more.	{go,react}	2022	Term 2	https://github.com/service-mesh-performance/service-mesh-performance/issues/325	https://smp-spec.io/	300000	121
369	2ceeee35-a85d-4768-9f22-d22838e27cd5	CNCF - Volcano: Avoid hot node in dynamic scheduling based on real workload	{"In v1.6.0, Volcano has supported dynamic scheduling based on real workload. However, the scheduler cannot be aware of hot nodes which may receive too many bound pods. Your task is to design an algorithm to avoid hot nodes and balance node pressure."}	In v1.6.0, Volcano has supported dynamic scheduling based on real workload. However, the scheduler cannot be aware of hot nodes which may receive too many bound pods. Your task is to design an algorithm to avoid hot nodes and balance node pressure.	{go,volcano}	2022	Term 3	https://github.com/volcano-sh/volcano/issues/2426	https://volcano.sh/	0	34
311	d7410cfd-7ead-42d7-8226-79126e0433fc	Open Mainframe- Review, Advise and Contribute to the Mainframe Open Education Project	{"With the Mainframe Open Education Platform just launching, we need help with reviewing content, advising the team if we are missing content, what would help those from Universities, etc to use, and contribute by helping us increase awareness of what we are trying to do for the Mainframe Community."}	With the Mainframe Open Education Platform just launching, we need help with reviewing content, advising the team if we are missing content, what would help those from Universities, etc to use, and contribute by helping us increase awareness of what we are trying to do for the Mainframe Community.	{mainframe}	2022	Term 2	https://github.com/openmainframeproject-internship		300000	38
472	fd40dc73-b915-4ed5-8472-6905334335ff	Resolve mentorship Issues- TEST_ DO NOT APPLY	{"Resolve Issues and practice various aspects"}	Resolve Issues and practice various aspects	{}	2023	Term 2	https://www.example.com/		0	166
562	ff8c6964-fad4-4629-a603-0d71a4857e1c	Hyperledger Collaborative Learning - Improve Documentation for DRMan - Unpaid	{"The task is to improve the documentation for DRMan, which aims to develop a utility for managing Verifiable Credential Registries. The core functionalities include creating, verifying, modifying, and revoking credentials, as well as managing DIDs. The project requires the creation of a DID registry for an organization, onboarding of members, and management of APIs needed to add",update,"revoke access to DIDs or DID documents. The mentee is expected to use MKDocs to document the project and make it production-ready for future use."}	The task is to improve the documentation for DRMan, which aims to develop a utility for managing Verifiable Credential Registries. The core functionalities include creating, verifying, modifying, and revoking credentials, as well as managing DIDs. The project requires the creation of a DID registry for an organization, onboarding of members, and management of APIs needed to add/update/revoke access to DIDs or DID documents. The mentee is expected to use MKDocs to document the project and make it production-ready for future use.	{dlt}	2023	Term 3	https://github.com/DIDman/DRman	https://wiki.hyperledger.org/pages/viewpage.action?pageId=80778516	0	180
327	2b3edea5-1f6a-4f6b-a80a-38f5be4ec339	CNCF - Karmada: Cluster Resource modeling	{"In the scheduling progress, the karmada-scheduler makes decisions as per a bunch of factors, one of the factors is the resource details of the cluster. We don't want to collect and store each node's resources in detail(That's a burden for Karmada to maintain the information), but we want to build a resource model for each cluster."}	In the scheduling progress, the karmada-scheduler makes decisions as per a bunch of factors, one of the factors is the resource details of the cluster. We don't want to collect and store each node's resources in detail(That's a burden for Karmada to maintain the information), but we want to build a resource model for each cluster.	{go,kubernetes,algorithm}	2022	Term 2	https://github.com/karmada-io/karmada/issues/772	https://karmada.io/	300000	124
332	bcccd290-87c5-4ad1-9360-e6041d51521c	CNCF - Karmada: Develop Override policy, Resource Binding, Work Page	{"These pages on the web dashboard will help to perform different operations for Override policies, Resource Binding in karmada."}	These pages on the web dashboard will help to perform different operations for Override policies, Resource Binding in karmada.	{figma,react,redux}	2022	Term 2	https://github.com/karmada-io/dashboard/issues/15	https://karmada.io/	300000	124
325	5def28a6-3ec2-4ba3-b56f-403a03a2b9ef	CNCF - WasmEdge: Implement component-model proposal in WasmEdge	{"The component-model proposal merges and supersedes the Module Linking and Interface Types proposals. With this feature, WasmEdge can execute multiple modules wasm with Module Linking and and more flexible types with Interface Type."}	The component-model proposal merges and supersedes the Module Linking and Interface Types proposals. With this feature, WasmEdge can execute multiple modules wasm with Module Linking and and more flexible types with Interface Type.	{wasm}	2022	Term 2	https://github.com/WasmEdge/WasmEdge/issues/1433	https://wasmedge.org/	300000	31
323	232a19f2-40de-4dfa-8966-2c1586bc6ecc	CNCF - WasmEdge: Support Durable Objects (DO) in WasmEdge	{"One of the most important features of WasmEdge is its support for non-blocking network sockets. However, the current WasmEdge API for async networking is still cumbersome. Rust developers would prefer to use a Tokio-like async "," await API for such tasks. But Tokio is multi-threaded and cannot run correctly in standard single-threaded WebAssembly. Yet, it is possible to provide a single-threaded Tokio runtime. Our goal is to create a WebAssembly compatible Tokio scheduler."}	One of the most important features of WasmEdge is its support for non-blocking network sockets. However, the current WasmEdge API for async networking is still cumbersome. Rust developers would prefer to use a Tokio-like async / await API for such tasks. But Tokio is multi-threaded and cannot run correctly in standard single-threaded WebAssembly. Yet, it is possible to provide a single-threaded Tokio runtime. Our goal is to create a WebAssembly compatible Tokio scheduler.	{wasm,rust,javascript,database}	2022	Term 2	https://github.com/WasmEdge/WasmEdge/issues/1431	https://wasmedge.org/	300000	31
564	f93ac457-60e5-489a-a25c-4be94f4f3d98	Hyperledger Collaborative Learning - An Optimized BFT on Fabric - Unpaid	{"The current existing implementation of Hyperledger Fabric with Raft consensus has many shortcomings such as transaction censorship and no mechanism to safeguard the network if a malicious leader gets elected. The network also has limitations on scalability which restrict the type of use cases that can be built such as real-time applications that use IoT devices. This impedes blockchain adoption and limits the type of applications and use cases that can be built on Fabric.\n\nThe goal of this project is to build BiniBFT, a Byzantine Fault Tolerance consensus library for Hyperledger Fabric which provides end-to-end security, high throughput with low latency and high scalability so that Fabric can be applicable for distributed and decentralized day-to-day applications.\n\nLearning Objectives\n- Building and Integrating a BFT library with Hyperledger Fabric.\n- Understanding how BFT consensus protocols work and optimizing algorithms for better performance.\n- Developing an understanding of how DLT networks like Hyperledger Fabric work.\n- Exploring the different ways in which BFT protocols can improve the performance and security of Fabric networks."}	The current existing implementation of Hyperledger Fabric with Raft consensus has many shortcomings such as transaction censorship and no mechanism to safeguard the network if a malicious leader gets elected. The network also has limitations on scalability which restrict the type of use cases that can be built such as real-time applications that use IoT devices. This impedes blockchain adoption and limits the type of applications and use cases that can be built on Fabric.\n\nThe goal of this project is to build BiniBFT, a Byzantine Fault Tolerance consensus library for Hyperledger Fabric which provides end-to-end security, high throughput with low latency and high scalability so that Fabric can be applicable for distributed and decentralized day-to-day applications.\n\nLearning Objectives\n- Building and Integrating a BFT library with Hyperledger Fabric.\n- Understanding how BFT consensus protocols work and optimizing algorithms for better performance.\n- Developing an understanding of how DLT networks like Hyperledger Fabric work.\n- Exploring the different ways in which BFT protocols can improve the performance and security of Fabric networks.	{go}	2023	Term 3	https://github.com/hyperledger/fabric	https://wiki.hyperledger.org/display/CLP/BiniBFT+-+An+Optimized+BFT+on+Fabric	0	180
336	25122df1-5e92-4d63-9ccc-b0c068237e00	CNCF - Karmada: Develop Propagation policy, Settings, About Pages	{"Propogation Policy page on the web dashboard will help to perform different operations for Propagation policies in karmada, Settings page will help the user to modify the dashboard according to their needs."}	Propogation Policy page on the web dashboard will help to perform different operations for Propagation policies in karmada, Settings page will help the user to modify the dashboard according to their needs.	{react,redux,figma}	2022	Term 2	https://github.com/karmada-io/dashboard/issues/16	https://karmada.io/	300000	124
334	947885e1-6b32-4b3c-85cb-955cb617237e	CNCF - OpenELB: Provide the OpenELB Web UI for managing EIP and IP pool	{"OpenELB is an open-source load balancer implementation designed for exposing the LoadBalancer type of Kubernetes services in bare metal, edge, and virtualization environments. Currently, Currently, the allocation of OpenELB EIP pool and EIP can only be viewed and managed using command in the terminal. We hope to provide a simple web console to make OpenELB much developer-friendly. Users could manage EIP pool and EIP resources using web UI."}	OpenELB is an open-source load balancer implementation designed for exposing the LoadBalancer type of Kubernetes services in bare metal, edge, and virtualization environments. Currently, Currently, the allocation of OpenELB EIP pool and EIP can only be viewed and managed using command in the terminal. We hope to provide a simple web console to make OpenELB much developer-friendly. Users could manage EIP pool and EIP resources using web UI.	{go,kubernetes,helm,docker}	2022	Term 2	https://github.com/openelb/openelb/issues/244	https://openelb.github.io/	300000	136
370	4dc62372-c04f-432f-847c-2cddd2cf786a	CNCF - Volcano: Implement pod filter chain for rescheduling	{"Currently, Volcano will regard all pods as potential victims in rescheduling, which is not so reasonable in some scenarios. Your task is to implement a pod filter chain to support custom configurations."}	Currently, Volcano will regard all pods as potential victims in rescheduling, which is not so reasonable in some scenarios. Your task is to implement a pod filter chain to support custom configurations.	{go,volcano}	2022	Term 3	https://github.com/volcano-sh/volcano/issues/2428	https://volcano.sh/	0	34
574	a80908c1-c05b-46c4-a41b-32392c4822cb	CNCF - Armada: Build a virtual-kubelet provider for Armada	{"Virtual Kubelet (https:","",github.com,virtual-kubelet,"virtual-kubelet) is a way to bring a Kubernetes front end and allow for your execution environment to be something other than Kubernetes. We want to add a virtual-kubelet provider for Armada so we can allow for a k8 front end integration with Armada.\n- Expected outcomes:\n  - A provider for virtual-kubelet that allows integration of Armada with Virtual-Kubelet"}	Virtual Kubelet (https://github.com/virtual-kubelet/virtual-kubelet) is a way to bring a Kubernetes front end and allow for your execution environment to be something other than Kubernetes. We want to add a virtual-kubelet provider for Armada so we can allow for a k8 front end integration with Armada.\n- Expected outcomes:\n  - A provider for virtual-kubelet that allows integration of Armada with Virtual-Kubelet	{go}	2023	Term 3	https://github.com/armadaproject/armada/issues/2702	https://armadaproject.io	0	176
343	4c4ccc69-8a71-4f5e-913f-c3c79f9af9a4	CNCF - Karmada: Design & Develop FederatedResourceQuota, SearchRegistry & MultiClusterIngress page	{"More pages need to be designed & added to the dashboard which are required to make great use of the functionalities of the karmada project using the web ui client."}	More pages need to be designed & added to the dashboard which are required to make great use of the functionalities of the karmada project using the web ui client.	{react,redux,figma}	2022	Term 2	https://github.com/karmada-io/dashboard/issues/17	https://karmada.io/	300000	124
410	51398c19-87c2-4b50-9dd3-760fbd820688	CNCF - Kubevela: Extend the capability of KubeVela by making several useful addons	{"- Description: KubeVela currently have a variety of addons , including experimental options, that address scenarios such as Continual Delivery and observability. To further enhance the out-of-box functionality for users of KubeVela, we can offer additional useful addons.\n- Expected Outcome: 10+ eperimetal addons, clear documentation should be provided for enabling and using these addons, including examples of useful use-cases."}	- Description: KubeVela currently have a variety of addons , including experimental options, that address scenarios such as Continual Delivery and observability. To further enhance the out-of-box functionality for users of KubeVela, we can offer additional useful addons.\n- Expected Outcome: 10+ eperimetal addons, clear documentation should be provided for enabling and using these addons, including examples of useful use-cases.	{go,kubernetes,cuelang}	2023	Term 1	https://github.com/kubevela/kubevela/issues/5358	https://kubevela.io/	300000	117
342	d145fd38-b533-49a8-9b02-60220f9f618c	CNCF - Kyverno: Integrate Kubernetes Pod Security with Kyverno	{"Integrate Kubernetes Pod Security with Kyverno for finer grained controls."}	Integrate Kubernetes Pod Security with Kyverno for finer grained controls.	{go,kubernetes}	2022	Term 2	https://github.com/kyverno/kyverno/issues/3830	https://kyverno.io/	540000	17
341	1218d516-45af-46a3-977b-e5a9de818cec	CNCF - Tremor: Pluggable logging	{"Tremor is an event processing system that can - among other things - process logs and metrics. Currently, Tremor uses log4rs to handle its own logging. We would like tremor to have a facility to handle its logs through its own pipelines (similar to the pluggable metrics experience). A starting point could be a sink for log4rs, which could then be replaced completely, making log4rs an optional output."}	Tremor is an event processing system that can - among other things - process logs and metrics. Currently, Tremor uses log4rs to handle its own logging. We would like tremor to have a facility to handle its logs through its own pipelines (similar to the pluggable metrics experience). A starting point could be a sink for log4rs, which could then be replaced completely, making log4rs an optional output.	{rust,testing}	2022	Term 2	https://github.com/tremor-rs/tremor-runtime/issues/1621	https://www.tremor.rs/	540000	20
346	d5a09dfc-c4cc-4733-8c4c-ff3aab2e3688	CNCF - Volcano: Official Website Docs Enhancement	{"Official website docs has not been updated for a long time including technology docs, talks, best practice and so on, which bothers users and developers a lot."}	Official website docs has not been updated for a long time including technology docs, talks, best practice and so on, which bothers users and developers a lot.	{go,kubernetes}	2022	Term 2	https://github.com/volcano-sh/website/issues/209	https://volcano.sh/	300000	34
345	8d1c8ff1-24da-4839-9072-3a6070eeb482	CNCF - Volcano: Volcano scalability enhancement	{"In order to have a better support of other AI","HPC platforms and GPU, it is necessary to enhance the integration with third-party operators and GPU support."}	In order to have a better support of other AI/HPC platforms and GPU, it is necessary to enhance the integration with third-party operators and GPU support.	{go,kubernetes}	2022	Term 2	https://github.com/volcano-sh/volcano/issues/2211	https://volcano.sh/	300000	34
465	6ebebb1c-11f2-43e1-a9ce-14eb8513d1a0	Cache architecture between GOWIN's RISC-V processor IP cores and GOWIN's embedded PSRAM controller I	{PSRAM,"HyperRAM is a low pin count DDR-type memory that provides data in sequential address bursts.  These memories are fairly large and would be great for applications that require more memory. Block SRAM inside GOWIN's FPGA's are dynamically addressable and are better for use with RISC-V MCUs since they can obtain instructions and data at any address within a clock cycle.  However, there is limited amount of BSRAM in most semiconductor devices including FPGAs.\n\nA cache between the PSRAM controller bus and the RISC-V processor bus can help fetch instructions","data from PSRAM with sequential addressing and store locally in BSRAM.  The processor can then dynamically obtain subsequent instructions and data from the BSRAM within a particular address range. GOWIN already has multiple RISC-V IP cores and PSRAM IP cores in it's IP Core Generator tool.  This project would be to develop the cache architecture and RTL that sits in between these two IPs and efficiently trades off the efficiency of using BSRAM for instructions","data versus the storage size","cost benefits of using PSRAM memory."}	PSRAM/HyperRAM is a low pin count DDR-type memory that provides data in sequential address bursts.  These memories are fairly large and would be great for applications that require more memory. Block SRAM inside GOWIN's FPGA's are dynamically addressable and are better for use with RISC-V MCUs since they can obtain instructions and data at any address within a clock cycle.  However, there is limited amount of BSRAM in most semiconductor devices including FPGAs.\n\nA cache between the PSRAM controller bus and the RISC-V processor bus can help fetch instructions/data from PSRAM with sequential addressing and store locally in BSRAM.  The processor can then dynamically obtain subsequent instructions and data from the BSRAM within a particular address range. GOWIN already has multiple RISC-V IP cores and PSRAM IP cores in it's IP Core Generator tool.  This project would be to develop the cache architecture and RTL that sits in between these two IPs and efficiently trades off the efficiency of using BSRAM for instructions/data versus the storage size/cost benefits of using PSRAM memory.	{}	2022	Term 1	www.gowinsemi.com		600000	171
414	a50fec46-7bc6-4fa0-ba84-848f0c136b5c	CNCF - KubeEdge: Re-design and implement the KubeEdge website	{"Description: KubeEdge's website has been running for a few years, and now we have more customer cases and more developer courses, so this project will update KubeEdge's website, with more readable documents on the homepage, covering user cases, developer courses, etc.\n\nExpected Outcome: The website has more readable documentation, covering user cases, developer courses, etc."}	Description: KubeEdge's website has been running for a few years, and now we have more customer cases and more developer courses, so this project will update KubeEdge's website, with more readable documents on the homepage, covering user cases, developer courses, etc.\n\nExpected Outcome: The website has more readable documentation, covering user cases, developer courses, etc.	{kubeedge,javascript,html}	2023	Term 1	https://github.com/kubeedge/website/issues/292	https://kubeedge.io/en/	300000	74
349	b503dac3-3b2f-49c5-be17-1d95404ef0a9	CNCF - Tremor: Hygenic error handling and validation for pipelines	{"Tremor uses its own language for pluggable user defined functionality. The language interconnects internal operators via the connect statement and the select statement. Currently, neither select nor connect verifies that the operator port of the receiving or the sending part is correct ( exists, and is an expected type ) - this can lead to silent or confusing errors. User experience is super important to tremor, so that is a solution state we’re not happy with. The goal of this mentorship is to add validation and provide targeted hygienic errors to users that are trivial to diagnose and resolve as this will massively improve user experience."}	Tremor uses its own language for pluggable user defined functionality. The language interconnects internal operators via the connect statement and the select statement. Currently, neither select nor connect verifies that the operator port of the receiving or the sending part is correct ( exists, and is an expected type ) - this can lead to silent or confusing errors. User experience is super important to tremor, so that is a solution state we’re not happy with. The goal of this mentorship is to add validation and provide targeted hygienic errors to users that are trivial to diagnose and resolve as this will massively improve user experience.	{rust}	2022	Term 2	https://github.com/tremor-rs/tremor-runtime/issues/1358	https://www.tremor.rs/	600000	20
352	8474dc39-a7f6-4f20-a265-d8486bdb3e0a	Open Mainframe- Enable JIT for LUA on IBM z	{"Complete existing work on providing JIT capability to LUA for IBM z"}	Complete existing work on providing JIT capability to LUA for IBM z	{ibm,jit}	2022	Term 2	www.openmainframe.org		300000	38
356	62606d5f-334c-471b-add9-bd82c0b4b170	OpenBLAS	{"OpenBLAS is an optimized BLAS implementation. Currently, OpenBLAS supports RISC-V Vector 0.7.1 for Xuantie C920.\n\nFor RISC-V Vector 1.0 version, OpenBLAS team already update Level-1 and Level-2 BLAS kernel from RVV 0.7.1 intrinsic to 1.0 intrinsic. The next work is developing the Level-3 BLAS kernels (e.g. gemm) for RISC-V Vector 1.0."}	OpenBLAS is an optimized BLAS implementation. Currently, OpenBLAS supports RISC-V Vector 0.7.1 for Xuantie C920.\n\nFor RISC-V Vector 1.0 version, OpenBLAS team already update Level-1 and Level-2 BLAS kernel from RVV 0.7.1 intrinsic to 1.0 intrinsic. The next work is developing the Level-3 BLAS kernels (e.g. gemm) for RISC-V Vector 1.0.	{}	2022	Term 3	https://github.com/xianyi/OpenBLAS		0	171
358	52faa755-1d30-4692-9796-e6bef09fa236	Open Horizon	{"Open Horizon is a platform for deploying container-based workloads and related machine learning models to edge compute nodes and clusters, especially on the far edge (or device edge).  It actively manages the full lifecycle of those assets autonomously, and regardless of a node's connected state.\n\nThis Spring 2022 Term, we have two openings: in our Documentation Working Group where we're working to create processes that will create pages that work for both the project and for downstream distributions, and in our Examples Working Group where we're standardizing on a convention for third-party contributed containerized application packaging."}	Open Horizon is a platform for deploying container-based workloads and related machine learning models to edge compute nodes and clusters, especially on the far edge (or device edge).  It actively manages the full lifecycle of those assets autonomously, and regardless of a node's connected state.\n\nThis Spring 2022 Term, we have two openings: in our Documentation Working Group where we're working to create processes that will create pages that work for both the project and for downstream distributions, and in our Examples Working Group where we're standardizing on a convention for third-party contributed containerized application packaging.	{go,bash,jekyll,markdown,make,yaml,json,git,travis,scala,jenkins}	2021	Term 1	https://github.com/open-horizon	https://www.lfedge.org/projects/openhorizon/	3300000	139
357	75533c12-1487-40b0-858c-42babeabf782	CNCF - Vitess: Add complete parsing support for Spatial MySQL functions	{"Vitess is a database clustering system for horizontal scaling of MySQL. One of the key goals of Vitess is to emulate MySQL behavior even while running multiple MySQL instances so that ORMs and frameworks work seamlessly. Vitess has its own in-built SQL-parser which it uses to understand the query and represent as structs for further processing. As of now, a lot of spatial MySQL functions are not parsed correctly and result in syntax errors. The task of the mentee would be to add parsing support for such functions and features which can be found at https:","",dev.mysql.com,doc,refman,8.0,en,spatial-analysis-functions.html}	Vitess is a database clustering system for horizontal scaling of MySQL. One of the key goals of Vitess is to emulate MySQL behavior even while running multiple MySQL instances so that ORMs and frameworks work seamlessly. Vitess has its own in-built SQL-parser which it uses to understand the query and represent as structs for further processing. As of now, a lot of spatial MySQL functions are not parsed correctly and result in syntax errors. The task of the mentee would be to add parsing support for such functions and features which can be found at https://dev.mysql.com/doc/refman/8.0/en/spatial-analysis-functions.html	{go,sql,yacc,compiler,lexers}	2022	Term 2	https://github.com/vitessio/vitess/issues/8604	https://vitess.io/	0	107
578	06a1fd86-1f54-4828-b39d-6d165f790498	RISC-V Education Apprenticeship	{"Got a flair for helping other learn about RISC-V? Unlock your creative potential and make a global impact! RISC-V International invites you to join our vibrant, international community and play a pivotal role in boosting our support for individuals new to RISC-V. RISC-V International is looking for an individual willing to help the community and technical program managers in a variety of tasks. A good candidate will be an independent self starter who can create documentation, is an excellent researcher, with program management and content creation experience.  See what its like to work in different departments of a Open Standard Project. Don’t miss this extraordinary opportunity to grow your skills and make a difference with this 30 hour a week, remote role. Apply now!"}	Got a flair for helping other learn about RISC-V? Unlock your creative potential and make a global impact! RISC-V International invites you to join our vibrant, international community and play a pivotal role in boosting our support for individuals new to RISC-V. RISC-V International is looking for an individual willing to help the community and technical program managers in a variety of tasks. A good candidate will be an independent self starter who can create documentation, is an excellent researcher, with program management and content creation experience.  See what its like to work in different departments of a Open Standard Project. Don’t miss this extraordinary opportunity to grow your skills and make a difference with this 30 hour a week, remote role. Apply now!	{}	2023	Term 3	https://riscv.org/learn/		0	171
276	e49f92f3-4cca-4f10-a0c2-806df1ea63b5	CNCF - Thanos: Run a community Thanos demo instance	{"Description: Thanos is a distributed system that has a user interface written in React. Let's create a community instance with continuous integration for easy testing of how Thanos works. Also, it could serve as a testing ground for new React components. A server is provided by CNCF (https:","",github.com,cncf,cluster,issues,190).}	Description: Thanos is a distributed system that has a user interface written in React. Let's create a community instance with continuous integration for easy testing of how Thanos works. Also, it could serve as a testing ground for new React components. A server is provided by CNCF (https://github.com/cncf/cluster/issues/190).	{linux,ansible,python,shell}	2022	Term 1	https://github.com/thanos-io/thanos/issues/4606	https://thanos.io/	300000	26
368	4853174a-267d-4cd4-a62d-6e68d0c338b1	CNCF - WasmEdge: Node API support for WasmEdge QuickJS	{"The [WasmEdge QuickJS runtime](https:","",wasmedge.org,book,en,dev,"js.html) is a secure, fast, and lightweight JavaScript runtime for cloud-native applications. Compared with more established JavaScript runtimes like Nodejs and Deno, the WasmEdge QuickJS runtime provides runtime isolation and security at a very low overhead. In order for WasmEdge QuickJS to be more widely adopted, it needs to support [nodejs](https:","",wasmedge.org,book,en,dev,js,"nodejs.html) applications. WasmEdge QuickJS already supports [NPM and CJS modules](https:","",wasmedge.org,book,en,dev,js,npm.html).}	The [WasmEdge QuickJS runtime](https://wasmedge.org/book/en/dev/js.html) is a secure, fast, and lightweight JavaScript runtime for cloud-native applications. Compared with more established JavaScript runtimes like Nodejs and Deno, the WasmEdge QuickJS runtime provides runtime isolation and security at a very low overhead. In order for WasmEdge QuickJS to be more widely adopted, it needs to support [nodejs](https://wasmedge.org/book/en/dev/js/nodejs.html) applications. WasmEdge QuickJS already supports [NPM and CJS modules](https://wasmedge.org/book/en/dev/js/npm.html).	{javascript,rust}	2022	Term 3	https://github.com/WasmEdge/WasmEdge/issues/1745	https://wasmedge.org/	0	31
365	da1162c6-2aaf-496f-9f23-a96a3e52c277	CNCF - WasmEdge: Support serialize and deserialize in WasmEdge	{"WasmEdge can load the WASM binary and instantiate into WASM module instances for execution. In an use case, we need to serialize the loaded WASM data structure back into the encoded WASM binary, or deserialize the serialized one into the WASM data structure in WasmEdge. With the serializing mechanism, WasmEdge can control the WASM binary wisely such as caching or snapshotting."}	WasmEdge can load the WASM binary and instantiate into WASM module instances for execution. In an use case, we need to serialize the loaded WASM data structure back into the encoded WASM binary, or deserialize the serialized one into the WASM data structure in WasmEdge. With the serializing mechanism, WasmEdge can control the WASM binary wisely such as caching or snapshotting.	{webassembly}	2022	Term 3	https://github.com/WasmEdge/WasmEdge/issues/1741	https://wasmedge.org/	0	31
579	1809398d-6c7b-4f30-add5-90e784a35d89	Add F-extension support to "Ripes" RISC-V Micro-Architectural Visual Educational Simulator	{"The core goal of this mentorship consists in implementing the much requested feature of adding support for the F-extension to Ripes. This involves two major steps: first, constructing inside Ripes source code a working implementation of the extension for correctly simulating the instructions with VSRTL (a Ripes-specific, Hardware-Description-Language-like C++ Domain Specific Language created ad-hoc for Ripes); second, implement the required additional Ripes GUI interface for correctly visualizing meaningful insights into the inner working of cores implementing the F-extension.\n\nThe mentee should expect to fully complete all complementary tasks to the development process, as writing and running comprehensive tests and updating the documentation to reflect the latest version of the project."}	The core goal of this mentorship consists in implementing the much requested feature of adding support for the F-extension to Ripes. This involves two major steps: first, constructing inside Ripes source code a working implementation of the extension for correctly simulating the instructions with VSRTL (a Ripes-specific, Hardware-Description-Language-like C++ Domain Specific Language created ad-hoc for Ripes); second, implement the required additional Ripes GUI interface for correctly visualizing meaningful insights into the inner working of cores implementing the F-extension.\n\nThe mentee should expect to fully complete all complementary tasks to the development process, as writing and running comprehensive tests and updating the documentation to reflect the latest version of the project.	{}	2023	Term 3	https://github.com/mortbopet/Ripes		0	171
374	0eb98f34-bfd8-4ba1-b9e5-47fc67b6fd41	CNCF - Kyverno: Policy Exceptions	{"Enable flexible management of policy exceptions without requiring changes to the policy definitions."}	Enable flexible management of policy exceptions without requiring changes to the policy definitions.	{go,kubernetes}	2022	Term 3	https://github.com/kyverno/kyverno/issues/2627	https://kyverno.io/	0	17
419	ce8883ce-9e32-4337-8fe0-5c51fed758e4	CNCF - Linkerd: Prototype multi-cluster service discovery and operations	{"Description: When deploying a multi-cluster resource one has to perform certain contortions such as providing a list of other clusters to each cluster. This places a dependency ordering on spinning up new clusters and a requirement for application operators to coordinate with cluster operators.\n\nExpected Outcome: Develop a prototype where each cluster only needs to reference a common service definition to discover peers without knowledge of the names or even number of other clusters."}	Description: When deploying a multi-cluster resource one has to perform certain contortions such as providing a list of other clusters to each cluster. This places a dependency ordering on spinning up new clusters and a requirement for application operators to coordinate with cluster operators.\n\nExpected Outcome: Develop a prototype where each cluster only needs to reference a common service definition to discover peers without knowledge of the names or even number of other clusters.	{kubernetes,rust,go}	2023	Term 1	https://github.com/linkerd/linkerd2/issues/7566	https://linkerd.io	600000	61
580	a61f6cdb-98b4-43c9-8ca2-ea9bb5d5c470	CNCF - Meshery: Overhaul UX Design System	{"Meshery (https:","","meshery.io) is a self-service engineering platform, Meshery enables collaborative design and operation of cloud native infrastructure. The Meshery Design System is a flexible, scalable design system built on the foundations of accessibility, beautiful design, and consistent user experience.\n- Expected outcome: Rebuild the Meshery Design System so that it provides the open source building blocks to design and implement consistent, accessible, and delightful product experiences."}	Meshery (https://meshery.io) is a self-service engineering platform, Meshery enables collaborative design and operation of cloud native infrastructure. The Meshery Design System is a flexible, scalable design system built on the foundations of accessibility, beautiful design, and consistent user experience.\n- Expected outcome: Rebuild the Meshery Design System so that it provides the open source building blocks to design and implement consistent, accessible, and delightful product experiences.	{figma}	2023	Term 3	https://github.com/meshery/meshery/issues/8347	https://meshery.io/	0	186
375	c6dff087-9b4d-4ff5-865d-abd876974534	CNCF - Volcano: Integrate Volcano with Ray	{"Volcano has supported a lot of mainstream computing platforms such as Spark and TensorFlow. As [Ray](https:","",github.com,ray-project,"ray) is a new and popular computing platform, Volcano should integrate with it."}	Volcano has supported a lot of mainstream computing platforms such as Spark and TensorFlow. As [Ray](https://github.com/ray-project/ray) is a new and popular computing platform, Volcano should integrate with it.	{go,volcano}	2022	Term 3	https://github.com/volcano-sh/volcano/issues/2429	https://volcano.sh/	0	34
378	02972292-469d-431a-96be-149a04ea2746	CNCF - Volcano: Support hot update daemon log level	{"Users have no ways to update log level of Volcano components now, which is difficult to track bugs especially in the production environment. Your task is to achieve it."}	Users have no ways to update log level of Volcano components now, which is difficult to track bugs especially in the production environment. Your task is to achieve it.	{go,volcano}	2022	Term 3	https://github.com/volcano-sh/volcano/issues/2430	https://volcano.sh/	0	34
420	4d9d8e17-8484-4c3e-9210-bb911633f57c	CNCF - KubeEdge: Design and implement the KubeEdge Dashboard	{"Description: Users now can use K8s API or Kubectl to talk to KubeEdge, in this project we will design and implement the KubeEdge dashboard, so users can talk to KubeEdge cluster through UI.\n\nExpected Outcome: Create the KubeEdge dashboard, users can view and operate the resource through UI."}	Description: Users now can use K8s API or Kubectl to talk to KubeEdge, in this project we will design and implement the KubeEdge dashboard, so users can talk to KubeEdge cluster through UI.\n\nExpected Outcome: Create the KubeEdge dashboard, users can view and operate the resource through UI.	{kubernetes,kubeedge,javascript,html}	2023	Term 1	https://github.com/kubeedge/dashboard/issues/1	https://kubeedge.io/en/	300000	74
383	bb0ff695-3d54-4ce2-b93c-3ab92842b3ee	CNCF - Kyverno: Implement new custom JMESPath filters	{"Kyverno uses JMESPath to perform filtering and selection of JSON content in a flexible and advanced way. Many custom filters have been implemented specifically for Kyverno to date. Implement two more filters which fill needed gaps in Kyverno today: a grouping filter and an index locator.\n\nUpstream Issue (URL):\n  - https:","",github.com,kyverno,kyverno,issues,"3981 \n  - https:","",github.com,kyverno,kyverno,issues,4336}	Kyverno uses JMESPath to perform filtering and selection of JSON content in a flexible and advanced way. Many custom filters have been implemented specifically for Kyverno to date. Implement two more filters which fill needed gaps in Kyverno today: a grouping filter and an index locator.\n\nUpstream Issue (URL):\n  - https://github.com/kyverno/kyverno/issues/3981 \n  - https://github.com/kyverno/kyverno/issues/4336	{go}	2022	Term 3	https://github.com/kyverno/kyverno/issues/3981	https://kyverno.io/	0	17
272	da75a8fa-0174-47f2-a5be-0a31c62b053f	CNCF - Meshery: Workflow engine (extended)	{"Integrate a new architectural component into Meshery: a workflow engine. This project involves shifting Meshery off of bitcask and off of sqlite over to postgres using gorm (golang). Interns will familiarize with concepts of orchestration engines, including chaining workflows, and content lifecycle management."}	Integrate a new architectural component into Meshery: a workflow engine. This project involves shifting Meshery off of bitcask and off of sqlite over to postgres using gorm (golang). Interns will familiarize with concepts of orchestration engines, including chaining workflows, and content lifecycle management.	{go,react}	2022	Term 1	https://github.com/meshery/meshery/issues/3934	https://meshery.io/	300000	186
388	e6105cb2-4948-4ba4-b923-4cd4e88027bf	Electron FDC3	{"Update FDC3-electron to support FDC3 2.0\nCertify FDC3-electron on FDC3 1.2 and 2.0 \nRelease fdc3-electron into mainstream repositories (NPM)\nInclude 3+ samples in fdc3-electron as default\nPoint fdc3-electron to FINOS global directory if existing\nWork with FDC3 maintainers to “approve” fdc3-electron as reference implementation and link into fdc3.finos.org"}	Update FDC3-electron to support FDC3 2.0\nCertify FDC3-electron on FDC3 1.2 and 2.0 \nRelease fdc3-electron into mainstream repositories (NPM)\nInclude 3+ samples in fdc3-electron as default\nPoint fdc3-electron to FINOS global directory if existing\nWork with FDC3 maintainers to “approve” fdc3-electron as reference implementation and link into fdc3.finos.org	{}	2023	Term 1	https://github.com/finos/electron-fdc3	https://fdc3.finos.org	0	144
588	989d0ad3-976a-4514-b2fc-34e9e6081567	CNCF - Konveyor: Extend use-case of detecting deprecated Kubernetes API usage	{"Konveyor provides a [unified experience](https:","",github.com,konveyor,enhancements,tree,master,enhancements,"unified_experience) of tools to help organizations modernize their applications at scale to Kubernetes and cloud-native technologies. We are looking for help on extending a use-case of detecting usage of [deprecated and removed Kubernetes APIs](https:","",kubernetes.io,docs,reference,using-api,deprecation-guide,") in applications.  This work will involve determining what API resources have been deprecated or removed in each version of Kubernetes and then building [Analyzer Rules](https:","",github.com,konveyor,analyzer-lsp,blob,main,docs,"rules.md) to be contributed to our [Rulesets repository](https:","",github.com,konveyor,"rulesets), curation or development of sample applications in Golang, Java, and YAML to aid test scenarios, and documentation to help show a guided walkthrough of this capability.  You can see the beginning of this use-case being addressed with a [sample rule](https:","",github.com,konveyor,analyzer-lsp,blob,main,"rule-example.yaml#L42-L45) in this [demo of analyzer-lsp](https:","",github.com,konveyor,analyzer-lsp,tree,"main#quick-demo). The development environment is based on Golang and Kubernetes. A minikube instance will work well for local development on Linux or Mac systems."}	Konveyor provides a [unified experience](https://github.com/konveyor/enhancements/tree/master/enhancements/unified_experience) of tools to help organizations modernize their applications at scale to Kubernetes and cloud-native technologies. We are looking for help on extending a use-case of detecting usage of [deprecated and removed Kubernetes APIs](https://kubernetes.io/docs/reference/using-api/deprecation-guide/) in applications.  This work will involve determining what API resources have been deprecated or removed in each version of Kubernetes and then building [Analyzer Rules](https://github.com/konveyor/analyzer-lsp/blob/main/docs/rules.md) to be contributed to our [Rulesets repository](https://github.com/konveyor/rulesets), curation or development of sample applications in Golang, Java, and YAML to aid test scenarios, and documentation to help show a guided walkthrough of this capability.  You can see the beginning of this use-case being addressed with a [sample rule](https://github.com/konveyor/analyzer-lsp/blob/main/rule-example.yaml#L42-L45) in this [demo of analyzer-lsp](https://github.com/konveyor/analyzer-lsp/tree/main#quick-demo). The development environment is based on Golang and Kubernetes. A minikube instance will work well for local development on Linux or Mac systems.	{go,kubectl}	2023	Term 3	https://github.com/konveyor/operator/issues/251	https://www.konveyor.io/	0	148
406	138e9cac-ec86-43cb-a04f-c2980e3c2865	CNCF - Kubescape: Release engineering: add Kubescape to commonly-requested package managers	{"The Kubescape client binary is built from GitHub using standard patterns. Support for homebrew and krew exists, but users have requested RPM and DEB packages. In this project you will stabilize the delivery of new builds to existing package managers, and implement support for RPM and DEB packages using GitHub Actions.\n\nExpected Outcome: When a new Kubescape version is released, it is available in homebrew, krew, RPM and DEB repositories."}	The Kubescape client binary is built from GitHub using standard patterns. Support for homebrew and krew exists, but users have requested RPM and DEB packages. In this project you will stabilize the delivery of new builds to existing package managers, and implement support for RPM and DEB packages using GitHub Actions.\n\nExpected Outcome: When a new Kubescape version is released, it is available in homebrew, krew, RPM and DEB repositories.	{scripting}	2023	Term 1	https://github.com/kubescape/kubescape/issues/400	https://github.com/kubescape	600000	150
623	2d07fac8-9206-4299-a9fe-a55f366a38f3	CNCF - Volcano: Support NPU accelerator scheduling in Volcano	{"Design and implement NPU accelerator scheduling in Volcano including the vitrualized NPU resource scheduling, job controller enhancement for NPU distributed training, NPU topology scheduling and so on.\n- Expected Outcome:  \n  - design doc and feature user guide\n  - implement NPU topology scheduling\n  - implement job controller enhancement\n  - vitrualized NPU resource scheduling"}	Design and implement NPU accelerator scheduling in Volcano including the vitrualized NPU resource scheduling, job controller enhancement for NPU distributed training, NPU topology scheduling and so on.\n- Expected Outcome:  \n  - design doc and feature user guide\n  - implement NPU topology scheduling\n  - implement job controller enhancement\n  - vitrualized NPU resource scheduling	{go,volcano,kubernetes}	2023	Term 3	https://github.com/volcano-sh/volcano/issues/3019	https://volcano.sh/	0	34
391	d6df3663-ca97-42b5-99b8-79f9a34b8294	Open Mainframe- Mainframe Open Education Project	{"With the Mainframe Open Education Platform just launching, we need help with reviewing content, advising the team if we are missing content, what would help those from Universities, etc to use, and contribute by helping us increase awareness of what we are trying to do for the Mainframe Community."}	With the Mainframe Open Education Platform just launching, we need help with reviewing content, advising the team if we are missing content, what would help those from Universities, etc to use, and contribute by helping us increase awareness of what we are trying to do for the Mainframe Community.	{mainframe}	2023	Term 1	https://github.com/openmainframeproject-internship		0	38
441	6a6e8093-660a-4b6e-8d29-24b8ef70e4f0	CNCF - Karmada: Provide interactive environments for Karmada users	{"Description: Using interactive environments(like killercoda) for users to get started quickly.\n\nExpected Outcome: Implement 2 Karmada examples in killercoda, including a CLI installation example and script installation example, both contains installation and deploying workload to multi-clusters steps."}	Description: Using interactive environments(like killercoda) for users to get started quickly.\n\nExpected Outcome: Implement 2 Karmada examples in killercoda, including a CLI installation example and script installation example, both contains installation and deploying workload to multi-clusters steps.	{kubernetes,karmada}	2023	Term 1	https://github.com/karmada-io/karmada/issues/3085	https://karmada.io/	300000	124
397	6d457c37-68cb-4d52-b9d6-798b09350255	CNCF - Konveyor: Move2Kube Implement a test suite	{"Move2Kube is a command-line tool for automating creation of Infrastructure as code (IaC) artifacts. It has inbuilt support for creating IaC artifacts for replatforming to Kubernetes","OpenShift. The project is actively developed with new features and bug fixes being added and it is being actively used by many users. There is a need for a concrete test suite to test various components of Move2Kube and integrate it to the existing CI","CD pipeline.\n\nExpected Outcome: A test suite for Move2Kube"}	Move2Kube is a command-line tool for automating creation of Infrastructure as code (IaC) artifacts. It has inbuilt support for creating IaC artifacts for replatforming to Kubernetes/OpenShift. The project is actively developed with new features and bug fixes being added and it is being actively used by many users. There is a need for a concrete test suite to test various components of Move2Kube and integrate it to the existing CI/CD pipeline.\n\nExpected Outcome: A test suite for Move2Kube	{golang,jest}	2023	Term 1	https://github.com/konveyor/move2kube/issues/957	https://www.konveyor.io/	300000	148
399	127da817-037b-4225-83a6-3a3eeea8b421	CNCF - NATS: End-to-end example of a multiplayer game using NATS in Unity	{"Description: This project consists of developing an example Unity setup of a multiplayer game using the latest version of the NATS Server.\n\nExpected Outcome: A well documented repository under the `nats-io` GitHub organization that contains the artifacts and sample code of the setup using the .NET NATS Client (https:","",github.com,nats-io,nats.net)}	Description: This project consists of developing an example Unity setup of a multiplayer game using the latest version of the NATS Server.\n\nExpected Outcome: A well documented repository under the `nats-io` GitHub organization that contains the artifacts and sample code of the setup using the .NET NATS Client (https://github.com/nats-io/nats.net)	{unity,nats}	2023	Term 1	https://github.com/nats-io/dot-net-nats-examples/issues/1	https://nats.io	300000	149
589	f603a2e7-9af2-40b2-a74f-109cad843de1	CNCF - OpenKruise: Integrate Openkruise workload with ArgoCD and Helm	{"ArgoCD and Helm are popular tools to delivery k8s workload, yet currently only the k8s built-in workload are supported out-of-box for ArgoCD and Helm. OpenKruise provide advanced worklood that resemble with the built-in workload,  users can use OpenKruise workload with ArgoCD and Helm, yet they cannot tell ArgoCD and Helm whether Openkruise workload is ready or not. \n- Expected Outcome:\n  - Improve ArgoCD integration by writing custom lua script to tell whether OpenKruise workload is healthy. The lua script can be submited to the Argo-CD repository.\n  - Improve Helm intergration by building a job container that can check whether OpenKruise workload is healthy during helm install","upgrade process."}	ArgoCD and Helm are popular tools to delivery k8s workload, yet currently only the k8s built-in workload are supported out-of-box for ArgoCD and Helm. OpenKruise provide advanced worklood that resemble with the built-in workload,  users can use OpenKruise workload with ArgoCD and Helm, yet they cannot tell ArgoCD and Helm whether Openkruise workload is ready or not. \n- Expected Outcome:\n  - Improve ArgoCD integration by writing custom lua script to tell whether OpenKruise workload is healthy. The lua script can be submited to the Argo-CD repository.\n  - Improve Helm intergration by building a job container that can check whether OpenKruise workload is healthy during helm install/upgrade process.	{lua,docker,kubernetes}	2023	Term 3	https://github.com/openkruise/kruise/issues/1345	https://openkruise.io	0	152
280	60e31adc-64b2-4ddf-9e09-01c64350aac1	CNCF - KubeEdge: Plans for Node Group Management	{"Description: In edge computing scenarios, nodes are geographically distributed. The same application may be deployed on nodes at different locations. We have plans for achieving the feature of Pod Scheduling among node groups."}	Description: In edge computing scenarios, nodes are geographically distributed. The same application may be deployed on nodes at different locations. We have plans for achieving the feature of Pod Scheduling among node groups.	{kubernetes,kubeedge}	2022	Term 1	https://github.com/kubeedge/kubeedge/issues/2756	https://kubeedge.io/	300000	74
403	a222f58a-08ee-4727-80c8-41c4d6f5a2a9	CNCF - LitmusChaos: Improve code quality and add unit tests of litmus chaos components	{"Description:  [LitmusChaos](https:","","litmuschaos.io) is an open-source Chaos Engineering platform that enables teams to identify weaknesses & potential outages in infrastructures by inducing chaos tests in a controlled way. This project aims to improve the code quality of the golang components of litmus chaos and refactor the codebase for adding the unit test cases.\n\nExpected Outcome: This will help the project to improve code quality, enhance the unit test suite, and identification of weaknesses"}	Description:  [LitmusChaos](https://litmuschaos.io) is an open-source Chaos Engineering platform that enables teams to identify weaknesses & potential outages in infrastructures by inducing chaos tests in a controlled way. This project aims to improve the code quality of the golang components of litmus chaos and refactor the codebase for adding the unit test cases.\n\nExpected Outcome: This will help the project to improve code quality, enhance the unit test suite, and identification of weaknesses	{go,kubernetes}	2023	Term 1	https://github.com/litmuschaos/litmus/issues/3892	https://litmuschaos.io/	480000	13
404	a0958ddf-1fd6-4c8e-887f-adb28639a9f4	CNCF - Thanos: Add query observability for new promql engine	{"Description: The new [Thanos Promql Engine](https:","",github.com,thanos-community,"promql-engine) lacks observability down to operator level and we don't have a way to track each operator's performance. This project aims to extend the `Explain` method of each operator, and return an operator tree with time taken recorded. Then Thanos Query UI could then visualize the operator trace.\n\nExpected Outcome: Add a button in Query UI that when enabled will show query tree + how much time has been spent in each operator"}	Description: The new [Thanos Promql Engine](https://github.com/thanos-community/promql-engine) lacks observability down to operator level and we don't have a way to track each operator's performance. This project aims to extend the `Explain` method of each operator, and return an operator tree with time taken recorded. Then Thanos Query UI could then visualize the operator trace.\n\nExpected Outcome: Add a button in Query UI that when enabled will show query tree + how much time has been spent in each operator	{go,react}	2023	Term 1	https://github.com/thanos-community/promql-engine/issues/106	https://thanos.io/	300000	26
405	d338ee93-e767-4f44-a0ea-02dbf803a55a	CNCF - Vitess: Add complete parsing support for Spatial MySQL functions III	{"Description: Vitess is a database clustering system for horizontal scaling of MySQL. One of the key goals of Vitess is to emulate MySQL behavior even while running multiple MySQL instances so that ORMs and frameworks work seamlessly. Vitess has its own in-built SQL-parser which it uses to understand the query and represent as structs for further processing. As of now, a lot of spatial MySQL functions are not parsed correctly and result in syntax errors. The task of the mentee would be to add parsing support for such functions and features which can be found at https:","",dev.mysql.com,doc,refman,8.0,en,spatial-analysis-functions.html}	Description: Vitess is a database clustering system for horizontal scaling of MySQL. One of the key goals of Vitess is to emulate MySQL behavior even while running multiple MySQL instances so that ORMs and frameworks work seamlessly. Vitess has its own in-built SQL-parser which it uses to understand the query and represent as structs for further processing. As of now, a lot of spatial MySQL functions are not parsed correctly and result in syntax errors. The task of the mentee would be to add parsing support for such functions and features which can be found at https://dev.mysql.com/doc/refman/8.0/en/spatial-analysis-functions.html	{go,sql,yacc,compilers,lexers}	2023	Term 1	https://github.com/vitessio/vitess/issues/8604	https://vitess.io/	300000	107
409	369f081d-398e-4ce8-b645-e9605b62326a	CNCF - KubeArmor: Adding OpenTelemetry Support	{"Description: To integrate KubeArmor with OpenTelemetry, an adapter needs to be created. OpenTelemetry is a standard for telemetry data, including distributed tracing, metrics, and logs, and has an SDK and a collector component that can run on Kubernetes. Applications can directly expose OpenTelemetry data through in-app instrumentation using the OpenTelemetry SDK. The collector can then gather data from multiple applications in a cluster and send it to various backends for storage and visualization, such as Jaeger.\n\nExpected Outcome: The mentee's task is to develop an OpenTelemetry adapter for KubeArmor that can receive logs, alerts, and telemetry from the kubearmor-relay-service and convert it into the OpenTelemetry format. They are also expected to create documentation and usage guides that describe how to set up and use the adapter, as well as demonstrate the integration with a backend that supports OpenTelemetry."}	Description: To integrate KubeArmor with OpenTelemetry, an adapter needs to be created. OpenTelemetry is a standard for telemetry data, including distributed tracing, metrics, and logs, and has an SDK and a collector component that can run on Kubernetes. Applications can directly expose OpenTelemetry data through in-app instrumentation using the OpenTelemetry SDK. The collector can then gather data from multiple applications in a cluster and send it to various backends for storage and visualization, such as Jaeger.\n\nExpected Outcome: The mentee's task is to develop an OpenTelemetry adapter for KubeArmor that can receive logs, alerts, and telemetry from the kubearmor-relay-service and convert it into the OpenTelemetry format. They are also expected to create documentation and usage guides that describe how to set up and use the adapter, as well as demonstrate the integration with a backend that supports OpenTelemetry.	{go,opentelemetry}	2023	Term 1	https://github.com/kubearmor/KubeArmor/issues/894	https://kubearmor.io/	300000	9
413	50cdbd65-e0cd-4c0f-8c63-6bd5c603ba89	CNCF - KubeEdge: Cloud-Robotic AI Benchmarking for Edge-cloud Collaborative Lifelong Learning	{"Description: Based on real-world datasets provided by industry members of KubeEdge SIG AI, the issue aims to build a lifelong learning benchmarking on KubeEdge-Ianvs. Namely, it aims to help all Edge AI application developers to validate and select the best-matched algorithm of lifelong learning.\n\nExpected Outcome: The benchmark includes: 1) Work together to release a new dataset to the public! 2) Implement critical algorithm or system metrics, e.g., BWT, FWT and thoughput; 3) (Optional) Develop a baseline algorithm for this benchmark."}	Description: Based on real-world datasets provided by industry members of KubeEdge SIG AI, the issue aims to build a lifelong learning benchmarking on KubeEdge-Ianvs. Namely, it aims to help all Edge AI application developers to validate and select the best-matched algorithm of lifelong learning.\n\nExpected Outcome: The benchmark includes: 1) Work together to release a new dataset to the public! 2) Implement critical algorithm or system metrics, e.g., BWT, FWT and thoughput; 3) (Optional) Develop a baseline algorithm for this benchmark.	{tensorflow,pytorch,python,kubernetes}	2023	Term 1	https://github.com/kubeedge/ianvs/issues/48	https://kubeedge.io/en/	300000	74
281	0e4c9797-2dc5-4621-b46a-f1b7371a2495	CNCF - Kubernetes SIG Network: Documentation assessment	{"Description: Gateway API is an evolution of Kubernetes Ingress and Service networking that aims to upgrade and improve these APIs. This project is to have a docs assessment performed, to help us come with a plan for improving our documentaion. In particular, we're looking for someone to look at the content organization, the clarity of the language and concepts, and to make sure it's as readable as possible for both implementors and end users. You'll be working with the mentors and maintainers of the project, with a stretch goal being to make the changes you produce in the initial assessment."}	Description: Gateway API is an evolution of Kubernetes Ingress and Service networking that aims to upgrade and improve these APIs. This project is to have a docs assessment performed, to help us come with a plan for improving our documentaion. In particular, we're looking for someone to look at the content organization, the clarity of the language and concepts, and to make sure it's as readable as possible for both implementors and end users. You'll be working with the mentors and maintainers of the project, with a stretch goal being to make the changes you produce in the initial assessment.	{documentation,english,git}	2022	Term 1	https://github.com/kubernetes-sigs/gateway-api/issues/1003	kubernetes.io	300000	75
412	a00294be-06a0-4e66-a2a5-6e2dfb3a097c	CNCF - Kyverno: Kubernetes Validating Admission Policy Support	{"Description: Kubernetes Validating Admission Policy Support\nExpected Outcome: Kyverno support for ValidatingAdmissionPolicy in one of the identified proposals."}	Description: Kubernetes Validating Admission Policy Support\nExpected Outcome: Kyverno support for ValidatingAdmissionPolicy in one of the identified proposals.	{go,kubernetes}	2023	Term 1	https://github.com/kyverno/kyverno/issues/5441	https://kyverno.io/	300000	17
411	59afc794-c33e-4930-a5b8-eb3abd8d9896	CNCF - Kyverno: Pod Security Admission Integrations	{"Description: Integrate Kubernetes Pod Security with Kyverno - Part II\nExpected Outcome: PR sent to kubernetes","kubernetes containing necessary changes to implement the behavior on the Kyverno side."}	Description: Integrate Kubernetes Pod Security with Kyverno - Part II\nExpected Outcome: PR sent to kubernetes/kubernetes containing necessary changes to implement the behavior on the Kyverno side.	{go,kubernetes}	2023	Term 1	https://github.com/kyverno/kyverno/issues/6144	https://kyverno.io/	300000	17
599	8c7c6769-edea-4ecd-8fb4-53aa1dacb070	CNCF - KubeEdge: Support latest version in keink and run demo on keink	{"keink (KubeEdge IN kind) is a project for running local KubeEdge clusters using Docker container \\"nodes\\", so developers can install a multi-node\n  edge cluster in one node. Now we need to support the latest version installation in keink. \n- Expected Outcome: keink can install the latest version of KubeEdge and developers can quickly use keink to run kubeedge, and then develop applications on KubeEdge."}	keink (KubeEdge IN kind) is a project for running local KubeEdge clusters using Docker container "nodes", so developers can install a multi-node\n  edge cluster in one node. Now we need to support the latest version installation in keink. \n- Expected Outcome: keink can install the latest version of KubeEdge and developers can quickly use keink to run kubeedge, and then develop applications on KubeEdge.	{kubeedge,kubernetes,golang}	2023	Term 3	https://github.com/kubeedge/keink/issues/8	https://kubeedge.io/en/	0	74
418	a04cfbe4-4dde-4c7e-8b70-9570639b48a7	CNCF - Thanos: Querying Apache Parquet files with PromQL	{"Description: The new [Thanos PromQL Engine](https:","",github.com,thanos-community,"promql-engine) has a sufficient separation between the syntax tree and the execution plan to allow us to query arbitrary data sources. In this project we would like to explore ways to query data stored in Apache Parquet files.\n\nExpected Outcome: The Thanos PromQL engine can query timeseries data from Apache Parquet files."}	Description: The new [Thanos PromQL Engine](https://github.com/thanos-community/promql-engine) has a sufficient separation between the syntax tree and the execution plan to allow us to query arbitrary data sources. In this project we would like to explore ways to query data stored in Apache Parquet files.\n\nExpected Outcome: The Thanos PromQL engine can query timeseries data from Apache Parquet files.	{go}	2023	Term 1	https://github.com/thanos-community/promql-engine/issues/167	https://thanos.io/	300000	26
421	dbce5279-d029-46f3-b117-9e9dd7f84bd6	CNCF - Thanos: Series Cardinality API	{"Description: Prometheus has a TSDB stats API https:","",prometheus.io,docs,prometheus,latest,querying,api,"#tsdb-stats which contains information about series cardinality and the API is supported by Thanos. However, it can only return 10 results per stats, which is not flexible to track the arbitrary metrics. This project aims to design and implement APIs that expose cardinalities. Stretch goal can be to add cardinality explorer page to Thanos UI.\n\nExpected Outcome: New Thanos APIs to expose series cardinality."}	Description: Prometheus has a TSDB stats API https://prometheus.io/docs/prometheus/latest/querying/api/#tsdb-stats which contains information about series cardinality and the API is supported by Thanos. However, it can only return 10 results per stats, which is not flexible to track the arbitrary metrics. This project aims to design and implement APIs that expose cardinalities. Stretch goal can be to add cardinality explorer page to Thanos UI.\n\nExpected Outcome: New Thanos APIs to expose series cardinality.	{go,react}	2023	Term 1	https://github.com/thanos-io/thanos/issues/6007	https://thanos.io/	300000	26
283	5c828028-f91c-4969-b4de-9efdb27bb869	CNCF - Tremor: Database Connectors	{"Description: Connectors are tremors interface to the outside world, they allow us to integrate with third-party systems. Currently, tremor only has a limited set of connectors for databases, we support s3 and google cloud storage for object stores, and have a k","v connector that offers a simple integrated key-value store. While this is a good starting point interfacing with more databases will make tremor easier to use for our end users. The primary target will be integrating with Yandex Clickhouse."}	Description: Connectors are tremors interface to the outside world, they allow us to integrate with third-party systems. Currently, tremor only has a limited set of connectors for databases, we support s3 and google cloud storage for object stores, and have a k/v connector that offers a simple integrated key-value store. While this is a good starting point interfacing with more databases will make tremor easier to use for our end users. The primary target will be integrating with Yandex Clickhouse.	{rust,database,testing}	2022	Term 1	https://github.com/tremor-rs/tremor-runtime/issues/1453	https://www.tremor.rs/	540000	20
426	db63c23a-2b41-40e0-a833-cf0e2c33c739	CNCF - Kubescape: Implement security controls based on penetration testing best practices	{"Kubescape covers different hardening guidelines around Kubernetes: NSA-CISA, MITRE and CIS. Detection capabilities of potential security issues could be even more enriched by researching pen-testing tools and practices regarding Kubernetes and implementing these as controls. An example pen-test writeup is https:","",hacktricks.boitatech.com.br,pentesting,"pentesting-kubernetes. This and others could help define a set of “offensive” controls to complement the “defensive” controls we have today.\n\nExpected Outcome: ~10 controls for detecting challenges that would commonly be found in a cluster penetration test. Documentation on how they were selected and how to use them."}	Kubescape covers different hardening guidelines around Kubernetes: NSA-CISA, MITRE and CIS. Detection capabilities of potential security issues could be even more enriched by researching pen-testing tools and practices regarding Kubernetes and implementing these as controls. An example pen-test writeup is https://hacktricks.boitatech.com.br/pentesting/pentesting-kubernetes. This and others could help define a set of “offensive” controls to complement the “defensive” controls we have today.\n\nExpected Outcome: ~10 controls for detecting challenges that would commonly be found in a cluster penetration test. Documentation on how they were selected and how to use them.	{cybersecurity,rego}	2023	Term 1	https://github.com/kubescape/kubescape/issues/1072	https://github.com/kubescape	300000	150
424	f502b839-a804-4a6c-8da5-3985ce25883e	CNCF - Kyverno: Artifact Hub listing of Kyverno Policy Library	{"Description: Develop a system to reflect all Kyverno Policies in the community library on Artifact Hub\nExpected Outcome: All Kyverno policies searchable on Artifact Hub with an extensible system for future use"}	Description: Develop a system to reflect all Kyverno Policies in the community library on Artifact Hub\nExpected Outcome: All Kyverno policies searchable on Artifact Hub with an extensible system for future use	{go}	2023	Term 1	https://github.com/kyverno/policies/issues/491	https://kyverno.io/	600000	17
422	9e0f01ab-615f-44ed-b65b-0f1296037a48	CNCF - OpenKruise: Support customize arbitary fields of workload subset in UnitedDeployment	{"Description: UnitedDeployment in OpenKruise enable users to manage a set of k8s workloads in whole while be able to customize the topology and replicas of each workload. This project extends the customization capability to arbitary workload fields by adding common patch fields, so that \neach subset of UnitedDeployment can have different metadata, container configuration etc.\n\nExpected Outcome: Support generate patches for new creating pods of each subset workload while the users can rollout and scale the UnitedDeployment in whole."}	Description: UnitedDeployment in OpenKruise enable users to manage a set of k8s workloads in whole while be able to customize the topology and replicas of each workload. This project extends the customization capability to arbitary workload fields by adding common patch fields, so that \neach subset of UnitedDeployment can have different metadata, container configuration etc.\n\nExpected Outcome: Support generate patches for new creating pods of each subset workload while the users can rollout and scale the UnitedDeployment in whole.	{go,kubernetes}	2023	Term 1	https://github.com/openkruise/kruise/issues/811	https://openkruise.io	300000	152
287	9ec7fc38-8850-46b9-8b7b-4d648a903bb3	CNCF - OpenTelemetry: Help drive OpenTelemetry PHP to Beta	{"Description: Help to drive our project board for OpenTelemetry PHP. This includes validating spec compliance and writing PHP code to implement some of these features"}	Description: Help to drive our project board for OpenTelemetry PHP. This includes validating spec compliance and writing PHP code to implement some of these features	{php}	2022	Term 1	https://github.com/open-telemetry/opentelemetry-php/projects/1	https://opentelemetry.io/	300000	47
429	33e0747c-4ab8-4074-aa90-3b908b3a588e	CNCF - Cloud Native Buildpacks: Pack Performance enhancements	{"Pack is the reference implementation of a Cloud Native Buildpacks platform used to build application images in multiple organizations. Because all developers want their code to build and deploy as quickly as possible, small speedups in pack can have significant benefits, and slow-downs in pack are undesirable. Today, pack has no benchmark suite that would safe-guard against regressions in execution speed.\n\nExpected Outcome: The mentee will create a benchmark suite around some critical path sections identified with help from maintainers. The mentee will be supported in applying profiling tools to identify possible speedups, hopefully leading to at least one user-facing performance improvement."}	Pack is the reference implementation of a Cloud Native Buildpacks platform used to build application images in multiple organizations. Because all developers want their code to build and deploy as quickly as possible, small speedups in pack can have significant benefits, and slow-downs in pack are undesirable. Today, pack has no benchmark suite that would safe-guard against regressions in execution speed.\n\nExpected Outcome: The mentee will create a benchmark suite around some critical path sections identified with help from maintainers. The mentee will be supported in applying profiling tools to identify possible speedups, hopefully leading to at least one user-facing performance improvement.	{go,git}	2023	Term 1	https://github.com/buildpacks/pack/issues/1610	https://buildpacks.io	300000	82
292	ccfa5ba6-b052-4144-8d28-38fdba5f66c7	Hyperledger Umbra - Chaos Monkey Engineering in Umbra Scalability Tests	{"Umbra has been developed in the past 2 years thanks to successful internship projects, its final target is the development of a platform for emulating blockchain projects and conducting academic research. \n\nThis year the goal of the internship project is to unleash the full potential of Umbra, therefore running large scale tests added to chaos engineering mechanisms. \n\nThe intern is going to work on mechanism that enable chaos engineering in Umbra, execute large scale tests in cloud providers (thousands of nodes), and present measurement reports of blockchain and infrastructure metrics.\n\nThe goal is to enhance Umbra to a stable release, prove its full potential, and produce material to be reproducible by different academic institutions teach and perform research with the developed Umbra experiments.\n\nExpected Outcome\nEnhance Umbra source code, enable a stable experimentation environment for Umbra in cloud environments with large topologies (e.g., thousands of nodes), generate chaos engineering mechanisms in Umbra, capture the events in dashboards, and generate comprehensive reports detailing an Umbra experiment."}	Umbra has been developed in the past 2 years thanks to successful internship projects, its final target is the development of a platform for emulating blockchain projects and conducting academic research. \n\nThis year the goal of the internship project is to unleash the full potential of Umbra, therefore running large scale tests added to chaos engineering mechanisms. \n\nThe intern is going to work on mechanism that enable chaos engineering in Umbra, execute large scale tests in cloud providers (thousands of nodes), and present measurement reports of blockchain and infrastructure metrics.\n\nThe goal is to enhance Umbra to a stable release, prove its full potential, and produce material to be reproducible by different academic institutions teach and perform research with the developed Umbra experiments.\n\nExpected Outcome\nEnhance Umbra source code, enable a stable experimentation environment for Umbra in cloud environments with large topologies (e.g., thousands of nodes), generate chaos engineering mechanisms in Umbra, capture the events in dashboards, and generate comprehensive reports detailing an Umbra experiment.	{fabric,iroha,blockchain,mininet}	2021	Term 2	https://github.com/hyperledger-labs/umbra	https://wiki.hyperledger.org/display/INTERN/Chaos+Monkey+Engineering+in+Umbra+Scalability+Tests	540000	110
430	891b4b92-0a78-409e-8b90-dcd58d126225	CNCF - Karmada: Bundle third-party resources into the Resource Interpreter framework	{"Description: Karmada's Resource Interpreter Framework is designed for interpreting resource structure. It consists of built-in and customized interpreters. Karmada could bundle some popular and open-sourced resources so that users can save the effort to customize them.\n\nExpected Outcome: The resources from projects, including Argo Workflow","Flux CD",Kyverno,"OpenKurise, could be bundled in Karmada, and the corresponding documentation should also be supplemented."}	Description: Karmada's Resource Interpreter Framework is designed for interpreting resource structure. It consists of built-in and customized interpreters. Karmada could bundle some popular and open-sourced resources so that users can save the effort to customize them.\n\nExpected Outcome: The resources from projects, including Argo Workflow/Flux CD/Kyverno/OpenKurise, could be bundled in Karmada, and the corresponding documentation should also be supplemented.	{go}	2023	Term 1	https://github.com/karmada-io/karmada/issues/3087	https://karmada.io/	300000	124
444	b7accea9-22bc-44e7-bac0-2f7b986fa626	CNCF - KubeArmor: Rancher Plugin Integration	{"Description: The goal is to create an extension for Rancher, a Kubernetes management platform, which will enable interaction with KubeArmor. The extension will have the capability to install KubeArmor, allow for the management of security policies, and provide monitoring of workload behavior through alerts and telemetry.\n\nExpected Outcome: Rancher plugin address the following points: Install KubeArmor within Rancher, document and demonstrate the usage."}	Description: The goal is to create an extension for Rancher, a Kubernetes management platform, which will enable interaction with KubeArmor. The extension will have the capability to install KubeArmor, allow for the management of security policies, and provide monitoring of workload behavior through alerts and telemetry.\n\nExpected Outcome: Rancher plugin address the following points: Install KubeArmor within Rancher, document and demonstrate the usage.	{javascript,rancher,grafana}	2023	Term 1	https://github.com/kubearmor/KubeArmor/issues/992	https://kubearmor.io/	300000	9
445	ddc368b7-1e24-42ed-9e30-02abdf6fcd33	CNCF - Kubewarden: Kubewarden SDKs feature parity	{"Description:  Kubewarden currently allow policy writers to use 4 different programming languages. Therefore, there are 4 SDKs to be maintained. However, they lack feature parity. In other words, some SDK have feature that have features not available in other SDKs. It's necessary to map what are the features missing between the Go and Rust SDKs and implement some of them. For that, it is necessary to read and understand what is done in the Rust SDK and implement the equivalent in the Go SDK.\n\nExpected Outcome: Map all the features missing between the Go and Rust SKDs and implement some of the missing features"}	Description:  Kubewarden currently allow policy writers to use 4 different programming languages. Therefore, there are 4 SDKs to be maintained. However, they lack feature parity. In other words, some SDK have feature that have features not available in other SDKs. It's necessary to map what are the features missing between the Go and Rust SDKs and implement some of them. For that, it is necessary to read and understand what is done in the Rust SDK and implement the equivalent in the Go SDK.\n\nExpected Outcome: Map all the features missing between the Go and Rust SKDs and implement some of the missing features	{go,rust,kubernetes}	2023	Term 1	https://github.com/kubewarden/kubewarden-controller/issues/392	https://www.kubewarden.io	300000	154
442	e5da551f-8a3d-42ec-8c00-e9ae10a86aa2	CNCF - Kyverno: OCI references support	{"Description: Use OCI References in image verification\nExpected Outcome: PR sent to kyverno","kyverno implementing support for OCI references in verifyImages rules"}	Description: Use OCI References in image verification\nExpected Outcome: PR sent to kyverno/kyverno implementing support for OCI references in verifyImages rules	{go,kubernetes}	2023	Term 1	https://github.com/kyverno/kyverno/issues/6142	https://kyverno.io/	300000	17
440	2ebb39fd-3497-44f3-90d7-e95b444b2bc8	CNCF - WasmEdge: Unified WasmEdge tools	{"Description: WasmEdge provides two tools in the release assets: `wasmedgec` and `wasmedge`. However, providing multiple tools will make it too complicated to use. That's why we want a simple entry point, `wasmedge`. As its subcommands, all the tools above should be collected into this new tool.\n\nExpected Outcome: A document to explain the new WasmEdge tools, a test suite covers the implementation details, and implement `wasmedge run` and `wasmedge compile` featues."}	Description: WasmEdge provides two tools in the release assets: `wasmedgec` and `wasmedge`. However, providing multiple tools will make it too complicated to use. That's why we want a simple entry point, `wasmedge`. As its subcommands, all the tools above should be collected into this new tool.\n\nExpected Outcome: A document to explain the new WasmEdge tools, a test suite covers the implementation details, and implement `wasmedge run` and `wasmedge compile` featues.	{wasm}	2023	Term 1	https://github.com/WasmEdge/WasmEdge/issues/2226	https://wasmedge.org/	300000	31
301	1ef6172d-3ea5-468e-99b2-0dcd05ea719b	Hyperledger - Privacy-preserving federated learning framework based on Fabric and Aries	{"This project utilizes Hyperledger Fabric to develop a federated learning framework with committee consensus. Hyperledger Fabric is used for storing and tracing global model update exchange. A customized committee consensus mechanism is required to reduce a load of consensus computation by selecting a random smaller subset of nodes to participate in consensus each round. In order to protect the identity of local data owners, the proposed framework will be integrated with Indy, Aries, and Ursa stack projects to offer self-sovereign identity. The mentee will work with the project mentors to gather and validate the requirements, and design the appropriate technical solution."}	This project utilizes Hyperledger Fabric to develop a federated learning framework with committee consensus. Hyperledger Fabric is used for storing and tracing global model update exchange. A customized committee consensus mechanism is required to reduce a load of consensus computation by selecting a random smaller subset of nodes to participate in consensus each round. In order to protect the identity of local data owners, the proposed framework will be integrated with Indy, Aries, and Ursa stack projects to offer self-sovereign identity. The mentee will work with the project mentors to gather and validate the requirements, and design the appropriate technical solution.	{}	2022	Term 2	https://github.com/OpenMined/PyAriesFL	https://wiki.hyperledger.org/pages/viewpage.action?pageId=62242785	600000	1
452	49749be9-5a67-4b2b-9312-7def13ae98b8	CNCF - Harbor: Regex replication rules	{"Description: Add more versatile replication filters\n\nExpected Outcome: Implement regex capability when defining relication rules, update documentation and present the functionality"}	Description: Add more versatile replication filters\n\nExpected Outcome: Implement regex capability when defining relication rules, update documentation and present the functionality	{angular,golang,javascript}	2023	Term 1	https://github.com/goharbor/harbor/issues/8614	https://goharbor.io	300000	158
447	1b2c5ff4-d6ea-4ca5-b138-75fce03407b4	CNCF - Karmada: Enhance Karmada testing coverage	{"Description: Karmada would like to improve the UT coverage of the code to better maintain the quality of the code and reduce the introduction of defects.\n\nExpected Outcome: Increase the UT coverage rate to 65% (currently, the UT coverage rate is [43%](https:","",app.codecov.io,gh,karmada-io,"karmada) ), increase the code coverage rate by about 20%."}	Description: Karmada would like to improve the UT coverage of the code to better maintain the quality of the code and reduce the introduction of defects.\n\nExpected Outcome: Increase the UT coverage rate to 65% (currently, the UT coverage rate is [43%](https://app.codecov.io/gh/karmada-io/karmada) ), increase the code coverage rate by about 20%.	{go,git}	2023	Term 1	https://github.com/karmada-io/karmada/issues/3086	https://karmada.io/	300000	124
450	570b1bba-206d-47ac-9667-22268ff7a6d9	CNCF - Kubescape: Build debugging capabilities for Helm	{"The Go standard templating package (`text","template`) is the base on which Helm templates are built. We wish to be able to backtrack lines and fields in objects after rendering Helm charts. This would help users of Helm to be able to understand quickly where different security issues in the final object are coming from in the source. To do this, the `text","template` package should be extended to include debug markers that point from the output lines to the input lines.\n\nExpected Outcome: Propose and implement an extension to the Go package which solves this."}	The Go standard templating package (`text/template`) is the base on which Helm templates are built. We wish to be able to backtrack lines and fields in objects after rendering Helm charts. This would help users of Helm to be able to understand quickly where different security issues in the final object are coming from in the source. To do this, the `text/template` package should be extended to include debug markers that point from the output lines to the input lines.\n\nExpected Outcome: Propose and implement an extension to the Go package which solves this.	{go}	2023	Term 1	https://github.com/helm/helm/issues/11552	https://github.com/kubescape	600000	150
446	484542b0-84d6-43e3-b3fe-16fb2624f1b2	CNCF - WasmEdge: Streaming data processing with WasmEdge	{"Description: WasmEdge would like to integrate WasmEdge as an alternative runtime for Fluvio. We would like to create a compile-time feature for the [fluvio-smartengine](https:","",github.com,infinyon,fluvio,tree,master,crates,"fluvio-smartengine) crate. Once this feature is turned on, the compiler will choose to embed WasmEdge into the binary build using the [WasmEdge Rust SDK](https:","",wasmedge.org,book,en,sdk,"rust.html).\n\nExpected Outcome: A complete PR and a demo app that uses WasmEdge to process streaming data using a Tensorflow or Pytorch model"}	Description: WasmEdge would like to integrate WasmEdge as an alternative runtime for Fluvio. We would like to create a compile-time feature for the [fluvio-smartengine](https://github.com/infinyon/fluvio/tree/master/crates/fluvio-smartengine) crate. Once this feature is turned on, the compiler will choose to embed WasmEdge into the binary build using the [WasmEdge Rust SDK](https://wasmedge.org/book/en/sdk/rust.html).\n\nExpected Outcome: A complete PR and a demo app that uses WasmEdge to process streaming data using a Tensorflow or Pytorch model	{rust}	2023	Term 1	https://github.com/WasmEdge/WasmEdge/issues/2231	https://wasmedge.org/	660000	31
530	73d90321-62b3-498e-bf37-d899ec99df9e	CNCF - Armada: Build interfaces around Postgres for Armada	{"Open source projects should not be hard coded to a particular Database. Armada currently only allows users to use Postgres. This project is to build interfaces around our connections to Postgres so we can allow other databases.\n\nExpected outcomes:\n  - A interface is created that allows Armada to interact with any SQL database without exposing implementation details of postgres\n  - increase Test coverage"}	Open source projects should not be hard coded to a particular Database. Armada currently only allows users to use Postgres. This project is to build interfaces around our connections to Postgres so we can allow other databases.\n\nExpected outcomes:\n  - A interface is created that allows Armada to interact with any SQL database without exposing implementation details of postgres\n  - increase Test coverage	{go,sql}	2023	Term 2	https://github.com/armadaproject/armada/issues/2121		300000	176
453	4a96c735-6480-4464-8b33-4f9c58ba1005	CNCF - Harbor: Harbor Robot accounts with full Harbor API access	{"Description: Robot accounts should be allowed to access the full Harbor API (more of a UI thing)\n\nExpected Outcome: Implement a way to configure and fully documented with examples usecase how to setup Harbor Robot accounts with full or managed access to Harbor"}	Description: Robot accounts should be allowed to access the full Harbor API (more of a UI thing)\n\nExpected Outcome: Implement a way to configure and fully documented with examples usecase how to setup Harbor Robot accounts with full or managed access to Harbor	{angular,golang,javascript,clarity}	2023	Term 1	https://github.com/goharbor/harbor/issues/8723	https://goharbor.io	300000	158
308	61cdc089-fa7e-44df-aac3-18eaaf8aa31f	Random Test Generation for RISC-V Vector Extension	{"\\"Following the ratification of RVV specification, the software simulation framework and RTL designs for RVV have been released. Given the complexity of RVV design and the combinations of its configuration, both developers and users need a golden test suite to verify the correctness of the respective RVV implementations and simulations.\n\nRIOS lab thus introduces this project and contributes to RISC-V foundation. In this project, you will be designing a RVV test generator to help verify the functionality of either simulator or ASIC design of RISC-V vector processors. This project has two phases. In the first phase, we will write the a constant test suite generator for RVV that automatically produces the same test streams given a fixed RVV configuration. This would help debug and ensure reproducibility needed by developers. While in phase 2, we will extend the test generator to produce random vector test to cover the corner case of the RVV design.\n\nRVV Spec: https:","",github.com,riscv,"riscv-v-spec\nRISC-V CTG: https:","",github.com,riscv-software-src,"riscv-ctg\nRISCOF: https:","",github.com,riscv-software-src,"riscof\nRISCOF Documentation: https:","",riscof.readthedocs.io,en,latest,"intro.html\n\nLearning Objectives:\n1. RVV instruction set architecture\n2. How does vector processor work? \n3. Formal Verification of RVV\n4. Test generation and verification model\n5. Design of advanced random test generation framework"}	"Following the ratification of RVV specification, the software simulation framework and RTL designs for RVV have been released. Given the complexity of RVV design and the combinations of its configuration, both developers and users need a golden test suite to verify the correctness of the respective RVV implementations and simulations.\n\nRIOS lab thus introduces this project and contributes to RISC-V foundation. In this project, you will be designing a RVV test generator to help verify the functionality of either simulator or ASIC design of RISC-V vector processors. This project has two phases. In the first phase, we will write the a constant test suite generator for RVV that automatically produces the same test streams given a fixed RVV configuration. This would help debug and ensure reproducibility needed by developers. While in phase 2, we will extend the test generator to produce random vector test to cover the corner case of the RVV design.\n\nRVV Spec: https://github.com/riscv/riscv-v-spec\nRISC-V CTG: https://github.com/riscv-software-src/riscv-ctg\nRISCOF: https://github.com/riscv-software-src/riscof\nRISCOF Documentation: https://riscof.readthedocs.io/en/latest/intro.html\n\nLearning Objectives:\n1. RVV instruction set architecture\n2. How does vector processor work? \n3. Formal Verification of RVV\n4. Test generation and verification model\n5. Design of advanced random test generation framework	{python}	2022	Term 2	https://github.com/riscv-software-src/riscof	https://riscv.org/risc-v-mentorship-program/	0	171
456	ca622980-cc8c-4f18-8a74-b9a7b4b49e3a	CNCF - TestGrid: Frontend development inside Lit Component Framework	{"Description: [TestGrid](http:","","testgrid.k8s.io) is the test visualization tool attached to Prow to  collate and display historical test results for the k8s and k8s-adjacent\n\nExpected Outcome: Create Lit-based view components for TestGrid (summary, index, etc.) that display data from the API. Implement Jasmine and Storybook testing for these components."}	Description: [TestGrid](http://testgrid.k8s.io) is the test visualization tool attached to Prow to  collate and display historical test results for the k8s and k8s-adjacent\n\nExpected Outcome: Create Lit-based view components for TestGrid (summary, index, etc.) that display data from the API. Implement Jasmine and Storybook testing for these components.	{typescript,css,go}	2023	Term 1	https://github.com/GoogleCloudPlatform/testgrid/issues/1005	https://github.com/GoogleCloudPlatform/testgrid	300000	159
209	3cc2be4b-e208-496c-b801-23ef3bb2716a	ATOM Syntax Highlighting	{"Update the syntax highlighters adding support for macro statements, highlighting commas and parentheses, and support for variables. Also, add new snippets to test different aspects of the syntax highlighting grammar."}	Update the syntax highlighters adding support for macro statements, highlighting commas and parentheses, and support for variables. Also, add new snippets to test different aspects of the syntax highlighting grammar.	{atom}	2021	Term 2	https://github.com/openmainframeproject/atompkg-language-zvm-asm	https://atom.io/users/openmainframeproject	300000	111
480	2c3dfc7d-1a30-4e7c-98ea-545f8149cb07	Hyperledger - Cacti - Ledger Data Sharing with Proof in Besu and Ethereum	{"Cacti provides a protocol and trigger mechanism for the sharing of ledger data or data derived from smart contracts (called \\"view\\", and identified by a \\"view address\\") deployed in a network with another network using relays for communication and consensus-driven proof generation and verification in the end networks. Some parts of the protocol are generic and DLT-agnostic but others, specifically protocol drivers or connectors and core ledger operators (typically built as smart contracts) are DLT-specific. Presently, Cacti provides DLT-specific data sharing mechanisms for Hyperldger Fabric and Corda. The goal of this project is to add similar support, i.e., build modules, plugins, and API for permissioned networks built on Hyperledger Besu. As an optional extension, these mechanisms will then be extrapolated to share data to and from public Ethereum. This will involve designing and basic building blocks or capabilities for view proof generation, proof verification, and access control of remote data requests, in the form of smart contracts. Higher up the stack, the Cacti SDK for Besu must then be augmented to enable network apps to trigger data sharing requests and submitting the responses for validation to local smart contracts."}	Cacti provides a protocol and trigger mechanism for the sharing of ledger data or data derived from smart contracts (called "view", and identified by a "view address") deployed in a network with another network using relays for communication and consensus-driven proof generation and verification in the end networks. Some parts of the protocol are generic and DLT-agnostic but others, specifically protocol drivers or connectors and core ledger operators (typically built as smart contracts) are DLT-specific. Presently, Cacti provides DLT-specific data sharing mechanisms for Hyperldger Fabric and Corda. The goal of this project is to add similar support, i.e., build modules, plugins, and API for permissioned networks built on Hyperledger Besu. As an optional extension, these mechanisms will then be extrapolated to share data to and from public Ethereum. This will involve designing and basic building blocks or capabilities for view proof generation, proof verification, and access control of remote data requests, in the form of smart contracts. Higher up the stack, the Cacti SDK for Besu must then be augmented to enable network apps to trigger data sharing requests and submitting the responses for validation to local smart contracts.	{solidity,ethereum}	2023	Term 2	https://github.com/hyperledger/cacti	https://wiki.hyperledger.org/display/INTERN/Cacti%3A+Ledger+Data+Sharing+with+Proof+in+Besu+and+Ethereum	300000	1
478	b82f2b67-e509-469f-ad2d-e465053dcdbf	Hyperledger - One-stop-shop Hyperledger Fabric performance analysis with Hyperledger Caliper	{"Performance evaluation of DLTs is a complex process stemming from the inherent complexity of distributed systems. One way to mitigate such complexity is the separation of concerns: use task-oriented solutions for different aspects of the process:\n\n- The System Under Test (SUT) is deployed in a representative operating environment.\n- A dedicated, purpose-specific load generator tool submits representative requests to the SUT.\n- The SUT is closely monitored during the load generation process, and the measured data is typically stored for post-mortem analysis. \n- A detailed analysis is performed on the measured data using dedicated data analysis techniques. A summarizing report is the typical final output of the process, containing insights about the SUT.\n\nHyperledger Caliper provides capabilities to integrate lightweight components that can retrieve aggregated results from external components and incorporate them into the generated report.\n\nThe goal of the project is to provide a single entry","exit point to the performance analysis (once the SUT and its monitoring are configured) by:\n\nProviding an open, well-designed, and thoroughly documented side-car service for the detailed performance analysis of distributed Fabric transaction traces.\nAnd integrating it into the Caliper load generation and response measurement process as part of the final report.\nThe project will heavily build on the PSWG's Performance Sandbox, aiming to \\"standardize\\" its flow and methodologies independently"}	Performance evaluation of DLTs is a complex process stemming from the inherent complexity of distributed systems. One way to mitigate such complexity is the separation of concerns: use task-oriented solutions for different aspects of the process:\n\n- The System Under Test (SUT) is deployed in a representative operating environment.\n- A dedicated, purpose-specific load generator tool submits representative requests to the SUT.\n- The SUT is closely monitored during the load generation process, and the measured data is typically stored for post-mortem analysis. \n- A detailed analysis is performed on the measured data using dedicated data analysis techniques. A summarizing report is the typical final output of the process, containing insights about the SUT.\n\nHyperledger Caliper provides capabilities to integrate lightweight components that can retrieve aggregated results from external components and incorporate them into the generated report.\n\nThe goal of the project is to provide a single entry/exit point to the performance analysis (once the SUT and its monitoring are configured) by:\n\nProviding an open, well-designed, and thoroughly documented side-car service for the detailed performance analysis of distributed Fabric transaction traces.\nAnd integrating it into the Caliper load generation and response measurement process as part of the final report.\nThe project will heavily build on the PSWG's Performance Sandbox, aiming to "standardize" its flow and methodologies independently	{git}	2023	Term 2	https://github.com/hyperledger/caliper	https://wiki.hyperledger.org/display/INTERN/One-stop-shop+Hyperledger+Fabric+performance+analysis+with+Hyperledger+Caliper	300000	1
516	78852896-9785-4156-bb9b-bc3c5cb6ed17	CNCF - Konveyor: Add Integration test suite and components testing to Konveyor	{"The Konveyor project helps modernize applications by providing open source tools to rehost, replatform, and refactor applications to Kubernetes and cloud-native technologies.We’re looking for help on building integration tests on application level as well as work on missing parts of Konveyor component tests.There is open testing work to better applications analysis, tasks coverage, more detailed Hub API tests and Hub integration with addons. All of those use the Hub API that is covered with basic tests already. Based on existing Hub API tests, it is expected to continue work to cover more Konveyor functionality with tests.\nThe development environment is based on golang and Kubernetes. A minikube instance will work well for local development on Linux or Mac systems.\n- Expected Outcome:\n  - Integration test suite and components testing added to existing Konveyor upstream automated test suite"}	The Konveyor project helps modernize applications by providing open source tools to rehost, replatform, and refactor applications to Kubernetes and cloud-native technologies.We’re looking for help on building integration tests on application level as well as work on missing parts of Konveyor component tests.There is open testing work to better applications analysis, tasks coverage, more detailed Hub API tests and Hub integration with addons. All of those use the Hub API that is covered with basic tests already. Based on existing Hub API tests, it is expected to continue work to cover more Konveyor functionality with tests.\nThe development environment is based on golang and Kubernetes. A minikube instance will work well for local development on Linux or Mac systems.\n- Expected Outcome:\n  - Integration test suite and components testing added to existing Konveyor upstream automated test suite	{golang}	2023	Term 2	https://github.com/konveyor/tackle2-operator/issues/220	https://www.konveyor.io/	300000	148
484	22b0d4e9-5e68-40e2-b5c1-8508a146a624	Hyperledger - Cacti - Implement Standardized Secure Asset Transfer Protocol	{"Cacti enables two blockchain or DLT networks to transfer digital assets between each other using different mechanisms and modules, inherited from the older Cactus and Weaver projects. The Node Server acts as a container for business logic to orchestrate (or trigger) transactions across multiple networks toward achieving some desired simultaneous state change in those networks. The Relay acts as a gateway, or an ingress","egress appliance, for a given network to send communication to, and receive communication from, another network. Using the Relay or the Node Server or both, one can effect a digital asset transfer from one network to another, sample protocols for which have already been implemented in Cacti with samples for demonstration.\n\nParallelly, there is an ongoing effort to standardize blockchain","DLT interoperation protocols through consultation and brainstorming among experts within the field under the IETF's aegis.\n\nThe goal of this project is to implement the more current SATP draft specification in Cacti, in the process augmenting the Cacti Relay to be a gateway that provides a reference or template for any other standardized implementation. Using other Cacti modules and sample apps, secure transfers of digital assets using SATP must also be demonstrated. If time permits, performance measurements may also be conducted to benchmark the protocol, and specifically the Relay."}	Cacti enables two blockchain or DLT networks to transfer digital assets between each other using different mechanisms and modules, inherited from the older Cactus and Weaver projects. The Node Server acts as a container for business logic to orchestrate (or trigger) transactions across multiple networks toward achieving some desired simultaneous state change in those networks. The Relay acts as a gateway, or an ingress/egress appliance, for a given network to send communication to, and receive communication from, another network. Using the Relay or the Node Server or both, one can effect a digital asset transfer from one network to another, sample protocols for which have already been implemented in Cacti with samples for demonstration.\n\nParallelly, there is an ongoing effort to standardize blockchain/DLT interoperation protocols through consultation and brainstorming among experts within the field under the IETF's aegis.\n\nThe goal of this project is to implement the more current SATP draft specification in Cacti, in the process augmenting the Cacti Relay to be a gateway that provides a reference or template for any other standardized implementation. Using other Cacti modules and sample apps, secure transfers of digital assets using SATP must also be demonstrated. If time permits, performance measurements may also be conducted to benchmark the protocol, and specifically the Relay.	{rust,java,kotlin}	2023	Term 2	https://github.com/hyperledger/cacti	https://wiki.hyperledger.org/display/INTERN/Cacti%3A+Implement+Standardized+Secure+Asset+Transfer+Protocol	540000	1
489	409242eb-183f-42f7-a2d8-77d9b947b380	Hyperledger - Cacti - Polkadot connector	{"Creation of polkadot connector plugin, alongside its cactus-polkadot-all-in-one docker image (hosting a sample polkadot ledger for testing purposes) and the test-tooling package for the AIO\n\nLearning Objectives\n- Hyperledger Cacti codebase\n- Hyperledger Cacti connector design\n- Understanding of various interoperability solutions, design and \nusage of cactus api server\n- Expected Outcome\n- Creation of polkadot connector\n- Creation of polkadot all in one docker image\n- Creation of polkadot test tooling classes\n- Documentation, architecture reference diagrams for the polkadot connector"}	Creation of polkadot connector plugin, alongside its cactus-polkadot-all-in-one docker image (hosting a sample polkadot ledger for testing purposes) and the test-tooling package for the AIO\n\nLearning Objectives\n- Hyperledger Cacti codebase\n- Hyperledger Cacti connector design\n- Understanding of various interoperability solutions, design and \nusage of cactus api server\n- Expected Outcome\n- Creation of polkadot connector\n- Creation of polkadot all in one docker image\n- Creation of polkadot test tooling classes\n- Documentation, architecture reference diagrams for the polkadot connector	{javascript,typescript}	2023	Term 2	https://github.com/hyperledger/cacti	https://wiki.hyperledger.org/display/INTERN/Cacti+-+Polkadot+connector	300000	1
487	b9c6ef64-25f2-4be2-94ef-76e8ae7c768e	Hyperledger - Learning Tokens @ Hyperledger Besu	{"Learning Tokens register the process of transfer and acquisition of knowledge. They track what we know and how we learned it in individual profiles that help personalize education. They measure engagement in the transfer of knowledge, support educational management, and foster Anatole France's words, nine-tenths of education is encouragement.\n\nLearning Tokens recognize, register, and reward community engagement in collective learning while certifying skills acquisition and individual competencies.\n\nIn 2021, we did a global directory of DLT","Blockchain learning places. Last year, we defined artifacts for learning tokens, and now we shall code their Smart Contracts for MOOCs.\n\nLearning Objectives\n- Understand the tokenization of learning experiences.\n- Use the Inter Work Alliance Token Taxonomy Framework, IWA-TTF. \n- Apply IWA Token Designer to create Learning Tokens Templates. \n- Deploy them in Hyperledger Besu.\n- Understand granular learning interactions in MOOCs.\n- Explore simple ways to integrate Learning Tokens and MOOCs.\n\nExpected Outcome\n- Deploy Learning Tokens in Hyperledger Besu.\n- Report about connecting IWA-TTF to Hyperledger Beso\n- Explore alternatives to integrate Learning Tokens into MOOCs.\n- Report about possible alternatives to integrate Learning Tokens into MOOCs"}	Learning Tokens register the process of transfer and acquisition of knowledge. They track what we know and how we learned it in individual profiles that help personalize education. They measure engagement in the transfer of knowledge, support educational management, and foster Anatole France's words, nine-tenths of education is encouragement.\n\nLearning Tokens recognize, register, and reward community engagement in collective learning while certifying skills acquisition and individual competencies.\n\nIn 2021, we did a global directory of DLT/Blockchain learning places. Last year, we defined artifacts for learning tokens, and now we shall code their Smart Contracts for MOOCs.\n\nLearning Objectives\n- Understand the tokenization of learning experiences.\n- Use the Inter Work Alliance Token Taxonomy Framework, IWA-TTF. \n- Apply IWA Token Designer to create Learning Tokens Templates. \n- Deploy them in Hyperledger Besu.\n- Understand granular learning interactions in MOOCs.\n- Explore simple ways to integrate Learning Tokens and MOOCs.\n\nExpected Outcome\n- Deploy Learning Tokens in Hyperledger Besu.\n- Report about connecting IWA-TTF to Hyperledger Beso\n- Explore alternatives to integrate Learning Tokens into MOOCs.\n- Report about possible alternatives to integrate Learning Tokens into MOOCs	{solidity,javascript,truffle,remix}	2023	Term 2	https://github.com/hyperledger-labs/learning-tokens	https://wiki.hyperledger.org/display/INTERN/Learning+Tokens+@+Hyperledger+Besu	300000	1
490	d7f3ec7d-c77b-4bd7-9107-f834434a5722	Hyperledger - Telecom Decentralized Identities Network	{"The proposed integration of Decentralized Identity (DID) with telecom services aims to provide customers with a secure and convenient way to manage their digital identities, while also offering telecom companies an opportunity to become trusted identity brokers. The integration addresses the challenge of maintaining customer engagement by enabling customers to have a single identity that they can use to access services. The integration also presents a significant opportunity for telecom companies to become identity brokers, leveraging their existing customer base and regulatory relationships. As issuers of verified KYC information, telecom companies can support identity verification by offering APIs to third-party service providers. The integration of DID with telecom services has the potential to revolutionize the telecom industry and offers significant benefits to customers and operators alike."}	The proposed integration of Decentralized Identity (DID) with telecom services aims to provide customers with a secure and convenient way to manage their digital identities, while also offering telecom companies an opportunity to become trusted identity brokers. The integration addresses the challenge of maintaining customer engagement by enabling customers to have a single identity that they can use to access services. The integration also presents a significant opportunity for telecom companies to become identity brokers, leveraging their existing customer base and regulatory relationships. As issuers of verified KYC information, telecom companies can support identity verification by offering APIs to third-party service providers. The integration of DID with telecom services has the potential to revolutionize the telecom industry and offers significant benefits to customers and operators alike.	{blockchain,react,javascript,docker,git}	2023	Term 2	https://github.com/hyperledger	https://wiki.hyperledger.org/pages/viewpage.action?pageId=80778712	300000	1
184	25ea21de-7f06-43ae-a4a6-52262ce1193c	CNCF - Buildpacks: Embed source metadata in OCI image	{"As a buildpack user using `pack`, I would like to be able to inspect the final app image and determine where the source of the code is located as well as what version (keeping in consideration SCM systems) was used."}	As a buildpack user using `pack`, I would like to be able to inspect the final app image and determine where the source of the code is located as well as what version (keeping in consideration SCM systems) was used.	{go,docker}	2021	Term 2	https://github.com/buildpacks/pack/issues/1139	https://buildpacks.io/	300000	27
493	10894ab0-eab2-4f14-b1b3-5f354410b48f	Hyperledger - Explorer User Interface redesign	{"Blockchain Explorer provides a graphical user interface to explore blocks, transactions, nodes, channels, chaincodes in a blockchain network.  The goal of this project is to redesign the  user interface of hyperledger explorer project to provide an easy as seamless experience for the user. The goal is to make Blockchain explorer as the common explorer tool for all the hyperledger DLT projects.\n\nLearning Objectives\n- To implement all the user research , analysis and design techniques to design a very compelling user experience for hyperledger fabric.\n- Gain an indepth understanding of Hyperledger DLT projects.\n- Developer user experience and user interface design skills.\n\nExpected Outcome\n- User flows , wireframes, style guide and high fidelity mockup of the new user interface design for Hyperledger Explorer"}	Blockchain Explorer provides a graphical user interface to explore blocks, transactions, nodes, channels, chaincodes in a blockchain network.  The goal of this project is to redesign the  user interface of hyperledger explorer project to provide an easy as seamless experience for the user. The goal is to make Blockchain explorer as the common explorer tool for all the hyperledger DLT projects.\n\nLearning Objectives\n- To implement all the user research , analysis and design techniques to design a very compelling user experience for hyperledger fabric.\n- Gain an indepth understanding of Hyperledger DLT projects.\n- Developer user experience and user interface design skills.\n\nExpected Outcome\n- User flows , wireframes, style guide and high fidelity mockup of the new user interface design for Hyperledger Explorer	{figma,wireframes}	2023	Term 2	https://github.com/hyperledger-labs/blockchain-explorer	https://wiki.hyperledger.org/display/INTERN/Hyperledger+Explorer+User+Interface+redesign	300000	1
494	d48f755f-598d-4b0f-898e-e1390bec48b1	Hyperledger - Improve Kubernetes Operators support for Fablo	{"Fablo is a Hyperledger Labs project that aims to manage a local Hyperledger Fabric network on Docker. Previous edition of Hyperledger Mentorship Program resulted with the implementation of the initial, experimental support for Kubernetes Operators (using Bevel Operator).\n\nThe overall objective of the current mentorship program is to improve test coverage for Kubernetes Operators support for Fablo, implement crucial missing features and fix most important issues that come up as a result of testing.\n\nFirst, the mentee is responsible for implementing Bash scripts with automatic tests of a running HLF network, in a similar manner it is already done for Fablo on Docker. Tests need to verify the support for Fablo’s features, like network lifecycle support, using Fablo hooks, managing complex HLF network topologies, and others. On the basis of it, the mentee is going to create a set of issues and start working on them in the second part of the internship. Additionally the project includes implementing a few Fablo features supporting testing:  `fablo invoke","query`, `fablo enroll` and `fablo chaincode list`."}	Fablo is a Hyperledger Labs project that aims to manage a local Hyperledger Fabric network on Docker. Previous edition of Hyperledger Mentorship Program resulted with the implementation of the initial, experimental support for Kubernetes Operators (using Bevel Operator).\n\nThe overall objective of the current mentorship program is to improve test coverage for Kubernetes Operators support for Fablo, implement crucial missing features and fix most important issues that come up as a result of testing.\n\nFirst, the mentee is responsible for implementing Bash scripts with automatic tests of a running HLF network, in a similar manner it is already done for Fablo on Docker. Tests need to verify the support for Fablo’s features, like network lifecycle support, using Fablo hooks, managing complex HLF network topologies, and others. On the basis of it, the mentee is going to create a set of issues and start working on them in the second part of the internship. Additionally the project includes implementing a few Fablo features supporting testing:  `fablo invoke/query`, `fablo enroll` and `fablo chaincode list`.	{bash,docker}	2023	Term 2	https://github.com/hyperledger-labs/fablo	https://wiki.hyperledger.org/display/INTERN/Improve+Kubernetes+Operators+support+for+Fablo	300000	1
315	300dbe6d-15b3-4974-9b67-5c997da220c4	Open Mainframe- SQL Pushdown Enhancements	{"The SQL Pushdown is an open-source Python based library providing familiar interfaces of Dataframe-mapper and Sklearn data transformation objects to allow to perform data pre-processing tasks in database servers (e.g.: Z hosted DB2, Postgres and others) as opposed to in-memory. The project will focus on building a test suite of the existing library and on extension of the library features with additional functions such as conversion of Scikit-based pipelines, serialization, and others."}	The SQL Pushdown is an open-source Python based library providing familiar interfaces of Dataframe-mapper and Sklearn data transformation objects to allow to perform data pre-processing tasks in database servers (e.g.: Z hosted DB2, Postgres and others) as opposed to in-memory. The project will focus on building a test suite of the existing library and on extension of the library features with additional functions such as conversion of Scikit-based pipelines, serialization, and others.	{sql,python}	2022	Term 2	https://github.com/openmainframeproject-internship		900000	38
497	7f9476df-4621-46ec-ae0e-635d19ee5bad	Hyperledger - Upgrade Fabric network from 1.4.x to 2.2.x using Hyperledger Bevel	{"yperledger Bevel supports production-worthy deployments for Hyperledger Fabric. And all production systems need to undergo updates which can often be live-updates. This project aims to complete a live upgrade of a Hyperledger Fabric network from version 1.4.x to 2.2.x using Hyperledger bevel and document the steps as well as make any changes needed to automate any steps possible."}	yperledger Bevel supports production-worthy deployments for Hyperledger Fabric. And all production systems need to undergo updates which can often be live-updates. This project aims to complete a live upgrade of a Hyperledger Fabric network from version 1.4.x to 2.2.x using Hyperledger bevel and document the steps as well as make any changes needed to automate any steps possible.	{ansible,helm,kubernetes}	2022	Term 2	https://github.com/hyperledger/bevel	https://wiki.hyperledger.org/display/INTERN/Upgrade+Fabric+network+from+1.4.x+to+2.2.x+using+Hyperledger+Bevel	300000	1
500	d2429c71-6a57-4de1-ba57-ac4f808d020c	Bringing the "Sliderules" Cheat Sheets to the Web	{"This mentorship builds on the \\"\\"Sliderule\\"\\" Encoder and Decoder Cheat Sheets project, which is a highly visual and condensed view of the information contained in the RISC-V specification documents for consultation and education purposes. The sliderules are especially designed to facilitate manual encoding and decoding of assembly, placing instructions coherently in the binary address space. This original project was compiled by hand on a Google Sheets document and later exported to PDF: this method has the advantage of producing aesthetically pleasant results, but it is slow to create and expand, error prone, non interactive and version controllable only in a very limited way. \n\nThe general goal of this mentorship consists in extending the information contained in the slide rule to RV64GC and obtaining a web page that interactively shows subsets of the sliderules to the user thanks to an information selection interface and is able to export PDFs from it. The main goal is to create a tool that serves as an education oriented consultive tool that documents the RISC-V instruction set in a more user-friendly way. The project shall be deployed from a GitHub repository."}	This mentorship builds on the ""Sliderule"" Encoder and Decoder Cheat Sheets project, which is a highly visual and condensed view of the information contained in the RISC-V specification documents for consultation and education purposes. The sliderules are especially designed to facilitate manual encoding and decoding of assembly, placing instructions coherently in the binary address space. This original project was compiled by hand on a Google Sheets document and later exported to PDF: this method has the advantage of producing aesthetically pleasant results, but it is slow to create and expand, error prone, non interactive and version controllable only in a very limited way. \n\nThe general goal of this mentorship consists in extending the information contained in the slide rule to RV64GC and obtaining a web page that interactively shows subsets of the sliderules to the user thanks to an information selection interface and is able to export PDFs from it. The main goal is to create a tool that serves as an education oriented consultive tool that documents the RISC-V instruction set in a more user-friendly way. The project shall be deployed from a GitHub repository.	{html,css,javascript,webassembly}	2023	Term 2	https://drive.google.com/drive/folders/10kOrfr8z3Vp7Z7fNlOpVD5ipJrGupK5U?usp=share_link		780000	171
506	cda1496b-0c36-4c1b-98b5-dee5d95f494a	Hyperledger - Iroha 2 DSL	{"Iroha 2 currently has WASM-based smartcontracts. WASM, however, imposes a maintainer burden on the user, so upgrading these takes work.\n\nWe also have an ISI based on easily upgradeable smartcontract facilities, which are difficult to compose into arbitrary logic but require no manual intervention for upgrading to newer versions of Iroha.\n\nWe would like to involve the community in creating a domain-specific language for creating and composing logic made out of ISI.\n\nIroha 2 DSL is our most important project. Therefore, we would like it to be considered with top priority if not all Iroha-based projects can be considered for the internship program.\n\nLearning Objectives\n- Explore domain-specific optimisations not ordinarily possible in regular compiler design.\n- Explore the design of ergonomic languages.\n- Improve the understanding of Rust.\n\nExpected Outcome\n- (Optional) Well-defined Iroha DSL grammar\n- A binary application (compiler) that parses DSL and produces ISI according to a well-defined grammar\n- A set of example files written in the DSL covering the following simple use-cases\n  -Cross-blockchain transfer\n  -Modification of metadata\n  -Transference of permission tokens"}	Iroha 2 currently has WASM-based smartcontracts. WASM, however, imposes a maintainer burden on the user, so upgrading these takes work.\n\nWe also have an ISI based on easily upgradeable smartcontract facilities, which are difficult to compose into arbitrary logic but require no manual intervention for upgrading to newer versions of Iroha.\n\nWe would like to involve the community in creating a domain-specific language for creating and composing logic made out of ISI.\n\nIroha 2 DSL is our most important project. Therefore, we would like it to be considered with top priority if not all Iroha-based projects can be considered for the internship program.\n\nLearning Objectives\n- Explore domain-specific optimisations not ordinarily possible in regular compiler design.\n- Explore the design of ergonomic languages.\n- Improve the understanding of Rust.\n\nExpected Outcome\n- (Optional) Well-defined Iroha DSL grammar\n- A binary application (compiler) that parses DSL and produces ISI according to a well-defined grammar\n- A set of example files written in the DSL covering the following simple use-cases\n  -Cross-blockchain transfer\n  -Modification of metadata\n  -Transference of permission tokens	{}	2023	Term 2	https://github.com/hyperledger/iroha	https://wiki.hyperledger.org/display/INTERN/Iroha+2%3A+DSL	600000	1
514	e4be265d-fa05-46b7-8fad-2585b6a76082	CNCF - Kyverno: ValidatingAdmissionPolicy support, Phase 2	{"Kyverno is working towards support of ValidatingAdmissionPolicy (CEL admission). Extend this support for other items such as CLI, reporting, and auto-generating ValidatingAdmissionPolicies from Kyverno policies.\n- Expected outcome: Extended support and integration with ValidatingAdmissionPolicies"}	Kyverno is working towards support of ValidatingAdmissionPolicy (CEL admission). Extend this support for other items such as CLI, reporting, and auto-generating ValidatingAdmissionPolicies from Kyverno policies.\n- Expected outcome: Extended support and integration with ValidatingAdmissionPolicies	{kubernetes,kyverno}	2023	Term 2	https://github.com/kyverno/kyverno/issues/7088	https://kyverno.io/	300000	17
508	09fd65f4-bba3-44bd-9b06-384434f393db	LF Networking - 2023 FD.io Website UX Improvements	{"This is a multi-phase project to improve the FD.io web site to modernize the UX experience across all modern computing platforms (desktop, laptop, tablet, mobile phone).\nThere are 2 areas of the FD.io web site which require UX improvement:\nFD.io Landing Page\nCSIT Dashboard (CDASH)"}	This is a multi-phase project to improve the FD.io web site to modernize the UX experience across all modern computing platforms (desktop, laptop, tablet, mobile phone).\nThere are 2 areas of the FD.io web site which require UX improvement:\nFD.io Landing Page\nCSIT Dashboard (CDASH)	{css,html,python,gerrit,hugo,netlify,wordpress}	2023	Term 2	https://github.com/fdio/site	https://fd.io	600000	161
512	082568c5-9af1-4ab3-9730-25336f16f145	LF Networking - 2023 OpenDaylight Website Refresh	{"This internship will provide the opportunity to learn open source community standards, values, and processes by joining and contributing to the OpenDaylight community.  The intern will interact with community members across many corporations large and small.  Application of modern UX analysis, design, and website implementation technologies will provide ample opportunity to grow the applicant's skill set. The applicant will also learn some basic marketing principles in working with the LFN marketing team on this project."}	This internship will provide the opportunity to learn open source community standards, values, and processes by joining and contributing to the OpenDaylight community.  The intern will interact with community members across many corporations large and small.  Application of modern UX analysis, design, and website implementation technologies will provide ample opportunity to grow the applicant's skill set. The applicant will also learn some basic marketing principles in working with the LFN marketing team on this project.	{css,html,python,gerrit,hugo,netlify,wordpress}	2023	Term 2	https://git.opendaylight.org/gerrit/	https://opendaylight.org	300000	161
522	08d245cb-001f-4292-90eb-e8895189c77a	CNCF - KubeArmor: Manage KubeArmor policies using OCI registry and use OCI hooks for container event	{"The feature aims to manage KubeArmor policies using OCI registry and use OCI hooks to get container events. Currently, KubeArmor uses a UNIX domain socket file to watch for container events, but the proposed feature aims to use OCI hooks instead.\n* Expected Outcome: To provide a more secure and efficient way of managing KubeArmor policies by leveraging OCI registry. Storing policies in OCI registries will make it easier to distribute policies across multiple clusters and environments. Using OCI hooks will also reduce the overhead of monitoring container events and make it easier to integrate KubeArmor with other container runtimes."}	The feature aims to manage KubeArmor policies using OCI registry and use OCI hooks to get container events. Currently, KubeArmor uses a UNIX domain socket file to watch for container events, but the proposed feature aims to use OCI hooks instead.\n* Expected Outcome: To provide a more secure and efficient way of managing KubeArmor policies by leveraging OCI registry. Storing policies in OCI registries will make it easier to distribute policies across multiple clusters and environments. Using OCI hooks will also reduce the overhead of monitoring container events and make it easier to integrate KubeArmor with other container runtimes.	{go,kubernetes,oci}	2023	Term 2	https://github.com/kubearmor/KubeArmor/issues/1130	https://kubearmor.io/	300000	9
520	8d301adf-94d8-4e5d-821d-f904ed15c3f9	CNCF - Strimzi: Proof of Concept of an MQTT to Apache Kafka bridge for producing messages	{"A really common use case we have been seeing is about enabling an IoT scenario with MQTT based devices and using an Apache Kafka cluster as the events and storage platform running on Kubernetes via Strimzi. In order to do that, there is the need to map the MQTT protocol to the custom Apache Kafka one and bridge from one to the other. This project idea is about designing such a mapping and developing a pure [Netty](https:","",github.com,netty,netty,tree,4.1,codec-mqtt,src,main,java,io,netty,handler,codec,"mqtt) based MQTT server component (not a full MQTT broker) able to accept MQTT client connections and handling the corresponding communication based on the [MQTT 3.1.1 specification](http:","",docs.oasis-open.org,mqtt,mqtt,v3.1.1,os,"mqtt-v3.1.1-os.html). Finally, developing the Kafka producer part to get messages from MQTT clients and sending them to an Apache Kafka cluster.\n- Expected outcome: POC source code for an MQTT to Apache Kafka bridge"}	A really common use case we have been seeing is about enabling an IoT scenario with MQTT based devices and using an Apache Kafka cluster as the events and storage platform running on Kubernetes via Strimzi. In order to do that, there is the need to map the MQTT protocol to the custom Apache Kafka one and bridge from one to the other. This project idea is about designing such a mapping and developing a pure [Netty](https://github.com/netty/netty/tree/4.1/codec-mqtt/src/main/java/io/netty/handler/codec/mqtt) based MQTT server component (not a full MQTT broker) able to accept MQTT client connections and handling the corresponding communication based on the [MQTT 3.1.1 specification](http://docs.oasis-open.org/mqtt/mqtt/v3.1.1/os/mqtt-v3.1.1-os.html). Finally, developing the Kafka producer part to get messages from MQTT clients and sending them to an Apache Kafka cluster.\n- Expected outcome: POC source code for an MQTT to Apache Kafka bridge	{kafka,java,mqtt}	2023	Term 2	https://github.com/strimzi/strimzi-kafka-operator/issues/8030	https://strimzi.io/	300000	174
519	1953e512-fa8c-4f0e-9b24-0e6c81a7cd39	CNCF - Thanos: Continuation of add query observability for the new engine	{"We have added solid foundation for query observability in the new engine during the previous LFX mentorship term. Let's continue the awesome work by Pradyumna by implementing other features.\n- Expected outcome: other query observability visualizations are implemented; extra observability data has been added"}	We have added solid foundation for query observability in the new engine during the previous LFX mentorship term. Let's continue the awesome work by Pradyumna by implementing other features.\n- Expected outcome: other query observability visualizations are implemented; extra observability data has been added	{go,react}	2023	Term 2	https://github.com/thanos-community/promql-engine/issues/106	https://thanos.io/	300000	26
529	1a6a7aaf-7436-431b-9131-9422e4b2fb71	CNCF - Kubescape: Store Kubescape configuration scan results as CRs	{"Kubescape is a utility that can scan a Kubernetes cluster and report on its security posture. There is an \\"operator\\" which can be installed in the cluster to perform scheduled scans scan, but this is largely used to send the data to an external service. In this project, you will implement a mechanism in the Kubescape operator to save scan results locally in a custom resource (CR), as well as a watch so that scans can be performed on cluster state changes.\n- Expected Outcome: Having the ability to scan a cluster when it changes, and have the results saved inside the cluster. This will allow users and automations to judge the security posture of changes that are made to the cluster (for example, deployments or rollouts.)"}	Kubescape is a utility that can scan a Kubernetes cluster and report on its security posture. There is an "operator" which can be installed in the cluster to perform scheduled scans scan, but this is largely used to send the data to an external service. In this project, you will implement a mechanism in the Kubescape operator to save scan results locally in a custom resource (CR), as well as a watch so that scans can be performed on cluster state changes.\n- Expected Outcome: Having the ability to scan a cluster when it changes, and have the results saved inside the cluster. This will allow users and automations to judge the security posture of changes that are made to the cluster (for example, deployments or rollouts.)	{go}	2023	Term 2	https://github.com/kubescape/kubescape/issues/1225	http://kubescape.io/	300000	150
531	fbaf3d52-77ee-469c-8eb4-3e0378896159	CNCF - Kubescape: Vulnerability-based Dockerfile generator	{"Kubescape can detect vulnerabilities in a container image. Some can automatically be remediated by changing the base image version (or other package information) inside the Dockerfile which created the image. This project is to automate this remediation.\n- Expected Outcome: An enhancement to Kubescape to generate a Dockerfile that proposes fixes for vulnerabilities found in a container image. This may be by integration with existing open source tools or developing something new."}	Kubescape can detect vulnerabilities in a container image. Some can automatically be remediated by changing the base image version (or other package information) inside the Dockerfile which created the image. This project is to automate this remediation.\n- Expected Outcome: An enhancement to Kubescape to generate a Dockerfile that proposes fixes for vulnerabilities found in a container image. This may be by integration with existing open source tools or developing something new.	{go}	2023	Term 2	https://github.com/kubescape/kubescape/issues/1227	http://kubescape.io/	300000	150
526	884ff3f2-3ea3-4010-8928-ca27bbae219a	CNCF - WasmEdge: Support Tensorflow and PyTorch in WasmEdge’s Python runtime	{"In this project, you will incorporate WasmEdge’s NN (Neural Network) extensions into the Python interpreter. WasmEdge provides C and Rust APIs for guest applications to access host functions in the underlying Tensorflow and PyTorch libraries. You will make those functions accessible from the CPython-based interpreter as Python wrappers. This way, Python applications can do lightweight AI inference on the WasmEdge container.\n- Expected outcome:\n  * Investigate and list all C-based host function APIs for Tensorflow and PyTorch inference in WasmEdge NN.\n  * Create CPython wrappers for those host functions.\n  * Create high-level Python wrapper functions that are ergonomic for Python developers.\n  * Create CI and demo apps to validate the Python wrapper API.\n  * Create detailed documentation and tutorials."}	In this project, you will incorporate WasmEdge’s NN (Neural Network) extensions into the Python interpreter. WasmEdge provides C and Rust APIs for guest applications to access host functions in the underlying Tensorflow and PyTorch libraries. You will make those functions accessible from the CPython-based interpreter as Python wrappers. This way, Python applications can do lightweight AI inference on the WasmEdge container.\n- Expected outcome:\n  * Investigate and list all C-based host function APIs for Tensorflow and PyTorch inference in WasmEdge NN.\n  * Create CPython wrappers for those host functions.\n  * Create high-level Python wrapper functions that are ergonomic for Python developers.\n  * Create CI and demo apps to validate the Python wrapper API.\n  * Create detailed documentation and tutorials.	{wasm,python}	2023	Term 2	https://github.com/WasmEdge/WasmEdge/issues/2471	https://wasmedge.org/	300000	31
535	8c701687-7cde-42fc-8195-08d35fdb5ee8	CNCF - Kubescape: Prometheus exporter for image vulnerabilities	{"Kubescape has a component that runs in-cluster which performs image scanning on all the container images deployed to a cluster. This function is largely used to send the data to an external service.  In this projet, you will develop a Prometheus exporter for the image vulnerability information produced by Kubescape.  This will allow users to access the data from within the cluster, as well as use it for alerting.\n- Expected Outcome: Access to cluster vulnerability data through Prometheus.  For example, you should have the ability to alert on number or percentage of \\"Critical\\" level vulnerabilities in containers running in the cluster."}	Kubescape has a component that runs in-cluster which performs image scanning on all the container images deployed to a cluster. This function is largely used to send the data to an external service.  In this projet, you will develop a Prometheus exporter for the image vulnerability information produced by Kubescape.  This will allow users to access the data from within the cluster, as well as use it for alerting.\n- Expected Outcome: Access to cluster vulnerability data through Prometheus.  For example, you should have the ability to alert on number or percentage of "Critical" level vulnerabilities in containers running in the cluster.	{go,prometheus}	2023	Term 2	https://github.com/kubescape/kubescape/issues/1226	http://kubescape.io/	300000	150
536	85ebe560-e9ee-42fe-9dff-f8dc6a11ef27	CNCF - Kyverno: Kuttl tests for the Kyverno policy library	{"Kyverno has the largest policy library of any policy tool for Kubernetes. Ensuring that policies work effectively across releases of both Kyverno and Kubernetes is important for users. Additionally, these tests can be leveraged in the CI processes ensuring that changes to the Kyverno codebase do not cause regressions which impact areas relevant to these policies. In this mentorship, you will learn how the `kuttl` tool works and write test cases using `kuttl` to cover all policies in the official Kyverno policy library.\n- Expected outcome: All policies have corresponding tests using the `kuttl` tool."}	Kyverno has the largest policy library of any policy tool for Kubernetes. Ensuring that policies work effectively across releases of both Kyverno and Kubernetes is important for users. Additionally, these tests can be leveraged in the CI processes ensuring that changes to the Kyverno codebase do not cause regressions which impact areas relevant to these policies. In this mentorship, you will learn how the `kuttl` tool works and write test cases using `kuttl` to cover all policies in the official Kyverno policy library.\n- Expected outcome: All policies have corresponding tests using the `kuttl` tool.	{kubernetes,kyverno}	2023	Term 2	https://github.com/kyverno/policies/issues/546	https://kyverno.io/	300000	17
537	4735d0fa-229f-43e7-9415-dff9220bf687	CNCF - Service Mesh Performance: IDE Plugin	{"The objective of this project is to develop IDE plugins that can enhance the developer experience while working with Service Mesh Performance Performance Profiles. The proposed plugins will leverage technologies such as golang and cuelang to provide features such as syntax highlighting, auto-completion, validation, and rendering previews for Service Mesh Performance profile and model definitions.\n- Expected outcome:\n- 1. Release VS Code Extension\n- 2. Syntax Highlighting and Auto-completion: The plugin can fetch SMP Model definitions such as cloud-native components and their relationships. This information can be used to provide syntax highlighting and auto-completion for these definitions in the JSON files, making it easier for developers to write error-free code.\n- 3. Validation and Reference: For Meshery MeshModel definitions such as cloud-native components and their relationships, the plugin can use the CUE language to provide validation for the CUE input and preview the rendering result. The plugin can also fetch the SMP Model schemas and display them in the IDE for reference."}	The objective of this project is to develop IDE plugins that can enhance the developer experience while working with Service Mesh Performance Performance Profiles. The proposed plugins will leverage technologies such as golang and cuelang to provide features such as syntax highlighting, auto-completion, validation, and rendering previews for Service Mesh Performance profile and model definitions.\n- Expected outcome:\n- 1. Release VS Code Extension\n- 2. Syntax Highlighting and Auto-completion: The plugin can fetch SMP Model definitions such as cloud-native components and their relationships. This information can be used to provide syntax highlighting and auto-completion for these definitions in the JSON files, making it easier for developers to write error-free code.\n- 3. Validation and Reference: For Meshery MeshModel definitions such as cloud-native components and their relationships, the plugin can use the CUE language to provide validation for the CUE input and preview the rendering result. The plugin can also fetch the SMP Model schemas and display them in the IDE for reference.	{cuelang}	2023	Term 2	https://github.com/service-mesh-performance/service-mesh-performance/issues/379	https://smp-spec.io/	300000	121
539	74cecdf7-e886-4830-8bb0-7814f0d1aa2d	CNCF - WasmEdge: zlib Plugin Support	{"The zlib is required for compiling and running many existing C "," C++ "," Rust apps in Wasm. Most noticeably, it is [needed in the Python port to Wasm](https:","",github.com,python,cpython,issues,"93819). The VMWare Wasm Labs team is using a zlib port from [Singlestore](https:","",github.com,singlestore-labs,"python-wasi) in [their Python Wasm runtime](https:","",wasmlabs.dev,articles,python-wasm32-wasi,"). In WasmEdge, we could support the zlib host functions through our [plugin system](https:","",wasmedge.org,book,en,"plugin.html). This way, any existing zlib apps can be compiled to Wasm and runs inside WasmEdge.\n- Expected outcome: Create a new [WasmEdge plugin](https:","",wasmedge.org,book,en,"plugin.html) that exports all public functions in `zlib`. Implement SDK (in C","Rust) that uses the C ABI to generate corresponding headers for the above plugin. Generate the unit tests and pass the unit tests. >80% of code coverage for verification."}	The zlib is required for compiling and running many existing C / C++ / Rust apps in Wasm. Most noticeably, it is [needed in the Python port to Wasm](https://github.com/python/cpython/issues/93819). The VMWare Wasm Labs team is using a zlib port from [Singlestore](https://github.com/singlestore-labs/python-wasi) in [their Python Wasm runtime](https://wasmlabs.dev/articles/python-wasm32-wasi/). In WasmEdge, we could support the zlib host functions through our [plugin system](https://wasmedge.org/book/en/plugin.html). This way, any existing zlib apps can be compiled to Wasm and runs inside WasmEdge.\n- Expected outcome: Create a new [WasmEdge plugin](https://wasmedge.org/book/en/plugin.html) that exports all public functions in `zlib`. Implement SDK (in C/Rust) that uses the C ABI to generate corresponding headers for the above plugin. Generate the unit tests and pass the unit tests. >80% of code coverage for verification.	{wasm,rust}	2023	Term 2	https://github.com/WasmEdge/WasmEdge/issues/2244	https://wasmedge.org/	300000	31
191	cab6e242-33d5-427d-a408-9adff1a95271	CNCF - Kubernetes Policy WG: Falco Adapter	{"This project will develop an adapter to run Falco in any Kubernetes cluster and periodically generate or update a [Policy Report custom resource](https:","",github.com,kubernetes-sigs,wg-policy-prototypes,blob,master,policy-report,"README.md). The candidate will learn about Kubernetes controllers and various security topics."}	This project will develop an adapter to run Falco in any Kubernetes cluster and periodically generate or update a [Policy Report custom resource](https://github.com/kubernetes-sigs/wg-policy-prototypes/blob/master/policy-report/README.md). The candidate will learn about Kubernetes controllers and various security topics.	{linux,go,cli,kubernetes}	2021	Term 2	https://github.com/kubernetes-sigs/wg-policy-prototypes/issues/51	https://github.com/kubernetes-sigs/wg-policy-prototypes/issues/51	300000	75
192	e88fb12c-5943-4c16-becc-83b449b15e9f	CNCF - Kubernetes Policy WG: Image Scanner Adapter	{"This project will develop an adapter to run an image scanning tool (like Clair or Trivy) in any Kubernetes cluster and periodically generate or update a [Policy Report custom resource](https:","",github.com,kubernetes-sigs,wg-policy-prototypes,blob,master,policy-report,"README.md). The candidate will learn about Kubernetes controllers, image security and management, and Kubernetes custom resources."}	This project will develop an adapter to run an image scanning tool (like Clair or Trivy) in any Kubernetes cluster and periodically generate or update a [Policy Report custom resource](https://github.com/kubernetes-sigs/wg-policy-prototypes/blob/master/policy-report/README.md). The candidate will learn about Kubernetes controllers, image security and management, and Kubernetes custom resources.	{linux,go,cli,kubernetes}	2021	Term 2	https://github.com/kubernetes-sigs/wg-policy-prototypes/issues/54		300000	75
545	cabb007c-5669-4b16-8778-36d995a71591	CNCF - Kyverno: Sigstore Cosign Updates	{"Kyverno supports image signature and attestation verification using the Sigstore Cosign tooling. Re-implement the Kyverno Sigstore Cosign module to use OCI artifacts and references and remove dependencies to the Cosign CLI packages.\n- Expected outcome: Kyverno can use OCI artifacts to verify container images that are in Cosign format."}	Kyverno supports image signature and attestation verification using the Sigstore Cosign tooling. Re-implement the Kyverno Sigstore Cosign module to use OCI artifacts and references and remove dependencies to the Cosign CLI packages.\n- Expected outcome: Kyverno can use OCI artifacts to verify container images that are in Cosign format.	{kubernetes,kyverno}	2023	Term 2	https://github.com/kyverno/kyverno/issues/7087	https://kyverno.io/	300000	17
543	2314fcc1-f09b-4dab-90fb-d0ef092b6c0e	CNCF - ORAS: Refactor the ORAS documentation structure and write new user guides	{"Refactor the ORAS documentation structure and write new user guides based on the latest version of ORAS. The detailed ORAS documentation structure and content should be refactored according to the proposal in the [upstream issue](https:","",github.com,oras-project,oras-www,issues,"65). \n- Expected Outcome: Deliver a developer-friendly documentation structure for ORAS and write new user guides according to the proposed documentation structure. Publish all content at https:","",oras.land,""}	Refactor the ORAS documentation structure and write new user guides based on the latest version of ORAS. The detailed ORAS documentation structure and content should be refactored according to the proposal in the [upstream issue](https://github.com/oras-project/oras-www/issues/65). \n- Expected Outcome: Deliver a developer-friendly documentation structure for ORAS and write new user guides according to the proposed documentation structure. Publish all content at https://oras.land/	{docker,oci,oras,markdown}	2023	Term 2	https://github.com/oras-project/oras-www/issues/65	https://oras.land	300000	156
321	65cabd48-6a5a-4eb6-9e80-4420c152da6a	CNCF - KubeArmor: Extend kArmor to include KubeArmor configuration	{"KubeArmor is a cloud-native runtime security enforcement system that restricts the behavior (such as process execution, file access, and networking operation) of containers and nodes (VMs) at the system level. kArmor is a KubeArmor CLI tool that connects to the kubearmor-relay service to provide command-line telemetry and observability data. The project aims to extend KubeArmor CLI-tool kArmor to check KubeArmor configurations in the running environment. This feature will provide various information about KubeArmor like the current running mode (audit or enforcement), the enforcer used by KubeArmor (SELinux or AppArmor or BPF-LSM), whether it's running in systemd mode or on k8s, etc."}	KubeArmor is a cloud-native runtime security enforcement system that restricts the behavior (such as process execution, file access, and networking operation) of containers and nodes (VMs) at the system level. kArmor is a KubeArmor CLI tool that connects to the kubearmor-relay service to provide command-line telemetry and observability data. The project aims to extend KubeArmor CLI-tool kArmor to check KubeArmor configurations in the running environment. This feature will provide various information about KubeArmor like the current running mode (audit or enforcement), the enforcer used by KubeArmor (SELinux or AppArmor or BPF-LSM), whether it's running in systemd mode or on k8s, etc.	{go,kubernetes}	2022	Term 2	https://github.com/kubearmor/kubearmor-client/issues/19#issue-1048413733	https://kubearmor.io/	600000	9
552	285d7dd0-fec5-43cd-b0db-c47e63aa5132	LF Networking - L3AF on Windows	{"In this project the student will participate and contribute to OSS projects L3AF and eBPF for Windows led by Walmart, Microsoft and contributed to by WiPro.\n\nL3AF is an API for cross-platform, multi-cloud Enterprise eBPF deployment. With the R1 public release of L3AF R1 one of our next goals is to strengthen our multi-platform capabilities by supporting eBPF for Windows. The intern will work with the L3AF and eBPF for Windows teams to bring the cross-platform vision of the L3AF project into reality for a future release."}	In this project the student will participate and contribute to OSS projects L3AF and eBPF for Windows led by Walmart, Microsoft and contributed to by WiPro.\n\nL3AF is an API for cross-platform, multi-cloud Enterprise eBPF deployment. With the R1 public release of L3AF R1 one of our next goals is to strengthen our multi-platform capabilities by supporting eBPF for Windows. The intern will work with the L3AF and eBPF for Windows teams to bring the cross-platform vision of the L3AF project into reality for a future release.	{ebpf,c,cilium,go}	2023	Term 2	https://github.com/l3af-project	https://l3af.io/	300000	161
326	0afef0c2-6ad0-41f1-8c56-10a52afe87b7	CNCF - Thanos: Implement Unified Endpoint Discovery	{"Thanos Querier microservice is one of the core component of the Thanos system responsible for asking relevent data stores for metrics, labels, metadata, exemplars, targets, alerts and more and merging their results. One of the key challenges to Querier configuration is telling which endpoints it should talk to and what APIs it should expect. For this we proposed the Unified Endoint Discovery idea, which allows consistency and easy to use configuration across all APIs. This project is meant to continue the implementation of this proposal and make sure it works well for all the edge cases using our e2e testing framework."}	Thanos Querier microservice is one of the core component of the Thanos system responsible for asking relevent data stores for metrics, labels, metadata, exemplars, targets, alerts and more and merging their results. One of the key challenges to Querier configuration is telling which endpoints it should talk to and what APIs it should expect. For this we proposed the Unified Endoint Discovery idea, which allows consistency and easy to use configuration across all APIs. This project is meant to continue the implementation of this proposal and make sure it works well for all the edge cases using our e2e testing framework.	{go,dns}	2022	Term 2	https://github.com/thanos-io/thanos/issues/5340	https://thanos.io/	300000	26
576	93854d9d-51ec-4460-b12b-42788492ace8	Hyperledger Collaborative Learning - Using Hyperledger Fabric in dataspaces - Unpaid	{"The data space approach aims at creating a data federation environment without the overhead of “enterprise” data integration.\n\nThe concept of data spaces is gaining more and more importance, as this approach offers an opportunity for different stakeholders (data owners, including public bodies and municipalities, service providers offering AI-as-a-service algorithms , service consumers, regulatory bodies, etc.) to collaborate. There are several domains from eHealth to education where this kind of data","service federation is expected to have an important role in the near future.\n\nBlockchain is a natural candidate to support several aspects and steps of such collaborations. However, current initiatives do not really seem to fully exploit the possibilities of HFL (and its support for designing privacy-preserving applications with channels and Private Data Collection).\n\nThe Eclipse Dataspace Connector, maintained by a coalition of European organizations, implements the International Dataspace Standard and also serves as a focal point of the Gaia-X. It is designed to connect multi-cloud environments and has an extension mechanism towards different cloud storage technologies. However, currently these extensions only support storage over designated cloud-based services, but do not offer consortial blockchain connection.\n\nThe aim of the mentorship would be to develop an extension which support storing data over Hyperledger Fabric as data pane, relying on the Private Data Collection."}	The data space approach aims at creating a data federation environment without the overhead of “enterprise” data integration.\n\nThe concept of data spaces is gaining more and more importance, as this approach offers an opportunity for different stakeholders (data owners, including public bodies and municipalities, service providers offering AI-as-a-service algorithms , service consumers, regulatory bodies, etc.) to collaborate. There are several domains from eHealth to education where this kind of data/service federation is expected to have an important role in the near future.\n\nBlockchain is a natural candidate to support several aspects and steps of such collaborations. However, current initiatives do not really seem to fully exploit the possibilities of HFL (and its support for designing privacy-preserving applications with channels and Private Data Collection).\n\nThe Eclipse Dataspace Connector, maintained by a coalition of European organizations, implements the International Dataspace Standard and also serves as a focal point of the Gaia-X. It is designed to connect multi-cloud environments and has an extension mechanism towards different cloud storage technologies. However, currently these extensions only support storage over designated cloud-based services, but do not offer consortial blockchain connection.\n\nThe aim of the mentorship would be to develop an extension which support storing data over Hyperledger Fabric as data pane, relying on the Private Data Collection.	{java,chaincode}	2023	Term 3	https://github.com/eclipse-edc/Connector/tree/main/extensions	https://wiki.hyperledger.org/pages/viewpage.action?pageId=80778816	0	180
328	bb6cf95e-bbdc-4b50-8446-45507c9e5a6a	CNCF - Meshery: Cloud Native Playground	{"Hosted at play.meshery.io, build a unique learning environment for learning modern application networking through Meshery's support of every service mesh, orchestration of Kubernetes, and integration with many other CNCF projects. Meshery's genesis is that of helping teach people about service mesh technology and enabling to operate this type of cloud native infrastructure confidently. The proposed project is aimed at furthering this mission with interactive API documentation connected to a cloud native playground (a running instance of Meshery)."}	Hosted at play.meshery.io, build a unique learning environment for learning modern application networking through Meshery's support of every service mesh, orchestration of Kubernetes, and integration with many other CNCF projects. Meshery's genesis is that of helping teach people about service mesh technology and enabling to operate this type of cloud native infrastructure confidently. The proposed project is aimed at furthering this mission with interactive API documentation connected to a cloud native playground (a running instance of Meshery).	{go,react}	2022	Term 2	https://github.com/meshery/play/issues/16	https://meshery.io/	300000	186
561	4ba2112d-662c-49f3-ab6d-b8dfe5e5ef63	Test Mentorship Program - Please don't apply	{"Test mentorship program"}	Test mentorship program	{algorithm}	2023	Term 2	www.example.com		0	179
555	3dc87de1-e879-4aa2-b1e0-71dc5e898d7e	Open Mainframe- App Store UI	{"Using the existing npm app registry, create an app store UI in the App Framework's virtual desktop to download and install apps. Via the new enhancement to zwe which adds new 'zwe components' commands and with configuration manager, we can define an extension registry and pull extensions. The app store UI will invoke zwe which has a pluggable handler API which then talks to a package manager. The package manager receives and downloads requested extension and dependencies. While the package manager can be of any type, current support and a good starting place is npm. This would be a completely new app, from top to bottom, that will exist in the personalization & system widgets area in the Desktop and is an excellent opportunity for both a talented engineer and UX designer to make a huge imprint on the future of the Zowe desktop. The ideal candidate would have experience in Typescript","Javascript, Angular or React, Node.js, and HTML & CSS but having at least working knowledge in one or two areas is alright."}	Using the existing npm app registry, create an app store UI in the App Framework's virtual desktop to download and install apps. Via the new enhancement to zwe which adds new 'zwe components' commands and with configuration manager, we can define an extension registry and pull extensions. The app store UI will invoke zwe which has a pluggable handler API which then talks to a package manager. The package manager receives and downloads requested extension and dependencies. While the package manager can be of any type, current support and a good starting place is npm. This would be a completely new app, from top to bottom, that will exist in the personalization & system widgets area in the Desktop and is an excellent opportunity for both a talented engineer and UX designer to make a huge imprint on the future of the Zowe desktop. The ideal candidate would have experience in Typescript/Javascript, Angular or React, Node.js, and HTML & CSS but having at least working knowledge in one or two areas is alright.	{javascript,html,css,react,angular}	2023	Term 2	https://github.com/zowe/zlux		600000	38
568	6c79696a-4c4f-470e-a004-e17d1e6a091a	Hyperledger Collaborative Learning - Fabric ERC tokens for assets data trading - Unpaid	{"The current existing implementation of Hyperledger Fabric with Raft consensus has many shortcomings such as transaction censorship and no mechanism to safeguard the network if a malicious leader gets elected. The network also has limitations on scalability which restrict the type of use cases that can be built such as real-time applications that use IoT devices. This impedes blockchain adoption and limits the type of applications and use cases that can be built on Fabric. n nThe goal of this project is to build BiniBFT, a Byzantine Fault Tolerance consensus library for Hyperledger Fabric which provides end-to-end security, high throughput with low latency and high scalability so that Fabric can be applicable for distributed and decentralized day-to-day applications. n nLearning Objectives n- Building and Integrating a BFT library with Hyperledger Fabric. n- Understanding how BFT consensus protocols work and optimizing algorithms for better performance. n- Developing an understanding of how DLT networks like Hyperledger Fabric work. n- Exploring the different ways in which BFT protocols can improve the performance and security of Fabric networks."}	The current existing implementation of Hyperledger Fabric with Raft consensus has many shortcomings such as transaction censorship and no mechanism to safeguard the network if a malicious leader gets elected. The network also has limitations on scalability which restrict the type of use cases that can be built such as real-time applications that use IoT devices. This impedes blockchain adoption and limits the type of applications and use cases that can be built on Fabric. n nThe goal of this project is to build BiniBFT, a Byzantine Fault Tolerance consensus library for Hyperledger Fabric which provides end-to-end security, high throughput with low latency and high scalability so that Fabric can be applicable for distributed and decentralized day-to-day applications. n nLearning Objectives n- Building and Integrating a BFT library with Hyperledger Fabric. n- Understanding how BFT consensus protocols work and optimizing algorithms for better performance. n- Developing an understanding of how DLT networks like Hyperledger Fabric work. n- Exploring the different ways in which BFT protocols can improve the performance and security of Fabric networks.	{}	2023	Term 3	https://github.com/hyperledger/fabric	https://wiki.hyperledger.org/display/CLP/Fabric+ERC+tokens+for+assets+data+trading	0	180
577	f8c14eaa-8e20-4398-83ab-b8ac55d20e12	Yocto Project	{"The Yocto Project (YP) is an open source collaboration project that helps developers create custom Linux-based systems regardless of the hardware architecture."}	The Yocto Project (YP) is an open source collaboration project that helps developers create custom Linux-based systems regardless of the hardware architecture.	{}	2022	Term 3	https://git.yoctoproject.org/		1501000	181
569	8a4e6a1e-a5b6-4f74-93e0-94c7177ba375	Hyperledger Collaborative Learning - Extend DRman & GitVCR for Gitlab - Unpaid	{"The project DRman aims to develop a standalone utility for Verifiable Credential Registries management. It includes functionalities like credential creation, verification, modification, and revocation, enabling easy management of the registry. The project has three significant aspects: DID Registry creation, member onboarding, and DID management. The project seeks to address migration issues by migrating the project from GitHub to GitLab to make it infrastructure-agnostic."}	The project DRman aims to develop a standalone utility for Verifiable Credential Registries management. It includes functionalities like credential creation, verification, modification, and revocation, enabling easy management of the registry. The project has three significant aspects: DID Registry creation, member onboarding, and DID management. The project seeks to address migration issues by migrating the project from GitHub to GitLab to make it infrastructure-agnostic.	{dlt}	2023	Term 3	https://github.com/DIDman/DRman	https://wiki.hyperledger.org/display/CLP/Design+and+Spec+DIDMan+based+openwallet	0	180
566	f9273aa3-f70a-4e14-aedb-e37d4c63c4ab	Hyperledger Collaborative Learning - Privacy-preserving data sharing for SLAs on Fabric - Unpaid	{"Current project targets privacy preservation developing modular tools for data sharing while envisioning the deployment of the necessary future blockchain and smart contract processes in Service Level Agreements (SLAs) evaluation.\n\nService Level Agreements (SLAs) define the way that service and infrastructure providers are making available products to their clientele, for example, the dedicated agreements for Public Cloud Infrastructure As A Service. An SLA is an accurately defined contract involving the specified service level objectives (SLOs) and contractual guarantees that are promised to be delivered to clients by the providers of the service or infrastructure. Particularly, monitoring of SLAs determines whether a provider meets an SLA's contractual terms or otherwise breaches of the agreement unfold. SLA breaches lead to definite violations of the terms, namely SLA Violations, as described in the contract. Ultimately, contractual definitions and parameter values computations vary across different provider setups, e.g. Telecom, 5G, Edge, and Cloud.\n\nThe main objective of the project is to develop privacy preservation through trusted execution environments, zero-knowledge proofs, or ring signatures for the corresponding transactional operations of SLAs assessment under a modular data sharing scheme on Hyperledger Fabric. The applicable deployment and engagement with such architectural elements of privacy leads the current project to preserve and secure data protection."}	Current project targets privacy preservation developing modular tools for data sharing while envisioning the deployment of the necessary future blockchain and smart contract processes in Service Level Agreements (SLAs) evaluation.\n\nService Level Agreements (SLAs) define the way that service and infrastructure providers are making available products to their clientele, for example, the dedicated agreements for Public Cloud Infrastructure As A Service. An SLA is an accurately defined contract involving the specified service level objectives (SLOs) and contractual guarantees that are promised to be delivered to clients by the providers of the service or infrastructure. Particularly, monitoring of SLAs determines whether a provider meets an SLA's contractual terms or otherwise breaches of the agreement unfold. SLA breaches lead to definite violations of the terms, namely SLA Violations, as described in the contract. Ultimately, contractual definitions and parameter values computations vary across different provider setups, e.g. Telecom, 5G, Edge, and Cloud.\n\nThe main objective of the project is to develop privacy preservation through trusted execution environments, zero-knowledge proofs, or ring signatures for the corresponding transactional operations of SLAs assessment under a modular data sharing scheme on Hyperledger Fabric. The applicable deployment and engagement with such architectural elements of privacy leads the current project to preserve and secure data protection.	{golang,typescript,react}	2023	Term 3	https://github.com/hyperledger/fabric	https://wiki.hyperledger.org/display/CLP/Privacy-preserving+data+sharing+for+SLAs+on+Fabric	0	180
571	38f04760-c6e5-4ea5-9336-d7dc685c0d59	Hyperledger Collaborative Learning - Zero-Knowledge Workflows on Fabric - Unpaid	{"Orchestrating and tracking the steps and cross-party messages of business collaborations is a key blockchain","distributed ledger technology use case. Several tools have emerged to support blockchain-orchestrated BPM (business process management); an important subset of these takes models of the cooperation, usually captured in the Business Process and Notation (BPMN) language and generate a process manager smart contract from the model.\n\nMost of the existing tools target Solidity, and all of them store the business process state in the clear, in the smart contract. This can be problematic even for Hyperledger Fabric – if the process state is not to be seen by the whole consortium, we must resort to the confidentiality toolbox of Fabric (multiple channels, private data collections, etc.).\n\nWe argue that this is not absolutely necessary. Our prototype implementation already supports Hyperledger Fabric to some extent (originally, we focused primarily on Solidity","EVM). Specifically, we have an implementation of the crucial element, the commitment manager and proof checker chaincode. However, proper support for Fabric needs a number of further improvements – as contributions to our open-source solution, and this is the topic of this mentorship."}	Orchestrating and tracking the steps and cross-party messages of business collaborations is a key blockchain/distributed ledger technology use case. Several tools have emerged to support blockchain-orchestrated BPM (business process management); an important subset of these takes models of the cooperation, usually captured in the Business Process and Notation (BPMN) language and generate a process manager smart contract from the model.\n\nMost of the existing tools target Solidity, and all of them store the business process state in the clear, in the smart contract. This can be problematic even for Hyperledger Fabric – if the process state is not to be seen by the whole consortium, we must resort to the confidentiality toolbox of Fabric (multiple channels, private data collections, etc.).\n\nWe argue that this is not absolutely necessary. Our prototype implementation already supports Hyperledger Fabric to some extent (originally, we focused primarily on Solidity/EVM). Specifically, we have an implementation of the crucial element, the commitment manager and proof checker chaincode. However, proper support for Fabric needs a number of further improvements – as contributions to our open-source solution, and this is the topic of this mentorship.	{linux,kotlin}	2023	Term 3	https://github.com/ftsrg/zkWF	https://wiki.hyperledger.org/display/CLP/Zero-Knowledge+Workflows+on+Fabric	0	180
575	79ec7d7d-506b-43c7-a7fe-a098391be542	Hyperledger Collaborative Learning - Semantic tools for interoperable climate accounting - Unpaid	{"Standards Working Group (Standards WG) of Hyperledger's Climate Action and Accounting Special Interest Group (CA2-SIG) has been working on developing an Anthropogenic Impact Accounting Ontology (AIAO). Climate accounting is currently the primary use case for AIAO. The ontology aims to enable interoperability between the plethora of impact accounting standards and methodologies in the market, allowing further development of systems such as a global climate impact accounting ledger.\n\nThe WG has formulated an ontology comprising a current total of 46 classes and 11 primary axioms with several more axioms on the secondary and tertiary level. The core concepts are explicated on the Standards WG's wiki and encoded in an OWL file. During the 2022","2023 southern hemisphere's summer break the Nova Institute sponsored and hosted a mentorship programme that recruited three second-year ICT university students from South Africa to refine the OWL file, develop a Turtle parser and set up an MVP triple store for the ontology.\n\nThe next steps towards operationalization of the ontology are aimed at developing the tools for interoperability between the AIA ontology and existing vocabularies in the field of impact accounting, starting with those from the climate accounting space. This will involve mapping the existing vocabularies and standards to the ontology, improving on the examples produced by the students; annotating existing climate impact data from selected case studies."}	Standards Working Group (Standards WG) of Hyperledger's Climate Action and Accounting Special Interest Group (CA2-SIG) has been working on developing an Anthropogenic Impact Accounting Ontology (AIAO). Climate accounting is currently the primary use case for AIAO. The ontology aims to enable interoperability between the plethora of impact accounting standards and methodologies in the market, allowing further development of systems such as a global climate impact accounting ledger.\n\nThe WG has formulated an ontology comprising a current total of 46 classes and 11 primary axioms with several more axioms on the secondary and tertiary level. The core concepts are explicated on the Standards WG's wiki and encoded in an OWL file. During the 2022/2023 southern hemisphere's summer break the Nova Institute sponsored and hosted a mentorship programme that recruited three second-year ICT university students from South Africa to refine the OWL file, develop a Turtle parser and set up an MVP triple store for the ontology.\n\nThe next steps towards operationalization of the ontology are aimed at developing the tools for interoperability between the AIA ontology and existing vocabularies in the field of impact accounting, starting with those from the climate accounting space. This will involve mapping the existing vocabularies and standards to the ontology, improving on the examples produced by the students; annotating existing climate impact data from selected case studies.	{}	2023	Term 3	https://github.com/hyperledger	https://wiki.hyperledger.org/display/CLP/Semantic+tools+for+interoperable+climate+accounting	0	180
584	ec286a9e-e48d-4c83-a991-9c79a4ec213a	CNCF - Konveyor: Move2Kube: WASM Transformers	{"Move2Kube is a command-line tool for automating creation of Infrastructure as code (IaC) artifacts. It has inbuilt support for creating IaC artifacts for replatforming to Kubernetes","OpenShift. Move2Kube has a very plugin friendly architecture, users can write custom logic in the form of \\"Transformers\\" that Move2Kube can integrate seamlessly into its transformation pipeline. So far we have support for both Starlark and container image based transformers. We would like to support writing transformers as WASM modules that Move2Kube can run. WASM provides extensive sandboxing for security, it allows writing transformers in different language stacks like Rust, C","C++, etc. other than Golang, and WASM is just as lightweight and fast as Starlark.\n- Expected Outcome:\n  - Implement a feature in Move2Kube CLI to allow running WASM modules as custom transformers."}	Move2Kube is a command-line tool for automating creation of Infrastructure as code (IaC) artifacts. It has inbuilt support for creating IaC artifacts for replatforming to Kubernetes/OpenShift. Move2Kube has a very plugin friendly architecture, users can write custom logic in the form of "Transformers" that Move2Kube can integrate seamlessly into its transformation pipeline. So far we have support for both Starlark and container image based transformers. We would like to support writing transformers as WASM modules that Move2Kube can run. WASM provides extensive sandboxing for security, it allows writing transformers in different language stacks like Rust, C/C++, etc. other than Golang, and WASM is just as lightweight and fast as Starlark.\n- Expected Outcome:\n  - Implement a feature in Move2Kube CLI to allow running WASM modules as custom transformers.	{go}	2023	Term 3	https://github.com/konveyor/move2kube/issues/1061	https://www.konveyor.io/	0	148
330	a91eab39-88b3-4a42-82bf-2bd963772acb	CNCF - KubeArmor: Supporting KubeArmor on ARM platforms	{"KubeArmor has garnered interests from edge computing platforms (such as LF Edge OpenHorizon) that leverages k8s control plane for workload orchestration. The primary requirement is to support ARM platforms that are prevalent on the edge devices (especially Raspberry PI). KubeArmor leverages eBPF for observability and Linux Security Modules (such as AppArmor) for policy enforcement. One of the challenges is to check if the eBPF primitives such as observing kprobe, kretprobe, tracepoints that are typically available on the x86 platform are also available on the ARM platform and check if the parameter list fulfills the requirement. Post this analysis, the KubeArmor code might have to be changed to accommodate any differences in the eBPF behavior."}	KubeArmor has garnered interests from edge computing platforms (such as LF Edge OpenHorizon) that leverages k8s control plane for workload orchestration. The primary requirement is to support ARM platforms that are prevalent on the edge devices (especially Raspberry PI). KubeArmor leverages eBPF for observability and Linux Security Modules (such as AppArmor) for policy enforcement. One of the challenges is to check if the eBPF primitives such as observing kprobe, kretprobe, tracepoints that are typically available on the x86 platform are also available on the ARM platform and check if the parameter list fulfills the requirement. Post this analysis, the KubeArmor code might have to be changed to accommodate any differences in the eBPF behavior.	{go,ebpf,kubernetes}	2022	Term 2	https://github.com/kubearmor/KubeArmor/issues/614	https://kubearmor.io/	0	9
583	919e8b57-8fc1-4f61-8bc8-3c8b06dd5e7a	CNCF - Kyverno: Kyverno Kuttl Enhancements	{"Add new features to the kuttl application used by the Kyverno project to aid in its end-to-end testing process.\n- Expected Outcome: Kyverno's fork of kuttl has these enhancements allowing more and better test cases to be written."}	Add new features to the kuttl application used by the Kyverno project to aid in its end-to-end testing process.\n- Expected Outcome: Kyverno's fork of kuttl has these enhancements allowing more and better test cases to be written.	{go}	2023	Term 3	https://github.com/kyverno/kuttl/issues/18	https://kyverno.io/	0	17
590	eba8fbf7-848b-4d69-8b0e-b6852acc7755	CNCF - KubeArmor: Extend Support Matrix and Document Usecases	{"The project aims to extend KubeArmor support and document how KubeArmor is relevant for securing Kubernetes on Edge in addition to generic Kubernetes based Application deployments\n- Expected Outcome:  \n  - Try KubeArmor on Different Kubernetes Platforms\n      - Microshift, k0s\n      - Make fixes to deployments to make them work if needed\n  - Document Usecases on these platforms\n  - Document Usecases at a broader level for EDGE and Container Security\n  - Produce Blogs about extended support and additional usecases"}	The project aims to extend KubeArmor support and document how KubeArmor is relevant for securing Kubernetes on Edge in addition to generic Kubernetes based Application deployments\n- Expected Outcome:  \n  - Try KubeArmor on Different Kubernetes Platforms\n      - Microshift, k0s\n      - Make fixes to deployments to make them work if needed\n  - Document Usecases on these platforms\n  - Document Usecases at a broader level for EDGE and Container Security\n  - Produce Blogs about extended support and additional usecases	{kubernetes,kubearmor}	2023	Term 3	https://github.com/kubearmor/KubeArmor/issues/1334	https://kubearmor.io/	0	9
592	a7f754b4-5c8c-48a3-8f5f-b3ff6307b0f4	CNCF - Kyverno: Pod Security Admission Integrations II	{"Integrate Kubernetes Pod Security with Kyverno - Part Ⅲ\n- Expected Outcome: Kyverno's podSecurity \\"subrule\\" has the ability to exclude based on specific field paths and not just the control level."}	Integrate Kubernetes Pod Security with Kyverno - Part Ⅲ\n- Expected Outcome: Kyverno's podSecurity "subrule" has the ability to exclude based on specific field paths and not just the control level.	{kubernetes,go}	2023	Term 3	https://github.com/kyverno/kyverno/issues/6144	https://kyverno.io/	0	17
586	9faff011-1027-49c0-aa37-8d5be7208d6f	CNCF - Meshery: WASM-based OPA policy evaluation with Rego	{"Meshery's highly dynamic infrastructure configuration capabilities require real-time evaluation of complex policies. Policies of various types and with a high number of parameters need to be evaluted client-side. With policies expressed in Rego, the goal of this project is to incorporate use of the https:","",github.com,open-policy-agent,"golang-opa-wasm project into Meshery UI.\n- Expected outcome: a powerful real-time multi-user collaboration experience."}	Meshery's highly dynamic infrastructure configuration capabilities require real-time evaluation of complex policies. Policies of various types and with a high number of parameters need to be evaluted client-side. With policies expressed in Rego, the goal of this project is to incorporate use of the https://github.com/open-policy-agent/golang-opa-wasm project into Meshery UI.\n- Expected outcome: a powerful real-time multi-user collaboration experience.	{golang,wasm}	2023	Term 3	https://github.com/meshery/meshery/issues/7019	https://meshery.io/	0	186
585	2fa8d7b8-01fa-4375-b3a8-1b626348fedd	CNCF - Volcano: Fix bugs for jobflow to enhance its stability	{"As a built-in orchestration engine for Volcano, jobflow acts as an improtant role for users and it's still new-born. Many issues related to its stability are reported recently. Please help make full test for job-flow on the classical scenarios and reslove bugs reported in issues.\n- Expected Outcome: Make full test for jobflow and output the test report, fix bugs reported in recent issues."}	As a built-in orchestration engine for Volcano, jobflow acts as an improtant role for users and it's still new-born. Many issues related to its stability are reported recently. Please help make full test for job-flow on the classical scenarios and reslove bugs reported in issues.\n- Expected Outcome: Make full test for jobflow and output the test report, fix bugs reported in recent issues.	{volcano,golang,ut,jobflow}	2023	Term 3	https://github.com/volcano-sh/volcano/issues/3014	https://volcano.sh/	0	34
596	37cf2714-668d-4014-ac44-953f70f9dc8e	CNCF - Jaeger: Build official support in Jaeger for Elasticsearch 8	{"Jaeger has always supported Elasticsearch (ES) as an official backend. Unfortunately, the Go driver we are using, `olivere","elastic`, does not support ESv8 and not planning to release any new versions. Despite the licensing changes, Elasticsearch remains a popular choice and v8 support question often comes up. In this project we want to add official support for ESv8. However, we also want to take this opportunity to do a better alignment with the OpenTelemetry Collector, which already has an [ESv8 exporter](https:","",github.com,open-telemetry,opentelemetry-collector-contrib,tree,main,exporter,"elasticsearchexporter), perhaps we can use it directly and minimize the amount of code.\n- Expected Outcomes:\n  - Use OTEL ESv8 exporter from Jaeger, if possible, otherwise build internal implementation\n  - Stretch goal: use ES data as the source for Jaeger SPM views"}	Jaeger has always supported Elasticsearch (ES) as an official backend. Unfortunately, the Go driver we are using, `olivere/elastic`, does not support ESv8 and not planning to release any new versions. Despite the licensing changes, Elasticsearch remains a popular choice and v8 support question often comes up. In this project we want to add official support for ESv8. However, we also want to take this opportunity to do a better alignment with the OpenTelemetry Collector, which already has an [ESv8 exporter](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/exporter/elasticsearchexporter), perhaps we can use it directly and minimize the amount of code.\n- Expected Outcomes:\n  - Use OTEL ESv8 exporter from Jaeger, if possible, otherwise build internal implementation\n  - Stretch goal: use ES data as the source for Jaeger SPM views	{go,elasticsearch}	2023	Term 3	https://github.com/jaegertracing/jaeger/issues/4600	https://www.jaegertracing.io/	0	178
593	271852c6-3348-4ec7-bf09-035913b1c86e	CNCF - Kubescape: Build an admission controller for Kubescape	{[Kubescape](http:,"",kubescape.io,") is a utility that can scan a Kubernetes cluster and report on its security posture. It can also scan individual workloads (e.g. YAML files) before they are applied. By creating a Kubescape admission controller, we will be able to combine the two, denying workloads into a cluster where it would reduce the security posture.\n- Expected Outcome: The Kubescape application will be extended and packaged to operate as an admission controller inside a cluster. The controller will be well documented, safe to install, and instrumented with logging and telemetry data to be able to diagnose problems."}	[Kubescape](http://kubescape.io/) is a utility that can scan a Kubernetes cluster and report on its security posture. It can also scan individual workloads (e.g. YAML files) before they are applied. By creating a Kubescape admission controller, we will be able to combine the two, denying workloads into a cluster where it would reduce the security posture.\n- Expected Outcome: The Kubescape application will be extended and packaged to operate as an admission controller inside a cluster. The controller will be well documented, safe to install, and instrumented with logging and telemetry data to be able to diagnose problems.	{go}	2023	Term 3	https://github.com/kubescape/kubescape/issues/1301	https://github.com/kubescape	0	150
606	10d70edd-60ec-409b-8801-0fb752501b12	CNCF - Vitess: Continue the migration to React and enhance existing frontend UI	{"Vitess uses arewefastyet to automatically benchmark its codebase and ensure no performance regression is introduced. The mentee will have the responsibility of continuing the UI that was previously created using React","Vite.\n- Expected Outcome: The expected outcome is to continue working on the Frontend UI that was developed during the 2023-summer term, that includes adding an admin UI, adding a feature to ensure the consistency of the results, improving the overall UX of the website, and add new pages to improve the scope of arewefastyet."}	Vitess uses arewefastyet to automatically benchmark its codebase and ensure no performance regression is introduced. The mentee will have the responsibility of continuing the UI that was previously created using React/Vite.\n- Expected Outcome: The expected outcome is to continue working on the Frontend UI that was developed during the 2023-summer term, that includes adding an admin UI, adding a feature to ensure the consistency of the results, improving the overall UX of the website, and add new pages to improve the scope of arewefastyet.	{golang,react,docker,vite}	2023	Term 3	https://github.com/vitessio/arewefastyet/issues/415	https://vitess.io/	0	107
594	757e329b-0083-4720-bd96-8bbbd6a7aeb3	CNCF - Volcano: Add user guidance for jobflow	{"Since jobflow is an important built-in orchestration engine for Volcano, it is still lack of user guidance. Please add more docs to demonstrate its installation, usage, tips and so on. \n- Expected Outcome: Add docs into volcano-sh",volcano,docs,"user-guide and describe the usage of jobflow."}	Since jobflow is an important built-in orchestration engine for Volcano, it is still lack of user guidance. Please add more docs to demonstrate its installation, usage, tips and so on. \n- Expected Outcome: Add docs into volcano-sh/volcano/docs/user-guide and describe the usage of jobflow.	{volcano,golang}	2023	Term 3	https://github.com/volcano-sh/volcano/issues/3013	https://volcano.sh/	0	34
337	fca1338f-5be0-41e6-a499-b44e2e722096	CNCF - WasmEdge: Provide a wasm-compatible Rust TLS implementation	{"The WasmEdge networking socket API provides support for TCP and HTTP connections. But many web services today require HTTPS connections. That means we need to support TLS in WasmEdge. The Rustls crate is the most popular TLS implementation in Rust. However, Rustls is based on the Ring library, which cannot be compiled into WebAssembly. In WasmEdge, we now support the wasi-crypto spec. That allows us to compile the rust-crypto library into WebAssembly and run on WasmEdge. The goal of this project is to create a Rust TLS implementation based on rust-crypto."}	The WasmEdge networking socket API provides support for TCP and HTTP connections. But many web services today require HTTPS connections. That means we need to support TLS in WasmEdge. The Rustls crate is the most popular TLS implementation in Rust. However, Rustls is based on the Ring library, which cannot be compiled into WebAssembly. In WasmEdge, we now support the wasi-crypto spec. That allows us to compile the rust-crypto library into WebAssembly and run on WasmEdge. The goal of this project is to create a Rust TLS implementation based on rust-crypto.	{wasm,rust,cryptocurrency}	2022	Term 2	https://github.com/WasmEdge/WasmEdge/issues/1430	https://wasmedge.org/	300000	31
604	53d7c0da-f066-486a-a9cb-c2da55e42375	CNCF - KubeArmor: Implement DNS visibility with KubeArmor II	{"The project aims to provide better visibility into the domains accessed from pods, with a focus on identifying and containing attacks that use techniques like Domain Generation Algorithms (DGA) to connect to remote command and control (C&C) servers. By gathering information on which domains are being accessed and applying network rules to allow only specific domains, the project aims to empower security operations (secops) teams to better prevent and respond to such attacks.\n- Expected Outcome:  \n  - KubeArmor to emit telemetry events for any DNS lookups from any pods.\n  - Ability to see egress DNS lookups done from any pods using karmor summary.\n  - Documentation"}	The project aims to provide better visibility into the domains accessed from pods, with a focus on identifying and containing attacks that use techniques like Domain Generation Algorithms (DGA) to connect to remote command and control (C&C) servers. By gathering information on which domains are being accessed and applying network rules to allow only specific domains, the project aims to empower security operations (secops) teams to better prevent and respond to such attacks.\n- Expected Outcome:  \n  - KubeArmor to emit telemetry events for any DNS lookups from any pods.\n  - Ability to see egress DNS lookups done from any pods using karmor summary.\n  - Documentation	{go,kubernetes,kubearmor}	2023	Term 3	https://github.com/kubearmor/KubeArmor/issues/1219	https://kubearmor.io/	0	9
605	3d6d3534-24e9-4261-9b91-7de3d78554f7	CNCF - Thanos: Release the distributed query engine in Thanos	{"The Thanos engine is capable of executing queries in a distributed manner, by pushing down aggregations to other querier nodes. This querying mode is not yet integrated well in the UI and is not exposed to users.\n  The goal of this project is to add the needed integrations to the Thanos UI and officially release the feature to end users.\n- Expected Outcome:\n  The expected outcome of the project is to have a fully integrated distributed querying capability through the Thanos UI."}	The Thanos engine is capable of executing queries in a distributed manner, by pushing down aggregations to other querier nodes. This querying mode is not yet integrated well in the UI and is not exposed to users.\n  The goal of this project is to add the needed integrations to the Thanos UI and officially release the feature to end users.\n- Expected Outcome:\n  The expected outcome of the project is to have a fully integrated distributed querying capability through the Thanos UI.	{golang,typescript,git,github}	2023	Term 3	https://github.com/thanos-io/thanos/issues/6124	https://thanos.io/	0	26
622	ffbb6bb3-b9d1-4e96-9f72-4a0566f6b0c3	CNCF - Volcano: Build a new elastic resource quota mechinism in Volcano	{"Design and implement a new hierarchical elastic resource quota mechinism to support resource lending and borrowing to improve the cluster utilization for multi-tenants environment. \n- Expected Outcome:  \n  - design doc and how to use\n  - implement a elastic resource quota mechinism to support min, max and resource sharing\n  - implement the hierarchical resource quota\n  - produce Blogs about these new cases."}	Design and implement a new hierarchical elastic resource quota mechinism to support resource lending and borrowing to improve the cluster utilization for multi-tenants environment. \n- Expected Outcome:  \n  - design doc and how to use\n  - implement a elastic resource quota mechinism to support min, max and resource sharing\n  - implement the hierarchical resource quota\n  - produce Blogs about these new cases.	{go,volcano,kubernetes}	2023	Term 3	https://github.com/volcano-sh/volcano/issues/3018	https://volcano.sh/	0	34
610	54940f04-54b8-41ee-94bf-153f977b31e7	CNCF - Karmada: Add Karmada API documentation on the website	{"Add the Karmada API documentation on the [website](https:","",github.com,karmada-io,"website),and complete the script for automatic document generation.\n- Expected Outcome:\n  - Technical Documentation: design description and analysis\n  - Script Complete: automatic document generation\n  - Maintaining Documentation: add maintaining document on the website"}	Add the Karmada API documentation on the [website](https://github.com/karmada-io/website),and complete the script for automatic document generation.\n- Expected Outcome:\n  - Technical Documentation: design description and analysis\n  - Script Complete: automatic document generation\n  - Maintaining Documentation: add maintaining document on the website	{go}	2023	Term 3	https://github.com/karmada-io/karmada/issues/3843	https://karmada.io/	0	124
265	849ac80c-2086-4df6-b2f8-b57b0a78b51d	CNCF - Kubevela: Extend monitoring through VelaQL	{"KubeVela is a modern application delivery platform based on Kubernetes. It is currently a CNCF Sandbox project. In KubeVela, a CUE-based query mechanism is developed to satisfy the demands of advanced queries behind Kubernetes resources, which is called VelaQL. This project aims to make extensions to this mechanism and support monitoring Kubernetes resources through VelaQL. For example, monitoring the logs of pods in KubeVela Application behind Grafana."}	KubeVela is a modern application delivery platform based on Kubernetes. It is currently a CNCF Sandbox project. In KubeVela, a CUE-based query mechanism is developed to satisfy the demands of advanced queries behind Kubernetes resources, which is called VelaQL. This project aims to make extensions to this mechanism and support monitoring Kubernetes resources through VelaQL. For example, monitoring the logs of pods in KubeVela Application behind Grafana.	{go,kubernetes}	2022	Term 1	https://github.com/oam-dev/kubevela/issues/3178	https://kubevela.io/	300000	117
609	7ffb0f63-e1e4-477b-ab0c-a69cb681f112	CNCF - Kyverno: Policy Exceptions 2.0	{"Enhancements for Kyverno Policy Exceptions including support for images and conditions.\n- Expected Outcome: A future version of Kyverno has enhanced support for its Policy Exception resource."}	Enhancements for Kyverno Policy Exceptions including support for images and conditions.\n- Expected Outcome: A future version of Kyverno has enhanced support for its Policy Exception resource.	{kubernetes,go}	2023	Term 3	https://github.com/kyverno/kyverno/issues/7907	https://kyverno.io/	0	17
616	41a8bf79-a903-4c03-afb7-256fd373c0b0	CNCF - WasmEdge: Create a ffmpeg plugin	{"The mediapipe-rs project provides a Rust SDK to support mediapipe AI models. The SDK provides utility functions to pre-process application data (such as images, audio and video) into TFLite "," PyTorch formats, and convert the inference results back into application data. In order to accomplish this, the [mediapipe-rs](https:","",github.com,WasmEdge,"mediapipe-rs) project has made extensive use of the [ffmpeg](https:","",www.ffmpeg.org,") library. It [compiles ffmpeg to Wasm](https:","",github.com,WasmEdge,mediapipe-rs,blob,main,src,"build.rs) and then builds it together with the application binary.\n\n  However, the issue with this approach is that those Wasm-compiled ffmpeg functions are slow. We believe a better approach is to create a ffmpeg plugin for WasmEdge, and allow Wasm applications to call native ffmpeg functions as host functions.\n- Expected Outcome:\n  - The deliverables will be\n    - A WasmEdge plugin for ffmpeg that is similar to the [WasmEdge OpenCV-mini plugin](https:","",github.com,WasmEdge,WasmEdge,tree,master,plugins,"wasmedge_opencvmini). That is to re-export ffmpeg functions in C style as plugin functions as covered in the [plugin developer guide](https:","",wasmedge.org,docs,category,"wasmedge-plugin-system).\n    - A Rust SDK for Wasm programs to access ffmpeg functions in the plugin. Similar to the [WasmEdge OpenCV-mini SDK](https:","",github.com,second-state,"opencvmini)\n    - Refactor the mediapipe-rs project to use the ffmpeg plugin"}	The mediapipe-rs project provides a Rust SDK to support mediapipe AI models. The SDK provides utility functions to pre-process application data (such as images, audio and video) into TFLite / PyTorch formats, and convert the inference results back into application data. In order to accomplish this, the [mediapipe-rs](https://github.com/WasmEdge/mediapipe-rs) project has made extensive use of the [ffmpeg](https://www.ffmpeg.org/) library. It [compiles ffmpeg to Wasm](https://github.com/WasmEdge/mediapipe-rs/blob/main/src/build.rs) and then builds it together with the application binary.\n\n  However, the issue with this approach is that those Wasm-compiled ffmpeg functions are slow. We believe a better approach is to create a ffmpeg plugin for WasmEdge, and allow Wasm applications to call native ffmpeg functions as host functions.\n- Expected Outcome:\n  - The deliverables will be\n    - A WasmEdge plugin for ffmpeg that is similar to the [WasmEdge OpenCV-mini plugin](https://github.com/WasmEdge/WasmEdge/tree/master/plugins/wasmedge_opencvmini). That is to re-export ffmpeg functions in C style as plugin functions as covered in the [plugin developer guide](https://wasmedge.org/docs/category/wasmedge-plugin-system).\n    - A Rust SDK for Wasm programs to access ffmpeg functions in the plugin. Similar to the [WasmEdge OpenCV-mini SDK](https://github.com/second-state/opencvmini)\n    - Refactor the mediapipe-rs project to use the ffmpeg plugin	{rust,webassembly,ffmpeg}	2023	Term 3	https://github.com/WasmEdge/WasmEdge/issues/2689	https://wasmedge.org/	0	31
615	63adcf97-cd3e-4c96-a5fc-6076b5a5d511	CNCF - WasmEdge: Create a Rust crate for YOLO model	{"With WASI-NN plugins, WasmEdge is well-suited for running AI applications. However, AI applications are more than just the model. The application must pre-process data (such as images, audio and video) into TFLite "," PyTorch formats, and convert the inference results back into application data in post-processing. Here are some examples:\n\n  * The [mediapipe-rs](https:","",github.com,WasmEdge,"mediapipe-rs) project provides a Rust SDK to build applications for the mediapipe AI models.\n  * The [llama2.c](https:","",github.com,karpathy,"llama2.c) application is [compiled to Wasm and runs in WasmEdge](https:","",medium.com,@michaelyuan_88928,"running-llama2-c-in-wasmedge-15291795c470) to generate text using the [llama2](https:","",ai.meta.com,llama,") models.\n\n  In this project, we would like to build a Rust SDK to support applications on the [YOLO models](https:","",pjreddie.com,darknet,yolo,").\n- Expected Outcome:\n  - A Rust SDK that implements the pre-processing and post-processing functions required for the YOLO models. Those functions are implemented in OpenCV and Python in the official YOLO release.\n  - Both image and video inputs should be supported.\n  - Examples and documentation should be provided.\n- Recommended Skills:\n  - OpenCV\n  - Rust\n  - Tensorflow "," Pytorch\n  - WebAssembly"}	With WASI-NN plugins, WasmEdge is well-suited for running AI applications. However, AI applications are more than just the model. The application must pre-process data (such as images, audio and video) into TFLite / PyTorch formats, and convert the inference results back into application data in post-processing. Here are some examples:\n\n  * The [mediapipe-rs](https://github.com/WasmEdge/mediapipe-rs) project provides a Rust SDK to build applications for the mediapipe AI models.\n  * The [llama2.c](https://github.com/karpathy/llama2.c) application is [compiled to Wasm and runs in WasmEdge](https://medium.com/@michaelyuan_88928/running-llama2-c-in-wasmedge-15291795c470) to generate text using the [llama2](https://ai.meta.com/llama/) models.\n\n  In this project, we would like to build a Rust SDK to support applications on the [YOLO models](https://pjreddie.com/darknet/yolo/).\n- Expected Outcome:\n  - A Rust SDK that implements the pre-processing and post-processing functions required for the YOLO models. Those functions are implemented in OpenCV and Python in the official YOLO release.\n  - Both image and video inputs should be supported.\n  - Examples and documentation should be provided.\n- Recommended Skills:\n  - OpenCV\n  - Rust\n  - Tensorflow / Pytorch\n  - WebAssembly	{rust,opencv,webassembly}	2023	Term 3	https://github.com/WasmEdge/WasmEdge/issues/2690	https://wasmedge.org/	0	31
619	91398424-f095-4b85-bb0f-e7c56e777ea0	CNCF - Carvel: kctrl to support exporting package repository as tar	{"While generating Package Repository kctrl to create the tar version of the Package Repository instead of pushing the OCI Image to a registry. \n- Expected Outcome: \n    - Proposal containing design discussions and options considered."}	While generating Package Repository kctrl to create the tar version of the Package Repository instead of pushing the OCI Image to a registry. \n- Expected Outcome: \n    - Proposal containing design discussions and options considered.	{golang}	2023	Term 3	https://github.com/carvel-dev/kapp-controller/issues/1277	https://carvel.dev/	0	184
350	6290c764-ae84-4b1c-aaf7-8a0b3c72bf88	Hyperledger - Developer Marketing	{"The Hyperledger Foundation is looking for a  Developer Marketing mentee to support marketing efforts tied to our developer ecosystem. This mentee will work alongside the Hyperledger Foundation Marketing Manager in support of social media marketing, developer events and Hyperledger Global Forum our annual global conference in September and conduct research on which marketing activities can be leveraged to grow developer engagement. Responsibilities will include conducting market analysis, monitoring social platforms, and preparing promotional materials \n\n- Collaborate with the team to perform market analysis and research on the latest trends around developer engagement and marketing \n- Contribute to the content calendar with social media postings on a weekly basis\n- Monitor analytics with the rest of the team to identify viable ideas, trends, and growth patterns"}	The Hyperledger Foundation is looking for a  Developer Marketing mentee to support marketing efforts tied to our developer ecosystem. This mentee will work alongside the Hyperledger Foundation Marketing Manager in support of social media marketing, developer events and Hyperledger Global Forum our annual global conference in September and conduct research on which marketing activities can be leveraged to grow developer engagement. Responsibilities will include conducting market analysis, monitoring social platforms, and preparing promotional materials \n\n- Collaborate with the team to perform market analysis and research on the latest trends around developer engagement and marketing \n- Contribute to the content calendar with social media postings on a weekly basis\n- Monitor analytics with the rest of the team to identify viable ideas, trends, and growth patterns	{blockchain}	2022	Term 2	https://github.com/hyperledger	https://wiki.hyperledger.org/display/INTERN/Developer+Marketing	540000	1
625	89ee4357-dd58-4e15-a601-c411742a587c	CNCF - Istio: Documentation for Ambient Mesh	{"Istio is working on [a new operating mode called ambient mesh](https:","",istio.io,latest,blog,2022,introducing-ambient-mesh,"). As this moves from experimental to the recommended method of operating a service mesh, we will need to revise our documentation to discuss the new model, explain the tradeoffs, and tell users how to choose.\n- Expected Outcome: Revisions to Istio's documentation to reflect the availability of ambient mesh.  These will be maintained in a parallel branch of istio.io that can be pulled from when Ambient is in Beta or GA."}	Istio is working on [a new operating mode called ambient mesh](https://istio.io/latest/blog/2022/introducing-ambient-mesh/). As this moves from experimental to the recommended method of operating a service mesh, we will need to revise our documentation to discuss the new model, explain the tradeoffs, and tell users how to choose.\n- Expected Outcome: Revisions to Istio's documentation to reflect the availability of ambient mesh.  These will be maintained in a parallel branch of istio.io that can be pulled from when Ambient is in Beta or GA.	{writing,advocacy}	2023	Term 3	https://github.com/istio/istio.io/issues/13481	https://istio.io/	0	185
98	6705be57-130f-43f5-ba80-11605ffdb1f9	CoreDNS	{"CoreDNS is a cloud native, authoritative DNS server written in Go. It provides dynamic service discovery backed by different data sources, including etcd and Kubernetes. It serves as a simple, flexible DNS server in both cloud-native and traditional environments."}	CoreDNS is a cloud native, authoritative DNS server written in Go. It provides dynamic service discovery backed by different data sources, including etcd and Kubernetes. It serves as a simple, flexible DNS server in both cloud-native and traditional environments.	{go}	2019	Term 3	https://github.com/coredns/coredns	https://coredns.io	1100000	104
626	0d6fd362-04a1-4086-a6e7-ec753ed4a60b	Meshery	{"As a self-service engineering platform, Meshery enables collaborative design and operation of cloud native infrastructure.\n\nAs a mentee, you will learn cloud native infrastructure management techniques, and will increase your understanding of distributed systems challenges and how to properly implement best-practice patterns of modern software design.\n\nTo best position your candidacy, see the FAQs on https:","",layer5.io,programs.}	As a self-service engineering platform, Meshery enables collaborative design and operation of cloud native infrastructure.\n\nAs a mentee, you will learn cloud native infrastructure management techniques, and will increase your understanding of distributed systems challenges and how to properly implement best-practice patterns of modern software design.\n\nTo best position your candidacy, see the FAQs on https://layer5.io/programs.	{go,react,javascript,rust,docker,kubernetes,jekyll,redux,helm,terraform,graphql,api}	2020	Term 3	https://github.com/meshery/meshery	https://meshery.io/	900000	186
459	2d998240-9363-4a77-a09c-e85b96611024	LF Networking - New OpenDaylight CSIT Framework	{"Currently, OpenDaylight project relies on Robot Framework to run CSIT (Continuous System and Integration Test). This has proven to be very stable and scalable framework for 10+ years, however times have changed and developers working in agile mode usually write system and integration code along with writing new features. In the new paradigm, Robot framework is more an obstacle than a help since developers have to ramp up in a new language that soon find very limiting compared to python or other language. The work in this internship is to look up modern open source test framework (python based) that are well supported and maintained (e.g. pytest, nosetest , etc), select one and write the core test libraries (e.g. REST, SSH, install ODL, etc) so that developers can start using this framework in addition to the existing Robot Framework.\n\nInterested Interns: DO NOT apply through this platform.  Due to a bug, please go to https:","",wiki.lfnetworking.org,x,"CKsZB\nRead the complete description.  When you a ready to apply send your application documents to your Mentor(s) and mentorship@lfnetworking.org"}	Currently, OpenDaylight project relies on Robot Framework to run CSIT (Continuous System and Integration Test). This has proven to be very stable and scalable framework for 10+ years, however times have changed and developers working in agile mode usually write system and integration code along with writing new features. In the new paradigm, Robot framework is more an obstacle than a help since developers have to ramp up in a new language that soon find very limiting compared to python or other language. The work in this internship is to look up modern open source test framework (python based) that are well supported and maintained (e.g. pytest, nosetest , etc), select one and write the core test libraries (e.g. REST, SSH, install ODL, etc) so that developers can start using this framework in addition to the existing Robot Framework.\n\nInterested Interns: DO NOT apply through this platform.  Due to a bug, please go to https://wiki.lfnetworking.org/x/CKsZB\nRead the complete description.  When you a ready to apply send your application documents to your Mentor(s) and mentorship@lfnetworking.org	{jenkins,bash,python,packer}	2022	Term 2	https://wiki.lfnetworking.org/x/CKsZB	https://wiki.lfnetworking.org/x/CKsZB	300000	161
80	f51284ab-f652-47b1-9819-cd4135e75c00	Thanos	{"Thanos is a set of components that can be composed into a highly available metric system with unlimited storage capacity, which can be added seamlessly on top of existing Prometheus deployments."}	Thanos is a set of components that can be composed into a highly available metric system with unlimited storage capacity, which can be added seamlessly on top of existing Prometheus deployments.	{go,kubernetes}	2020	Term 1	https://github.com/thanos-io/thanos	https://thanos.io/	3100000	26
470	19794b46-5311-4fcc-86dd-6393061fb7ec	Porting AOSP 12 emulator to RISC-V RV64G	{"This program pairs one or more mentees with an experienced mentor to deliver an AOSP 12 emulator for 64-bit RISC-V.\n\nRISC-V International has setup Android SIG and released RISC-V Android github repo as https:","",github.com,"riscv-android-src. Current RVI Android codebase is based on AOSP 10, while the latest AOSP is 12 now. We hope to catch up with latest AOSP 12 codebase, and as part of our plan, an emulator is required first as test and run environment.\n\nDeliverables (bullet list of components and the changes expected):\n- Add RISC-V target in the emulator.\n- Porting Ranchu virtual board to support RISC-V machine.\n- Corresponding unit tests implementation\n- Regression tests pass\nNote: upon work should be based on AOSP 12.\n\n\n3. acceptance criteria\n\nAcceptance criteria (bullet list with measurable results defined):\n- Phase I: Virtual peripherals of emulator can work with RISC-V 64bit cores and pass unit tests.\n- Phase II: Emulator can boot-up with Linux plus basic rootfs w","o GUI for AOSP 12.\n- Phase III: Emulator can boot-up with Linux plus rootfs w"," GUI for AOSP 12.\nNote: due to AOSP 12 rootfs is still in developing, a AOSP 10 rootfs from Android SIG repo can be used instead to help verification, but the kernel version should be matched with AOSP 12."}	This program pairs one or more mentees with an experienced mentor to deliver an AOSP 12 emulator for 64-bit RISC-V.\n\nRISC-V International has setup Android SIG and released RISC-V Android github repo as https://github.com/riscv-android-src. Current RVI Android codebase is based on AOSP 10, while the latest AOSP is 12 now. We hope to catch up with latest AOSP 12 codebase, and as part of our plan, an emulator is required first as test and run environment.\n\nDeliverables (bullet list of components and the changes expected):\n- Add RISC-V target in the emulator.\n- Porting Ranchu virtual board to support RISC-V machine.\n- Corresponding unit tests implementation\n- Regression tests pass\nNote: upon work should be based on AOSP 12.\n\n\n3. acceptance criteria\n\nAcceptance criteria (bullet list with measurable results defined):\n- Phase I: Virtual peripherals of emulator can work with RISC-V 64bit cores and pass unit tests.\n- Phase II: Emulator can boot-up with Linux plus basic rootfs w/o GUI for AOSP 12.\n- Phase III: Emulator can boot-up with Linux plus rootfs w/ GUI for AOSP 12.\nNote: due to AOSP 12 rootfs is still in developing, a AOSP 10 rootfs from Android SIG repo can be used instead to help verification, but the kernel version should be matched with AOSP 12.	{}	2022	Term 1	https://github.com/riscv-android-src		300000	171
509	b71de1f8-950a-4794-a1e8-6eb62514ae9c	Performance Model of an Example RISC-V Out-of-Order Superscalar Processor for Community-wide Use	{"RISC-V Performance Modeling SIG is driving the development of Performance Modeling and Simulation Tools for use across RISCV membership, to reduce duplication of effort and to\nfoster collaboration. As such, we have developed a barebones performance model of an example RISCV superscalar processor using C++ based on Sparta simulation framework.\n\nWe have also identified a clear set of features that need to be added to this model to make it more complete. In particular, we would like to add a branch predictor, register renaming and a memory subsystem hierarchy to the model. The identified set \nof features are listed here as issues in the repository: \nhttps:","",github.com,riscv-software-src,riscv-perf-model,"issues\n\nWe invite mentees who are passionate about computer architecture and performance modeling to join us in this effort. This is an excellent opportunity to develop a key piece of \nsoftware that will have high visibility and utility across the entire RISCV community, both in industry and academia. This also presents an opportunity to work with veteran engineers\nfrom industry involved in this project."}	RISC-V Performance Modeling SIG is driving the development of Performance Modeling and Simulation Tools for use across RISCV membership, to reduce duplication of effort and to\nfoster collaboration. As such, we have developed a barebones performance model of an example RISCV superscalar processor using C++ based on Sparta simulation framework.\n\nWe have also identified a clear set of features that need to be added to this model to make it more complete. In particular, we would like to add a branch predictor, register renaming and a memory subsystem hierarchy to the model. The identified set \nof features are listed here as issues in the repository: \nhttps://github.com/riscv-software-src/riscv-perf-model/issues\n\nWe invite mentees who are passionate about computer architecture and performance modeling to join us in this effort. This is an excellent opportunity to develop a key piece of \nsoftware that will have high visibility and utility across the entire RISCV community, both in industry and academia. This also presents an opportunity to work with veteran engineers\nfrom industry involved in this project.	{}	2023	Term 2	https://github.com/riscv-software-src/riscv-perf-model		1020000	171
110	f1b12fb5-af97-40e5-927b-acbd30288183	LF Networking ONAP - Modeling/etsicatalog	{"The Modeling","etsicatalog project provides VNF",PNF,"NS package management service by Micro Service. It also includes a TOSCA parser service which provides generic parser service. As the unified run-time catalog, its VNF",PNF,"NS package management interfaces align with SOL003","005 specification.\n\nEtsicatalog is a standalone web application based on Python3 and DJango framework.\n\nEtsicatalog is going to support SDC Subscription","Notification functions in Guilin version. SDC is the most important design-time component of ONAP. The mentee will be required to make the technical research and take part in the development related this function under the guide of our team. Some work on Alignment with SOL specification for API is also required for the mentee. Besides, We will consider the support for CNF which is a stretch goal (need volunteer）alignment with the CNF task force where applicable."}	The Modeling/etsicatalog project provides VNF/PNF/NS package management service by Micro Service. It also includes a TOSCA parser service which provides generic parser service. As the unified run-time catalog, its VNF/PNF/NS package management interfaces align with SOL003/005 specification.\n\nEtsicatalog is a standalone web application based on Python3 and DJango framework.\n\nEtsicatalog is going to support SDC Subscription/Notification functions in Guilin version. SDC is the most important design-time component of ONAP. The mentee will be required to make the technical research and take part in the development related this function under the guide of our team. Some work on Alignment with SOL specification for API is also required for the mentee. Besides, We will consider the support for CNF which is a stretch goal (need volunteer）alignment with the CNF task force where applicable.	{python,django,database,oop,git}	2020	Term 2	https://gerrit.onap.org/	https://wiki.lfnetworking.org/pages/viewpage.action?pageId=33423625	300000	161
46	79c00d0b-def3-4bfb-9b3a-e79c85426f5d	Zowe Client Java SDK	{"Create SDK written for Java to interact with the Zowe"}	Create SDK written for Java to interact with the Zowe	{java}	2021	Term 2	https://github.com/zowe/api-layer	https://zowe.org	300000	38
245	c035515b-cf76-4225-8f28-da16b0a832e5	CNCF - Meshery: Service mesh playground	{"Create the world’s service mesh playground. Meshery’s genesis is that of helping teach people about service mesh technology and enabling to operate this type of cloud native infrastructure confidently. The proposed project is aimed at furthering this mission with interactive API documentation connected to a service mesh learning playground (a running instance of Meshery)."}	Create the world’s service mesh playground. Meshery’s genesis is that of helping teach people about service mesh technology and enabling to operate this type of cloud native infrastructure confidently. The proposed project is aimed at furthering this mission with interactive API documentation connected to a service mesh learning playground (a running instance of Meshery).	{go,react}	2021	Term 3	https://github.com/layer5io/meshery/issues/2931	https://meshery.io/	300000	186
104	763cd2cf-cce7-425d-a9c0-4eb67ffda8d6	LF Networking ONAP - Automation Testing - Portal/SDC	{"The ONAP Portal is a platform that provides the ability to integrate different ONAP applications into a centralized Portal Core. \n\nSDC is the ONAP visual modeling and design tool. It creates internal metadata that describes assets used by all ONAP components, both at design time and run time.\n\nThe SDC manages the content of a catalog, and logical assemblies of selected catalog items --as needed-- to completely define how and when VNFs  are realized in a target environment.  \n\nDuring the latest release retrospective, the ONAP Integration Team has identified 3 areas of improvements:\n\n1. Integration tests in target deployments using OOM.\n2. Improve robot healthcheck across projects.\n3. More projects added to CI pipeline and more project-specific testing.\n\nThis project will focus to address these improvements defined by the SDC","Portal Projects Teams.\n"}	The ONAP Portal is a platform that provides the ability to integrate different ONAP applications into a centralized Portal Core. \n\nSDC is the ONAP visual modeling and design tool. It creates internal metadata that describes assets used by all ONAP components, both at design time and run time.\n\nThe SDC manages the content of a catalog, and logical assemblies of selected catalog items --as needed-- to completely define how and when VNFs  are realized in a target environment.  \n\nDuring the latest release retrospective, the ONAP Integration Team has identified 3 areas of improvements:\n\n1. Integration tests in target deployments using OOM.\n2. Improve robot healthcheck across projects.\n3. More projects added to CI pipeline and more project-specific testing.\n\nThis project will focus to address these improvements defined by the SDC/Portal Projects Teams.	{java,python,git}	2020	Term 2	https://gerrit.onap.org/	https://wiki.lfnetworking.org/pages/viewpage.action?pageId=34603195	300000	161
111	a325c565-a624-489a-8813-ac44e6f3862b	LF Networking ONAP - Security Requirements - SDC	{"SDC is the ONAP visual modeling and design tool. It creates internal metadata that describes assets used by all ONAP components, both at design time and run time.\n\nThe SDC manages the content of a catalog, and logical assemblies of selected catalog items --as needed-- to completely define how and when VNFs  are realized in a target environment.  A complete virtual assembly of specific catalog items, together with selected workflows and instance configuration data,  completely defines how the deployment, activation, and life-cycle management of VNFs are accomplished.  Selected sub-assemblies may also be represented in the catalog and may be combined with other catalog items, including other sub-assemblies.\n\nONAP has adopted the CII (Core Infrastructure Initiative) Badge Program.\n\nThis project will focus to address the remaining Security requirements defined by the ONAP SECCOM Community based on JIRA Backlog.\n"}	SDC is the ONAP visual modeling and design tool. It creates internal metadata that describes assets used by all ONAP components, both at design time and run time.\n\nThe SDC manages the content of a catalog, and logical assemblies of selected catalog items --as needed-- to completely define how and when VNFs  are realized in a target environment.  A complete virtual assembly of specific catalog items, together with selected workflows and instance configuration data,  completely defines how the deployment, activation, and life-cycle management of VNFs are accomplished.  Selected sub-assemblies may also be represented in the catalog and may be combined with other catalog items, including other sub-assemblies.\n\nONAP has adopted the CII (Core Infrastructure Initiative) Badge Program.\n\nThis project will focus to address the remaining Security requirements defined by the ONAP SECCOM Community based on JIRA Backlog.	{java,python,git}	2020	Term 2	https://gerrit.onap.org/	https://wiki.lfnetworking.org/display/LN/ONAP+Security+Requirements+-+SDC	600000	161
360	9959277e-eefc-4c88-83b6-e8c4b011d557	CNCF - Service Mesh Performance: Adaptive Load Controller	{"The adaptive load controller is to execute optimization routines recursivley to determine the maximum load a system can sustain. The maximum load is usually defined by the maximum requests per second (rps) the system can handle. The metrics (CPU usage, latency etc) collected from the system under test are the constraints we provide to judge whether a system under test (SUT) is sustaining the load.\n\nA use-case that fits very well is be the ability to use it to run performance tests on a schedule and track the maximum load a system can handle over time. This could give insights to performance improvements or degradations."}	The adaptive load controller is to execute optimization routines recursivley to determine the maximum load a system can sustain. The maximum load is usually defined by the maximum requests per second (rps) the system can handle. The metrics (CPU usage, latency etc) collected from the system under test are the constraints we provide to judge whether a system under test (SUT) is sustaining the load.\n\nA use-case that fits very well is be the ability to use it to run performance tests on a schedule and track the maximum load a system can handle over time. This could give insights to performance improvements or degradations.	{go,grpc,docker,kubernetes}	2022	Term 3	https://github.com/service-mesh-performance/service-mesh-performance/issues/350	https://smp-spec.io/	0	121
6	dbe51fb4-b796-471a-ade8-b80edbfa2501	Linux kernel Bug Fixing Fall Unpaid 2022	{"There are many bugs being found in the Linux kernel by automated tools, yet not many people available to help fix these.  This project will help the applicants choose bugs to fix, work through the problem found, and help get the issue resolved in the Linux kernel by producing a fix or working with the community on a fix.\n\nAfter the end of the project, the applicant will have a much deeper knowledge of many different areas of the Linux kernel and hopefully, many bugs fixed and many patches accepted."}	There are many bugs being found in the Linux kernel by automated tools, yet not many people available to help fix these.  This project will help the applicants choose bugs to fix, work through the problem found, and help get the issue resolved in the Linux kernel by producing a fix or working with the community on a fix.\n\nAfter the end of the project, the applicant will have a much deeper knowledge of many different areas of the Linux kernel and hopefully, many bugs fixed and many patches accepted.	{c,shell,kernel}	2022	Term 3	https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/	https://wiki.linuxfoundation.org/lkmp	0	116
5	e7b7c417-e2cf-448a-bd8e-16ed7f3f28e2	Linux kernel Bug Fixing Spring 2022	{"There are many bugs being found in the Linux kernel by automated tools, yet not many people available to help fix these.  This project will help the applicants choose bugs to fix, work through the problem found, and help get the issue resolved in the Linux kernel by producing a fix or working with the community on a fix.\n\nAfter the end of the project, the applicant will have a much deeper knowledge of many different areas of the Linux kernel and hopefully, many bugs fixed and many patches accepted."}	There are many bugs being found in the Linux kernel by automated tools, yet not many people available to help fix these.  This project will help the applicants choose bugs to fix, work through the problem found, and help get the issue resolved in the Linux kernel by producing a fix or working with the community on a fix.\n\nAfter the end of the project, the applicant will have a much deeper knowledge of many different areas of the Linux kernel and hopefully, many bugs fixed and many patches accepted.	{c,shell,kernel}	2022	Term 1	https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/	https://wiki.linuxfoundation.org/lkmp	2340000	116
181	859e75d7-d85a-419e-801e-f1c64c4b3b47	Linux kernel Bug Fixing Summer 2021	{"There are many bugs being found in the Linux kernel by automated tools, yet not many people available to help fix these.  This project will help the applicants choose bugs to fix, work through the problem found, and help get the issue resolved in the Linux kernel by producing a fix or working with the community on a fix.\n\nAfter the end of the project, the applicant will have a much deeper knowledge of many different areas of the Linux kernel and hopefully, many bugs fixed and many patches accepted."}	There are many bugs being found in the Linux kernel by automated tools, yet not many people available to help fix these.  This project will help the applicants choose bugs to fix, work through the problem found, and help get the issue resolved in the Linux kernel by producing a fix or working with the community on a fix.\n\nAfter the end of the project, the applicant will have a much deeper knowledge of many different areas of the Linux kernel and hopefully, many bugs fixed and many patches accepted.	{c,shell,kernel}	2021	Term 2	https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/	https://wiki.linuxfoundation.org/lkmp	3121000	116
7	8bdac7b8-403f-489d-8999-ed16bca09cd2	Linux kernel Bug Fixing Summer 2022	{"There are many bugs being found in the Linux kernel by automated tools, yet not many people available to help fix these.  This project will help the applicants choose bugs to fix, work through the problem found, and help get the issue resolved in the Linux kernel by producing a fix or working with the community on a fix.\n\nAfter the end of the project, the applicant will have a much deeper knowledge of many different areas of the Linux kernel and hopefully, many bugs fixed and many patches accepted."}	There are many bugs being found in the Linux kernel by automated tools, yet not many people available to help fix these.  This project will help the applicants choose bugs to fix, work through the problem found, and help get the issue resolved in the Linux kernel by producing a fix or working with the community on a fix.\n\nAfter the end of the project, the applicant will have a much deeper knowledge of many different areas of the Linux kernel and hopefully, many bugs fixed and many patches accepted.	{c,shell,kernel}	2022	Term 2	https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/	https://wiki.linuxfoundation.org/lkmp	2100000	116
362	8b4aeab0-f891-4a67-a510-61393ca38520	CNCF - Devfile: Add Compose file support in the spec API II	{"Devfiles are YAML files that define remote development environments. The main part of a Devfile is the `components` section and that's where the containers required to code, build and test an application are specified. The Devfile can either include those containers defintions or reference external files such as Dockerfiles or Kubernetes manifests. [The Compose file](https:","",github.com,compose-spec,compose-spec,blob,master,"spec.md) is a popular format in open source development projects to define runtime environments for testing the application but those cannot be referenced by a Devfile yet. The goal is to continue the work that has been started a couple of months ago to allow referencing a Compose file from a Devfile. The expected outcome is to create a PoC written in go that parses a Compose file such as [this one](https:","",github.com,microservices-demo,microservices-demo,blob,master,deploy,docker-compose,"docker-compose.yml) using [kompose](https:","",github.com,kubernetes,"kompose) (as a library, not as an executable) and that creates the objects corresponding to the Compose file services in a Kubernetes cluster."}	Devfiles are YAML files that define remote development environments. The main part of a Devfile is the `components` section and that's where the containers required to code, build and test an application are specified. The Devfile can either include those containers defintions or reference external files such as Dockerfiles or Kubernetes manifests. [The Compose file](https://github.com/compose-spec/compose-spec/blob/master/spec.md) is a popular format in open source development projects to define runtime environments for testing the application but those cannot be referenced by a Devfile yet. The goal is to continue the work that has been started a couple of months ago to allow referencing a Compose file from a Devfile. The expected outcome is to create a PoC written in go that parses a Compose file such as [this one](https://github.com/microservices-demo/microservices-demo/blob/master/deploy/docker-compose/docker-compose.yml) using [kompose](https://github.com/kubernetes/kompose) (as a library, not as an executable) and that creates the objects corresponding to the Compose file services in a Kubernetes cluster.	{go,compose,kubernetes}	2022	Term 3	https://github.com/devfile/api/issues/501	https://devfile.io/	0	12
621	65d4c337-66fd-4d22-8a21-e836fafbebc4	Linux kernel Bug Fixing Fall Unpaid 2023	{"There are many bugs being found in the Linux kernel by automated tools, yet not many people available to help fix these.  This project will help the applicants choose bugs to fix, work through the problem found, and help get the issue resolved in the Linux kernel by producing a fix or working with the community on a fix.\n\nAfter the end of the project, the applicant will have a much deeper knowledge of many different areas of the Linux kernel and hopefully, many bugs fixed and many patches accepted."}	There are many bugs being found in the Linux kernel by automated tools, yet not many people available to help fix these.  This project will help the applicants choose bugs to fix, work through the problem found, and help get the issue resolved in the Linux kernel by producing a fix or working with the community on a fix.\n\nAfter the end of the project, the applicant will have a much deeper knowledge of many different areas of the Linux kernel and hopefully, many bugs fixed and many patches accepted.	{c,shell,kernel}	2023	Term 3	https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/	https://wiki.linuxfoundation.org/lkmp	0	116
547	f1b93bd9-d58f-42e1-a088-752633a088ff	Linux kernel Bug Fixing Summer Unpaid 2023	{"There are many bugs being found in the Linux kernel by automated tools, yet not many people available to help fix these.  This project will help the applicants choose bugs to fix, work through the problem found, and help get the issue resolved in the Linux kernel by producing a fix or working with the community on a fix.\n\nAfter the end of the project, the applicant will have a much deeper knowledge of many different areas of the Linux kernel and hopefully, many bugs fixed and many patches accepted."}	There are many bugs being found in the Linux kernel by automated tools, yet not many people available to help fix these.  This project will help the applicants choose bugs to fix, work through the problem found, and help get the issue resolved in the Linux kernel by producing a fix or working with the community on a fix.\n\nAfter the end of the project, the applicant will have a much deeper knowledge of many different areas of the Linux kernel and hopefully, many bugs fixed and many patches accepted.	{c,shell,kernel}	2023	Term 2	https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/	https://wiki.linuxfoundation.org/lkmp	0	116
16	6c89d370-df87-4924-bc1e-c5565d592419	Hyperledger Fabric - Add Interactive Examples to Fabric Python SDK	{"This project aims to add interactive examples to fabric-python-sdk using jupyter notebook. The intern is expected to first understand the functionalities of fabric sdk (v0.8.1) and add missing functionalities of fabric 2.0 (e.g. wallet storage, etc.) to fabric sdk based on nodejs","golang sdk. Then, write jupyter notebook examples using fabric sdk and demonstrate functionalities of sdk. It provides new developers with better understanding of using fabric sdk."}	This project aims to add interactive examples to fabric-python-sdk using jupyter notebook. The intern is expected to first understand the functionalities of fabric sdk (v0.8.1) and add missing functionalities of fabric 2.0 (e.g. wallet storage, etc.) to fabric sdk based on nodejs/golang sdk. Then, write jupyter notebook examples using fabric sdk and demonstrate functionalities of sdk. It provides new developers with better understanding of using fabric sdk.	{python,blockchain}	2020	Term 2	https://github.com/hyperledger/fabric	https://wiki.hyperledger.org/pages/viewpage.action?pageId=29035315	300000	110
171	85add1cb-8721-4715-af1e-d1cc75fe6f73	Hyperledger Fabric + Aries Integration - To Support Fabric as Blockchain Ledger	{"Hyperledger Aries currently support only Hyperledger Indy blockchain. Verifiable credential(VC) and decentralised identifier(DID) standard & specification emerged in last few years. Trust over IP , Sovrin network, covid credential initiative  are using Indy & Aries technology framework. \n\nHyperledger Fabric general enterprise blockchain framework and most used Hyperledger project, latest Forbes Blockchain 50 report shows that 60% companies using Hyperledger Fabric for their enterprise grade blockchain. All the major cloud provides fabric as BaaS( Blockchain as a Service). Currently every identity","credential application using Indy","Aries stack to build W3C complaint VC & DID applications. There will be huge adoption and applicability of Hyperledger Fabric & Aries integration project.\n\nAs per Hyperledger project governance there should be Hyperledger project interoperability to huge adoption of Hyperledger technologies.\n\nMentee will learn:\n1) Hyperledger Fabric Architecture & SDK","API\n2) Hyperledger Aries Javascript framework\n3) VC","DID standard and Application\n\nExpected Outcome\nHyperledger Aries & fabric Wrapper or Aries will have support to use Hyperledger Fabric as Blockchain for storing credentials."}	Hyperledger Aries currently support only Hyperledger Indy blockchain. Verifiable credential(VC) and decentralised identifier(DID) standard & specification emerged in last few years. Trust over IP , Sovrin network, covid credential initiative  are using Indy & Aries technology framework. \n\nHyperledger Fabric general enterprise blockchain framework and most used Hyperledger project, latest Forbes Blockchain 50 report shows that 60% companies using Hyperledger Fabric for their enterprise grade blockchain. All the major cloud provides fabric as BaaS( Blockchain as a Service). Currently every identity/credential application using Indy/Aries stack to build W3C complaint VC & DID applications. There will be huge adoption and applicability of Hyperledger Fabric & Aries integration project.\n\nAs per Hyperledger project governance there should be Hyperledger project interoperability to huge adoption of Hyperledger technologies.\n\nMentee will learn:\n1) Hyperledger Fabric Architecture & SDK/API\n2) Hyperledger Aries Javascript framework\n3) VC/DID standard and Application\n\nExpected Outcome\nHyperledger Aries & fabric Wrapper or Aries will have support to use Hyperledger Fabric as Blockchain for storing credentials.	{go,javascript}	2021	Term 2	https://github.com/hyperledger/fabric	https://wiki.hyperledger.org/display/INTERN/Hyperledger+Fabric+-+Hyperledger+Aries+Integration+to+support+Fabric+as+Blockchain+ledger	300000	110
33	625d1198-a741-45a8-bf03-ff439fb2ebe8	Hyperledger Iroha - Extend HL Iroha queries with optional arguments	{"Hyperledger Iroha has implemented set of commands and queries. In the internship's project for few Iroha's queries (GetAccountTransactions and GetAccountAssetTransactions and optionally GetPendingTransactions) should be added optional arguments e.g.: from and to to return transactions in the provided time range or in provided blocks range.\n\nProblem description\n\nFetching transactions in Iroha can be from first account's transaction or from provided transaction hash. On the other bound transactions are limited by max number of transactions to fetch. So when it is necessarily to fetch transactions for a month: hash of first transaction needs to be stored in blockchain and number of transactions should be \\"guessed\\" -if too big it would take longer, if too small another query would be necessarily). It also requires few queries instead of one.\n\nThe mentee will be able to learn:\n1) architecture of Iroha (1.x)\n2) work in true spirit of open-source, communicating with Iroha community, joining calls\n3) writing documentation\n4) following rules of open-source code of Hyperledger Iroha,\ncreating automatic tests of code\n\nExpected Outcome\nImplementation according to description and plan of documentation and tests. Added possibility of running modified queries from iroha-python library."}	Hyperledger Iroha has implemented set of commands and queries. In the internship's project for few Iroha's queries (GetAccountTransactions and GetAccountAssetTransactions and optionally GetPendingTransactions) should be added optional arguments e.g.: from and to to return transactions in the provided time range or in provided blocks range.\n\nProblem description\n\nFetching transactions in Iroha can be from first account's transaction or from provided transaction hash. On the other bound transactions are limited by max number of transactions to fetch. So when it is necessarily to fetch transactions for a month: hash of first transaction needs to be stored in blockchain and number of transactions should be "guessed" -if too big it would take longer, if too small another query would be necessarily). It also requires few queries instead of one.\n\nThe mentee will be able to learn:\n1) architecture of Iroha (1.x)\n2) work in true spirit of open-source, communicating with Iroha community, joining calls\n3) writing documentation\n4) following rules of open-source code of Hyperledger Iroha,\ncreating automatic tests of code\n\nExpected Outcome\nImplementation according to description and plan of documentation and tests. Added possibility of running modified queries from iroha-python library.	{sql,postgresql,rocketdb,protobuf,python}	2021	Term 2	https://github.com/hyperledger/iroha	https://wiki.hyperledger.org/display/INTERN/Extend+HL+Iroha+queries+with+optional+arguments	300000	110
69	fe4eed7e-12a9-4fe8-9fd0-83d948d4da98	Hyperledger Iroha - Integration with Hyperledger Quilt	{"Hyperledger Quilt is a Java implementation of Interledger, a suite of open protocols and standards that allows payments interoperability across any currency - fiat or crypto. And Iroha is great with asset management, which makes those two projects a perfect fit! \n\nWith this internship program we will not only allow the interoperability between different asset-carrying blockchains but also help create a system of Iroha networks that will only share some information, that is a requested feature."}	Hyperledger Quilt is a Java implementation of Interledger, a suite of open protocols and standards that allows payments interoperability across any currency - fiat or crypto. And Iroha is great with asset management, which makes those two projects a perfect fit! \n\nWith this internship program we will not only allow the interoperability between different asset-carrying blockchains but also help create a system of Iroha networks that will only share some information, that is a requested feature.	{java}	2020	Term 2	https://github.com/hyperledger/iroha	https://wiki.hyperledger.org/display/INTERN/HL+Iroha+and+HL+Quilt+Integration	300000	110
415	85f61cae-02d7-4931-8d87-d3da3128060e	CNCF - Kubevela: Support auto generation of CUE schema and docs from Go struct	{"Description: In KubeVela's provider system, we can use our defined Go functions in CUE schema. The Go providers usually have a parameter and return. Fields in Go providers are the same as fields in CUE schema, so it is possible and important to support automatic generation of CUE schemas and documents from Go structs.\n\nExpected Outcome: Auto-generators of CUE schemas and docs from Go structs, the capabilities should be wrapped in vela cli command."}	Description: In KubeVela's provider system, we can use our defined Go functions in CUE schema. The Go providers usually have a parameter and return. Fields in Go providers are the same as fields in CUE schema, so it is possible and important to support automatic generation of CUE schemas and documents from Go structs.\n\nExpected Outcome: Auto-generators of CUE schemas and docs from Go structs, the capabilities should be wrapped in vela cli command.	{go,cuelang}	2023	Term 1	https://github.com/kubevela/kubevela/issues/5364	https://kubevela.io/	300000	117
428	2981c1de-49af-4bd8-b87d-02e455a96ee1	CNCF - Kubevela: Support auto generation of multiple languages SDK from CUE	{"Description: In KubeVela, we use CUElang to code the definition. We want to support auto generation of multiple languages SDK from CUE, so that users can use KubeVela in their own language.\n\nExpected Outcome: Support auto generation of multiple languages SDK from CUE, including Golang, Java, Python, etc. The capabilities should be wrapped in vela cli command."}	Description: In KubeVela, we use CUElang to code the definition. We want to support auto generation of multiple languages SDK from CUE, so that users can use KubeVela in their own language.\n\nExpected Outcome: Support auto generation of multiple languages SDK from CUE, including Golang, Java, Python, etc. The capabilities should be wrapped in vela cli command.	{go,kubernetes,cuelang}	2023	Term 1	https://github.com/kubevela/kubevela/issues/5365	https://kubevela.io/	300000	117
170	8ed6015e-d233-4ead-9ec4-1ac88a7d7056	Hyperledger Iroha + Burrow - Extend Existing Solidity VM Integration	{"Hyperledger Iroha has Smart Contracts from Hyperledger Burrow. It was done in internship project in 2019, then integrated into main Iroha code in version 1.2. Unfortunately integration does not integrate all Iroha commands and queries (currently only: TansferAsset and GetAccountAssets are integrated). So task is to integrate all commands and queries and also permissions.\n\nThe mentee will be able to learn:\n1) ways of integrating different projects from architectural point of view,\n2) architecture of Iroha (1.x) and optionally Burrow,\n3) work in true spirit of open-source, communicating with both Iroha and optionally Burrow community, joining calls and using other community tools,\n4) writing documentation, so anyone in the community could use the results of their work,\n5) ollowing rules and standards of open-source projects created by hyperledger,\n6) Intern would get experience in virtual machines\n\nExpected Outcome\nDocumented integration of all Iroha (version 1.x) commands, queries and permissions with unit tests."}	Hyperledger Iroha has Smart Contracts from Hyperledger Burrow. It was done in internship project in 2019, then integrated into main Iroha code in version 1.2. Unfortunately integration does not integrate all Iroha commands and queries (currently only: TansferAsset and GetAccountAssets are integrated). So task is to integrate all commands and queries and also permissions.\n\nThe mentee will be able to learn:\n1) ways of integrating different projects from architectural point of view,\n2) architecture of Iroha (1.x) and optionally Burrow,\n3) work in true spirit of open-source, communicating with both Iroha and optionally Burrow community, joining calls and using other community tools,\n4) writing documentation, so anyone in the community could use the results of their work,\n5) ollowing rules and standards of open-source projects created by hyperledger,\n6) Intern would get experience in virtual machines\n\nExpected Outcome\nDocumented integration of all Iroha (version 1.x) commands, queries and permissions with unit tests.	{go,solidity,vm}	2021	Term 2	https://wiki.hyperledger.org/display/iroha	https://wiki.hyperledger.org/display/INTERN/HL+Burrow+and+HL+Iroha+extend+existing+Solidity+VM+integration	300000	110
624	bbebd511-1a3e-4c4f-b106-2f09690825c5	CNCF - Istio: Implement performance testing	{"Up until version 1.16, [Istio](https:","",istio.io,") published [performance and scale testing results](https:","",istio.io,v1.16,docs,ops,deployment,performance-and-scalability,"). These should be returned to service, and updated to support ambient mesh. Third-party benchmarking tools should be updated to support testing the performance of ambient mesh.\n- Expected Outcome: Performance testing pages are returned to istio.io, and include both sidecar and ambient mesh results."}	Up until version 1.16, [Istio](https://istio.io/) published [performance and scale testing results](https://istio.io/v1.16/docs/ops/deployment/performance-and-scalability/). These should be returned to service, and updated to support ambient mesh. Third-party benchmarking tools should be updated to support testing the performance of ambient mesh.\n- Expected Outcome: Performance testing pages are returned to istio.io, and include both sidecar and ambient mesh results.	{python,networking}	2023	Term 3	https://github.com/istio/istio/issues/44009	https://istio.io/	0	185
52	57571877-84ec-415a-ad0b-b076e20f3ad0	CNCF - Meshery/SMI: Service mesh playground	{"Create the world’s service mesh playground. Meshery’s genesis is that of helping teach people about service mesh technology and enabling to operate this type of cloud native infrastructure confidently. The proposed project is aimed at furthering this mission with interactive API documentation connected to a service mesh learning playground (a running instance of Meshery)."}	Create the world’s service mesh playground. Meshery’s genesis is that of helping teach people about service mesh technology and enabling to operate this type of cloud native infrastructure confidently. The proposed project is aimed at furthering this mission with interactive API documentation connected to a service mesh learning playground (a running instance of Meshery).	{go,react}	2021	Term 2	https://github.com/layer5io/meshery/issues/2931	https://meshery.io/	300000	186
347	cf4de999-41a9-4769-a298-d7d4b208ab3d	CNCF - Meshery: Design Configurator	{"Integrate a new user experience into Meshery: a cloud native design configurator. This project involves presentation of Kuberenetes core resources and any custom resource (CRD) as a configurable component in a React-based user interface in which users design (in great detail) and deploy their cloud native infrastructure. Interns will familiarize with concepts of content lifecycle management."}	Integrate a new user experience into Meshery: a cloud native design configurator. This project involves presentation of Kuberenetes core resources and any custom resource (CRD) as a configurable component in a React-based user interface in which users design (in great detail) and deploy their cloud native infrastructure. Interns will familiarize with concepts of content lifecycle management.	{react,openapi}	2022	Term 2	https://github.com/meshery/meshery/issues/5504	https://meshery.io/	300000	186
363	ea439582-8c63-498d-9066-dc563ce1172e	CNCF - Meshery: Integration of Open Policy Agent (OPA) and Meshery	{"As a golang library integrate OPA into Meshery Server, enabling users to define policies to dictate the manner in which their cloud native infrastructure is to both run and be configured. Design an extensible policy framework in which rules may be augmented and dynamically supplied at runtime."}	As a golang library integrate OPA into Meshery Server, enabling users to define policies to dictate the manner in which their cloud native infrastructure is to both run and be configured. Design an extensible policy framework in which rules may be augmented and dynamically supplied at runtime.	{go,react,rego}	2022	Term 3	https://github.com/meshery/meshery/issues/544	https://meshery.io/	0	186
433	2ee7a912-e26e-4602-9dfc-4febe3842df3	CNCF - Meshery: Multi-user cloud native playground	{"Advance the cloud native playground in which any CNCF project can be explored. Meshery’s genesis is that of helping teach people about cloud native technology and enabling to operate various types of cloud native infrastructure confidently. The proposed project is aimed at furthering this mission by infusing multi-user collaboration as a pervasisve feature so that users can learn together in a running instance of Meshery."}	Advance the cloud native playground in which any CNCF project can be explored. Meshery’s genesis is that of helping teach people about cloud native technology and enabling to operate various types of cloud native infrastructure confidently. The proposed project is aimed at furthering this mission by infusing multi-user collaboration as a pervasisve feature so that users can learn together in a running instance of Meshery.	{go,react,css}	2023	Term 1	https://github.com/meshery/meshery/issues/7020	https://meshery.io/	300000	186
384	7592d7db-5517-445b-95e8-14144c49e9b1	CNCF - Meshery: User Interface Overhaul: State Management w/Apollo/GraphQL	{"Overcome current architectural issues of:\n1) No Caching - In Meshery UI, List of adapters is a state that is being used in multiple components i.e Settings , Dashboard , Connection Wizard and Performance. Refetching the data on every mount of each of these components degrades the user experience. The same goes for all the other data that are being used across multiple components.\n2) Multiple Sources of Truth - There is no single source of truth in Meshery UI as all react components manage their own state. Since Meshery UI has to deal with data that frequently changes, like Control Plane Data, Meshsync data etc. it will become hard to keep them in sync if they all manage their own copy of them  in their local state."}	Overcome current architectural issues of:\n1) No Caching - In Meshery UI, List of adapters is a state that is being used in multiple components i.e Settings , Dashboard , Connection Wizard and Performance. Refetching the data on every mount of each of these components degrades the user experience. The same goes for all the other data that are being used across multiple components.\n2) Multiple Sources of Truth - There is no single source of truth in Meshery UI as all react components manage their own state. Since Meshery UI has to deal with data that frequently changes, like Control Plane Data, Meshsync data etc. it will become hard to keep them in sync if they all manage their own copy of them  in their local state.	{react,apollo,graphql,redux}	2022	Term 3	https://github.com/meshery/meshery/issues/5094	https://meshery.io/	300000	186
251	ce535d43-16a5-4db8-a048-2f56327b7939	CNCF - Kubernetes: Automation of AMI build/test/publish pipelines for Cluster API Provider AWS	{"Description: Cluster API (CAPI) is a Kubernetes sub-project focused on providing declarative APIs and tooling to simplify lifecycle management of Kubernetes clusters. CAPA is the infrastructure provider that extends Cluster API to manage Kubernetes clusters on AWS. As a mentee, you will start with learning CAPI","CAPA concepts and then, will work on the main project which is to automate AMI build, test, and publish workflows using Prow, Github, and other Kubernetes automation tools."}	Description: Cluster API (CAPI) is a Kubernetes sub-project focused on providing declarative APIs and tooling to simplify lifecycle management of Kubernetes clusters. CAPA is the infrastructure provider that extends Cluster API to manage Kubernetes clusters on AWS. As a mentee, you will start with learning CAPI/CAPA concepts and then, will work on the main project which is to automate AMI build, test, and publish workflows using Prow, Github, and other Kubernetes automation tools.	{go,testing,git,automation,ci,cd}	2022	Term 1	https://github.com/kubernetes-sigs/cluster-api-provider-aws/issues/1982	http://kubernetes.io/	300000	75
130	953e5f12-460b-4bf1-80b3-5171c2044462	CNCF - Kubernetes: Create Application for Elections Authenticated by External Oauth	{"Create a web-based voting application, with voting logic based on CIVS project, that allows use of external Oauth, such as Github, for voter authentication.\n\nFull Specification for Kubernetes Voting Application - https:","",docs.google.com,document,d,e,2PACX-1vQ4Z3jOpIsGZHaBy9Xa6me6IyL_rj-JZgAl-xRO-M2KacWhcRexcV3mILjwclsc9QI4ghRfic2ESFtB,pub}	Create a web-based voting application, with voting logic based on CIVS project, that allows use of external Oauth, such as Github, for voter authentication.\n\nFull Specification for Kubernetes Voting Application - https://docs.google.com/document/d/e/2PACX-1vQ4Z3jOpIsGZHaBy9Xa6me6IyL_rj-JZgAl-xRO-M2KacWhcRexcV3mILjwclsc9QI4ghRfic2ESFtB/pub	{go,python,javascript}	2021	Term 1	https://github.com/kubernetes/community/issues/5096	https://docs.google.com/document/d/e/2PACX-1vQ4Z3jOpIsGZHaBy9Xa6me6IyL_rj-JZgAl-xRO-M2KacWhcRexcV3mILjwclsc9QI4ghRfic2ESFtB/pub	300000	75
231	9e7f18e3-68ee-44f8-ac74-55e802fce8e3	CNCF - Kubernetes: Improvements to Cluster API provider for GCP (CAPG)	{"The Cluster API is a Kubernetes project to bring declarative, Kubernetes-style APIs to cluster creation, configuration, and management. CAPG provides this Kubernetes-native declarative infrastructure for GCP. The project would start with some help wanted issues around quick start and documentation, this will help with understanding mentee with CAPI","CAPG concepts and current implementation. Then we will move on to some long pending improvements documented in the issues link below."}	The Cluster API is a Kubernetes project to bring declarative, Kubernetes-style APIs to cluster creation, configuration, and management. CAPG provides this Kubernetes-native declarative infrastructure for GCP. The project would start with some help wanted issues around quick start and documentation, this will help with understanding mentee with CAPI/CAPG concepts and current implementation. Then we will move on to some long pending improvements documented in the issues link below.	{go,testing}	2021	Term 2	https://github.com/kubernetes-sigs/cluster-api-provider-gcp/issues	http://kubernetes.io/	600000	75
559	005db8db-7efe-4433-9605-91d14174c72c	CNCF - Meshery: OPA policy evaluation in-browser using WebAssembly and Rego	{"Meshery's highly dynamic infrastructure configuration capabilities require real-time evaluation of complex policies. Policies of various types and with a high number of parameters need to be evaluted client-side. With policies expressed in Rego, the goal of this project is to incorporate use of the https:","",github.com,open-policy-agent,"golang-opa-wasm project into Meshery UI.\n- Expected outcome: a powerful real-time multi-user collaboration experience."}	Meshery's highly dynamic infrastructure configuration capabilities require real-time evaluation of complex policies. Policies of various types and with a high number of parameters need to be evaluted client-side. With policies expressed in Rego, the goal of this project is to incorporate use of the https://github.com/open-policy-agent/golang-opa-wasm project into Meshery UI.\n- Expected outcome: a powerful real-time multi-user collaboration experience.	{go,wasm}	2023	Term 2	https://github.com/meshery/meshery/issues/7019	https://meshery.io/	300000	186
587	aff716df-c257-4ead-8b48-39a3f9272b7f	CNCF - Meshery: Package Meshery Catalog Artifacts as OCI Images	{"Meshery (https:","","meshery.io) is a self-service engineering platform, Meshery enables collaborative design and operation of cloud native infrastructure. [Meshery Catalog](https:","",meshery.io,"catalog) content represents a schema-based description of cloud native infrastructure. Catalog content need to be portable between Meshery deployments as well as easily version-able in external repositories.\n- Expected outcome: Rebuild the Meshery Design System so that it provides the open source building blocks to design and implement consistent, accessible, and delightful product experiences."}	Meshery (https://meshery.io) is a self-service engineering platform, Meshery enables collaborative design and operation of cloud native infrastructure. [Meshery Catalog](https://meshery.io/catalog) content represents a schema-based description of cloud native infrastructure. Catalog content need to be portable between Meshery deployments as well as easily version-able in external repositories.\n- Expected outcome: Rebuild the Meshery Design System so that it provides the open source building blocks to design and implement consistent, accessible, and delightful product experiences.	{golang,graphql,reactjs}	2023	Term 3	https://github.com/meshery/meshery/issues/8348	https://meshery.io/	0	186
467	f994928b-8998-4cd3-b66e-c576aa99c9d5	RISC-V Mentorship: MLIR Convolution Vectorization	{"The RISC-V Mentorship Program enables one or more 12-week internship-style projects per session, funded by RISC-V, to match mentors","project leaders together with mentees","interns . Mentees are guided through a series of milestones by one or more project mentors, with whom the mentees meet on a weekly basis.\n\nConvolution is the core operation of deep learning models and computer vision applications. MLIR supports various convolution operations. Our project is to vectorize them for the RISC-V backend. There are several methods to implement convolution vectorization, such as optimizing nested loops, implementing vectorization algorithm, converting to GEMM, etc. This project needs to choose a vectorization method and implement a conversion pass for the convolution operations. As for the vector semantic support, MLIR has the “Vector” dialect for the general vector abstraction, and it also allows the backend-specific vector dialect, such as the “x86vector” dialect, “arm_neon” dialect, and “arm_neon” dialect. Like these dialects, the project also needs to propose an “RVV” dialect and work with existing dialects and tools.\n\nDeliverables:\n- An MLIR “RVV” Dialect. (Operations in the dialect can support the convolution vectorization)\n- A conversion pass to vectorize convolution operations in “Linalg” dialect with “RVV” dialect enabled.\n- A conversion pass to lower the operations in “RVV” dialect to “LLVM IR” dialect.\n- Unit tests for “RVV” dialect and conversion passes."}	The RISC-V Mentorship Program enables one or more 12-week internship-style projects per session, funded by RISC-V, to match mentors/project leaders together with mentees/interns . Mentees are guided through a series of milestones by one or more project mentors, with whom the mentees meet on a weekly basis.\n\nConvolution is the core operation of deep learning models and computer vision applications. MLIR supports various convolution operations. Our project is to vectorize them for the RISC-V backend. There are several methods to implement convolution vectorization, such as optimizing nested loops, implementing vectorization algorithm, converting to GEMM, etc. This project needs to choose a vectorization method and implement a conversion pass for the convolution operations. As for the vector semantic support, MLIR has the “Vector” dialect for the general vector abstraction, and it also allows the backend-specific vector dialect, such as the “x86vector” dialect, “arm_neon” dialect, and “arm_neon” dialect. Like these dialects, the project also needs to propose an “RVV” dialect and work with existing dialects and tools.\n\nDeliverables:\n- An MLIR “RVV” Dialect. (Operations in the dialect can support the convolution vectorization)\n- A conversion pass to vectorize convolution operations in “Linalg” dialect with “RVV” dialect enabled.\n- A conversion pass to lower the operations in “RVV” dialect to “LLVM IR” dialect.\n- Unit tests for “RVV” dialect and conversion passes.	{}	2021	Term 2	https://github.com/riscv	https://riscv.org/community/risc-v-mentorship-program/	900000	171
468	f4314dad-5cdd-4179-a6ea-53a2fc9f55a4	Syzkaller on FreeBSD/RISC-V	{"The goal of this project is to get syzkaller, a coverage-guided OS kernel fuzzer, working under FreeBSD","RISC-V. As syzkaller is written mostly in Go, the first step is to ensure that Go is able to compile syzkaller. Previous work can be harnessed from two projects to make this tractable. First, Go has a RISC-V port targeting GNU","Linux, and second, the CHERI project has a FreeBSD","mips port of Go. The next step is to add FreeBSD","RISC-V support to syzkaller itself. A work-in-progress patch for FreeBSD","arm64 support can be used as a template. Two bonus tasks are: 1. upstream Go support for FreeBSD","RISC-V to the Go Project, and 2. begin fuzzing system calls under FreeBSD","RISC-V.\n\nSyzkaller is flexible enough to target OS interfaces other than system calls. For example, it has been used to fuzz the Linux USB stack and has found dozens of bugs in the USB subsystem alone. The details are complicated, but the idea is simple.\n\n1. Generate a program, which invokes one or more system calls.\n2. Run the generated program.\n3. Check to see if the system diagnosed an error.\n  - If not, collect kernel code coverage information and decide whether to try iterating upon the previous test program, or start anew.\n  - If so, collect information about the crash and try to discover a minimal test case that triggers the crash.\n\nThe mentors will have weekly meetings with the mentee to discuss progress and strategies for overcoming any challenges."}	The goal of this project is to get syzkaller, a coverage-guided OS kernel fuzzer, working under FreeBSD/RISC-V. As syzkaller is written mostly in Go, the first step is to ensure that Go is able to compile syzkaller. Previous work can be harnessed from two projects to make this tractable. First, Go has a RISC-V port targeting GNU/Linux, and second, the CHERI project has a FreeBSD/mips port of Go. The next step is to add FreeBSD/RISC-V support to syzkaller itself. A work-in-progress patch for FreeBSD/arm64 support can be used as a template. Two bonus tasks are: 1. upstream Go support for FreeBSD/RISC-V to the Go Project, and 2. begin fuzzing system calls under FreeBSD/RISC-V.\n\nSyzkaller is flexible enough to target OS interfaces other than system calls. For example, it has been used to fuzz the Linux USB stack and has found dozens of bugs in the USB subsystem alone. The details are complicated, but the idea is simple.\n\n1. Generate a program, which invokes one or more system calls.\n2. Run the generated program.\n3. Check to see if the system diagnosed an error.\n  - If not, collect kernel code coverage information and decide whether to try iterating upon the previous test program, or start anew.\n  - If so, collect information about the crash and try to discover a minimal test case that triggers the crash.\n\nThe mentors will have weekly meetings with the mentee to discuss progress and strategies for overcoming any challenges.	{}	2022	Term 1	https://github.com/google/syzkaller	https://wiki.freebsd.org/riscv	310000	171
402	2d76dbe6-43eb-465e-a852-64b2e48f2c68	CNCF - Kubernetes: CAPA Reimagining how we handle AWS account preparation	{"Description: Cluster API Provider AWS (CAPA) can create and manage the lifecycle of Kubernetes clusters in AWS (with the help of Cluster API in general). For each target AWS account where a user wants to create clusters it must be prepared for usage first. This is currently done using [clusterawsadm](https:","",cluster-api-aws.sigs.k8s.io,topics,"using-clusterawsadm-to-fulfill-prerequisites.html) which creates","updates a CloudFormation stack that in turn creates","updates IAM resources. This approach has caused issues as CloudFormation is region specific but IAM is global and users often run the tool in different regions which results in failed stacks that cannot easily be deleted. As a project we want to move away from using CloudFormation and instead use API calls (like the rest of CAPA). We also want to make the process idempotent so it doesn't matter if you run it against different regions. This account preparation is key to CAPA and with out it CAPA cannot run.\n\nExpected Outcome: A new approach to handling the prerequisites required for CAPA. We need to continue to support the cli based approach (so clusterawsadm will be updated) but we can also explore a declarative approach with an operator."}	Description: Cluster API Provider AWS (CAPA) can create and manage the lifecycle of Kubernetes clusters in AWS (with the help of Cluster API in general). For each target AWS account where a user wants to create clusters it must be prepared for usage first. This is currently done using [clusterawsadm](https://cluster-api-aws.sigs.k8s.io/topics/using-clusterawsadm-to-fulfill-prerequisites.html) which creates/updates a CloudFormation stack that in turn creates/updates IAM resources. This approach has caused issues as CloudFormation is region specific but IAM is global and users often run the tool in different regions which results in failed stacks that cannot easily be deleted. As a project we want to move away from using CloudFormation and instead use API calls (like the rest of CAPA). We also want to make the process idempotent so it doesn't matter if you run it against different regions. This account preparation is key to CAPA and with out it CAPA cannot run.\n\nExpected Outcome: A new approach to handling the prerequisites required for CAPA. We need to continue to support the cli based approach (so clusterawsadm will be updated) but we can also explore a declarative approach with an operator.	{go,kubernetes}	2023	Term 1	https://github.com/kubernetes-sigs/cluster-api-provider-aws/issues/3715	http://kubernetes.io/	300000	75
293	1db3f29c-30cb-4018-82c9-63b135caa6d5	CNCF - Kubernetes SIG ContribEx: Improvements to Kubernetes maintainers-related automation	{"Description: Kubernetes uses OWNERS files to delegate responsibility over different parts of the codebase. These files are also used in the code review process. Unfortunately, over time, there are lots of OWNERS files which have languished and have stale information. Since the velocity of a project is also determined by the number of people reviewing code, it is essential to keep the OWNERS files up-to-date. To ensure this, the maintainers project was created. This internship involves improving maintainers through adding new features and integrating the tool in suitable automation so that it is actively used by the community to signal out-of-date OWNERS files. A stretch goal would also be to improve similar automation tools used to handle GitHub membership for the community."}	Description: Kubernetes uses OWNERS files to delegate responsibility over different parts of the codebase. These files are also used in the code review process. Unfortunately, over time, there are lots of OWNERS files which have languished and have stale information. Since the velocity of a project is also determined by the number of people reviewing code, it is essential to keep the OWNERS files up-to-date. To ensure this, the maintainers project was created. This internship involves improving maintainers through adding new features and integrating the tool in suitable automation so that it is actively used by the community to signal out-of-date OWNERS files. A stretch goal would also be to improve similar automation tools used to handle GitHub membership for the community.	{go}	2022	Term 1	https://github.com/kubernetes/org/issues/3208	kubernetes.io	300000	75
355	4d633e10-48bd-4834-891c-0e076eb79a18	Feature optimizations for RISCV-CTG and RISCV-ISAC	{"ISAC & CTG are essential components of the RISC-V Architectural Testing(AT) ecosystem. CTG generates tests for AT using the coverpoints as stimulus to ensure that complete coverage can be achieved with minimal testing. ISAC measures the coverage of the tests and reports on the quality of the test suite. This mentorship involves adding multiple features to both these tools to improve the quality of the architectural test suite.\n\nFor further information on the tasks to be done during your internship refer here - https:","",gist.github.com,pawks,"e2dec6d67224a9f68bf96fc909398c45\n\nAs part of the selection process, all candidates are required to successfully complete the coding challenge described here before August 3rd : https:","",gist.github.com,pawks,98863e5eca71c8b80b18e060cb4d9558}	ISAC & CTG are essential components of the RISC-V Architectural Testing(AT) ecosystem. CTG generates tests for AT using the coverpoints as stimulus to ensure that complete coverage can be achieved with minimal testing. ISAC measures the coverage of the tests and reports on the quality of the test suite. This mentorship involves adding multiple features to both these tools to improve the quality of the architectural test suite.\n\nFor further information on the tasks to be done during your internship refer here - https://gist.github.com/pawks/e2dec6d67224a9f68bf96fc909398c45\n\nAs part of the selection process, all candidates are required to successfully complete the coding challenge described here before August 3rd : https://gist.github.com/pawks/98863e5eca71c8b80b18e060cb4d9558	{}	2022	Term 3	https://github.com/riscv-software-src/riscv-isac/	https://github.com/riscv-software-src/riscv-ctg/	0	171
387	df2ae723-8c13-4aaf-9dbb-0782e7923a45	RT-Smart Open Source Micro-kernel Operating System Running on RISCV64 Platform	{"(1) Port RT-Smart open source micro-kernel operating system to Allwinner D1 development board(RISC-V Architecture), deepen the study of RISCV64 processor architecture, boot process and debugging approaches.\n(2) Research on device driver development, component development, system device framework development, software component development, protocol stack, file system, device tree and some others. \n(3) GUI Application: port open source LVGL, create a practical product based solution, with network, file system, GUI display, Internet of Things applications. \n(4) Put this project into mass production."}	(1) Port RT-Smart open source micro-kernel operating system to Allwinner D1 development board(RISC-V Architecture), deepen the study of RISCV64 processor architecture, boot process and debugging approaches.\n(2) Research on device driver development, component development, system device framework development, software component development, protocol stack, file system, device tree and some others. \n(3) GUI Application: port open source LVGL, create a practical product based solution, with network, file system, GUI display, Internet of Things applications. \n(4) Put this project into mass production.	{}	2022	Term 3	https://github.com/zhangsz0516/rt-smart_riscv64	https://github.com/RT-Thread/rt-thread/tree/rt-smart	0	171
573	21804123-5f2d-4935-86ae-db4ddb61ea98	Converting sample bpf programs to kselftests	{"bpf samples are rotting so they should be converted to self tests that can be run through CI.\n\nFunding: Will be Provided by eBPF Foundation"}	bpf samples are rotting so they should be converted to self tests that can be run through CI.\n\nFunding: Will be Provided by eBPF Foundation	{ebpf}	2023	Term 3	https://github.com/ebpffoundation	https://ebpf.foundation	0	171
106	f2c89a34-d1cb-4ed0-ace2-cd959617ed14	Linux Kernel Add PR_SET_VMA support to Linux Kernel	{"PR_SET_VMA mechanism is used heavily in Android for naming memory regions. It is passed to the prctl(2) system call along with the range of addresses to be named. The name appears in ",proc,pid,"maps and can be used to identify memory regions for debugging and accounting purposes. The patch has been carried in the Android tree for several years. The patch needs to be adapted to mainline, improved and finally upstreamed. This project will give the mentee a good understanding of virtual memory area structures in the Linux kernel as the name for the memory region is stored in these structures and there are various cases where a memory map needs to be split or merged depending on its name compared to its immediate neighbors."}	PR_SET_VMA mechanism is used heavily in Android for naming memory regions. It is passed to the prctl(2) system call along with the range of addresses to be named. The name appears in /proc/pid/maps and can be used to identify memory regions for debugging and accounting purposes. The patch has been carried in the Android tree for several years. The patch needs to be adapted to mainline, improved and finally upstreamed. This project will give the mentee a good understanding of virtual memory area structures in the Linux kernel as the name for the memory region is stored in these structures and there are various cases where a memory map needs to be split or merged depending on its name compared to its immediate neighbors.	{c,shell}	2020	Term 3	https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/	https://wiki.linuxfoundation.org/lkmp	0	21
107	6b275fcc-2d40-421c-a1ff-0b229f25fd21	Linux Kernel trace events in pstore	{"Pstore is a subsystem in the Linux kernel that is used to store kernel logs which can retrieved after a machine warm-reboots.\n\nCurrently, during ftrace tracing, ftrace events in the kernel are lost when a machine hard-locks up. There is a need for writing these events into the pstore so that after a lock up, the trace events that led to the crash can be retrieved from the Pstore after a warm reboot. This has to be done in a scalable way so that tracing a live system does not impact performance of the system – ftrace is extremely fast. Initial patches posted to write and retrieve events to","from pstore are not scalable due to locking and other issues. Introducing a scalable approach will open up more possibilities for crash analysis. Prototype work for reference:\n\nhttps:","",linaroconnectsandiego.sched.com,event,Sue7,event-tracing-and-pstore-with-a-pinch-of-dynamic-debug}	Pstore is a subsystem in the Linux kernel that is used to store kernel logs which can retrieved after a machine warm-reboots.\n\nCurrently, during ftrace tracing, ftrace events in the kernel are lost when a machine hard-locks up. There is a need for writing these events into the pstore so that after a lock up, the trace events that led to the crash can be retrieved from the Pstore after a warm reboot. This has to be done in a scalable way so that tracing a live system does not impact performance of the system – ftrace is extremely fast. Initial patches posted to write and retrieve events to/from pstore are not scalable due to locking and other issues. Introducing a scalable approach will open up more possibilities for crash analysis. Prototype work for reference:\n\nhttps://linaroconnectsandiego.sched.com/event/Sue7/event-tracing-and-pstore-with-a-pinch-of-dynamic-debug	{c,shell}	2020	Term 3	https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/	https://wiki.linuxfoundation.org/lkmp	0	21
331	b97a815b-6900-4a3e-a4af-7460551f933c	CNCF - OpenELB: Support BGP policy in OpenELB	{"OpenELB is an open-source load balancer implementation designed for exposing the LoadBalancer type of Kubernetes services in bare metal, edge, and virtualization environments. Currently, OpenELB supports the BGP protocol. However, the BGP policy is not fully supported in OpenELB. Therefore, based on the BGP protocol, OpenELB is supposed to support the BGP policy to enable leveraging the GoBGP policy feature for controlling the route advertisement."}	OpenELB is an open-source load balancer implementation designed for exposing the LoadBalancer type of Kubernetes services in bare metal, edge, and virtualization environments. Currently, OpenELB supports the BGP protocol. However, the BGP policy is not fully supported in OpenELB. Therefore, based on the BGP protocol, OpenELB is supposed to support the BGP policy to enable leveraging the GoBGP policy feature for controlling the route advertisement.	{go,kubernetes,helm,docker}	2022	Term 2	https://github.com/openelb/openelb/issues/267	https://openelb.github.io/	300000	136
333	0c5cecb1-3de8-435a-acdf-7adfcbd759d3	CNCF - OpenFunction: Support and update the Python Functions Framework	{"OpenFunction is a cloud-native open source FaaS (Function as a Service) platform. OpenFunction 0.6.0 brings notable features including function plugin, distributed tracing for functions, control autoscaling behavior, HTTP trigger to async function, etc. Meanwhile, the asynchronous runtime definition has also been refactored. The core API has been upgraded from v1alpha1 to v1beta1. So far, the Go Function Framework fully supports the latest features of OpenFunction 0.6.0. We hope the Python Functions Framework could also be applicable in OpenFunction 0.6.0."}	OpenFunction is a cloud-native open source FaaS (Function as a Service) platform. OpenFunction 0.6.0 brings notable features including function plugin, distributed tracing for functions, control autoscaling behavior, HTTP trigger to async function, etc. Meanwhile, the asynchronous runtime definition has also been refactored. The core API has been upgraded from v1alpha1 to v1beta1. So far, the Go Function Framework fully supports the latest features of OpenFunction 0.6.0. We hope the Python Functions Framework could also be applicable in OpenFunction 0.6.0.	{python,kubernetes,openfunction}	2022	Term 2	https://github.com/OpenFunction/functions-framework/issues/18	https://openfunction.dev/	300000	137
160	7a0a21ed-feb9-4dd4-b315-04ee20c0f4a4	OpenHPC Mentorship	{"OpenHPC is accepting applications for a new mentorship program that aims to give  students practical experience contributing to an open-source project and an opportunity to work with volunteers from the project’s Technical Steering Committee (TSC). OpenHPC is a software stack that aggregates a variety of software components ranging from administrative tools like bare metal provisioning and resource management to end-user development that span a range of scientific","numerical uses. If your interest is in the domain of high performance computing , scientific computing or cluster management, this might be the right program for you. This initial program will occur during the Fall of 2020 with future versions expected to occur during the summer. See our website for details and project ideas - http:","",bit.ly,"ohcmentorship2020  . For questions feel free to email - mentorship@openhpc.community"}	OpenHPC is accepting applications for a new mentorship program that aims to give  students practical experience contributing to an open-source project and an opportunity to work with volunteers from the project’s Technical Steering Committee (TSC). OpenHPC is a software stack that aggregates a variety of software components ranging from administrative tools like bare metal provisioning and resource management to end-user development that span a range of scientific/numerical uses. If your interest is in the domain of high performance computing , scientific computing or cluster management, this might be the right program for you. This initial program will occur during the Fall of 2020 with future versions expected to occur during the summer. See our website for details and project ideas - http://bit.ly/ohcmentorship2020  . For questions feel free to email - mentorship@openhpc.community	{git,bash,latex}	2020	Term 3	https://github.com/openhpc/ohpc	https://github.com/openhpc/ohpc/wiki/Mentorship-Program-Call-for-Participation---Fall-2020	1600000	4
427	0071e2ff-f538-4817-978b-07b267cfcd6a	CNCF - Cortex: Automated nightly benchmarks	{"- Description: In order to make sure Cortex doesn’t introduce performance regressions across releases and major changes, we would like to introduce an automated way to run some nightly macro","micro benchmarks for Cortex clusters. This project could potentially involve setting up Kubernetes clusters, Cortex components, and load generators. We’d love to keep track of performance metrics for each test run and visualize them through a UI.\n- Expected Outcome: An automated workflow that runs performance macro","micro benchmarks everyday or on demand and performance metrics can be visualized through a UI."}	- Description: In order to make sure Cortex doesn’t introduce performance regressions across releases and major changes, we would like to introduce an automated way to run some nightly macro/micro benchmarks for Cortex clusters. This project could potentially involve setting up Kubernetes clusters, Cortex components, and load generators. We’d love to keep track of performance metrics for each test run and visualize them through a UI.\n- Expected Outcome: An automated workflow that runs performance macro/micro benchmarks everyday or on demand and performance metrics can be visualized through a UI.	{go,kubernetes}	2023	Term 1	https://github.com/cortexproject/cortex/issues/5107	https://cortexmetrics.io/	600000	57
290	b11c375b-2a70-4f23-8d04-c456cec5df4e	Discovering Linux kernel subsystems used by OpenAPS	{"OpenAPS is an open source Artificial Pancreas System designed to automatically adjust an insulin pump’s insulin delivery to keep Blood Glucose in a safe range at all times.\nIt is an open and transparent effort to make safe and effective basic Automatic Pancreas System technology widely available to anyone with compatible medical devices who is willing to build their own system.\n\nWhat happens when an OpenAPS workload runs on Linux? What are the subsystems and modules  that are in active use when OpenAPS is running? What are the interactions between OpenAPS and the kernel when a user checks how much insulin is left in the insulin pump?\n\nELISA Medical Devices WG set out to answer these questions. Understanding the kernel footprint necessary to run a workload helps us focus on the  subsystem and modules that make up the footprint for safety.\n\nThe mentee learn to use Linux kernel tracing and strace tool to discover Linux kernel subsystems used by OpenAPS.  Mentee will learn  how to find Linux system calls supported on various architectures. Mentee will write a blog","whitepaper on the findings which will aid ELISA Medical Devices WG to focus on the  subsystem and modules that make up the footprint for safety."}	OpenAPS is an open source Artificial Pancreas System designed to automatically adjust an insulin pump’s insulin delivery to keep Blood Glucose in a safe range at all times.\nIt is an open and transparent effort to make safe and effective basic Automatic Pancreas System technology widely available to anyone with compatible medical devices who is willing to build their own system.\n\nWhat happens when an OpenAPS workload runs on Linux? What are the subsystems and modules  that are in active use when OpenAPS is running? What are the interactions between OpenAPS and the kernel when a user checks how much insulin is left in the insulin pump?\n\nELISA Medical Devices WG set out to answer these questions. Understanding the kernel footprint necessary to run a workload helps us focus on the  subsystem and modules that make up the footprint for safety.\n\nThe mentee learn to use Linux kernel tracing and strace tool to discover Linux kernel subsystems used by OpenAPS.  Mentee will learn  how to find Linux system calls supported on various architectures. Mentee will write a blog/whitepaper on the findings which will aid ELISA Medical Devices WG to focus on the  subsystem and modules that make up the footprint for safety.	{shell}	2022	Term 1	https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/	https://openaps.org/	300000	21
267	3d219bc9-0d8f-4ca3-b2fb-9058aad4067d	CNCF - WasmEdge: Enable OpenVINO backend for WASI-NN	{"Description: WasmEdge is a lightweight, high-performance, and extensible WebAssembly runtime for cloud native, edge, and decentralized applications. WasmEdge is an official sandbox project hosted by the CNCF. WasmEdge has implemented some features of WASI-NN. However, the backend is using ONNX. In this ticket, we would like to have both ONNX and OpenVINO backend."}	Description: WasmEdge is a lightweight, high-performance, and extensible WebAssembly runtime for cloud native, edge, and decentralized applications. WasmEdge is an official sandbox project hosted by the CNCF. WasmEdge has implemented some features of WASI-NN. However, the backend is using ONNX. In this ticket, we would like to have both ONNX and OpenVINO backend.	{rust}	2022	Term 1	https://github.com/WasmEdge/WasmEdge/issues/1063	https://wasmedge.org/	300000	31
549	f96ed478-6a50-4cd7-8a4d-c0f7bddfe2e1	Open Mainframe- Creating a ZSS backend provider for Zowe CLI	{"The Zowe CLI is a program that runs on PCs for users to interact with a z","OS mainframe remotely. It can be used directly, for automation such as CICD, or to enable popular but otherwise z","OS-unaware programs such as editors and IDEs. The CLI accesses a mainframe via network APIs present on that mainframe, such as webserver REST APIs. Often, such APIs will be from non-Zowe dependencies, but in this project you will be working within the Zowe community to enhance the CLI so that it can use Zowe's own ZSS webserver as a source for z","OS data.\n\nThis project will at a minimum involve programming to utilize existing HTTP network APIs to fulfill a nodeJS interface for each type of data possible to use in the CLI.\n\nAs the project progresses, you may also get the opportunity to add new HTTP APIs to the ZSS server, which is a z","OS native C webserver. You may also get to reuse the code written for the CLI to similarly enhance Zowe's addon to Microsoft's Visual Studio Code, the Zowe Explorer, to enable VSCode to edit files and datasets on z","OS using an all-Zowe codebase."}	The Zowe CLI is a program that runs on PCs for users to interact with a z/OS mainframe remotely. It can be used directly, for automation such as CICD, or to enable popular but otherwise z/OS-unaware programs such as editors and IDEs. The CLI accesses a mainframe via network APIs present on that mainframe, such as webserver REST APIs. Often, such APIs will be from non-Zowe dependencies, but in this project you will be working within the Zowe community to enhance the CLI so that it can use Zowe's own ZSS webserver as a source for z/OS data.\n\nThis project will at a minimum involve programming to utilize existing HTTP network APIs to fulfill a nodeJS interface for each type of data possible to use in the CLI.\n\nAs the project progresses, you may also get the opportunity to add new HTTP APIs to the ZSS server, which is a z/OS native C webserver. You may also get to reuse the code written for the CLI to similarly enhance Zowe's addon to Microsoft's Visual Studio Code, the Zowe Explorer, to enable VSCode to edit files and datasets on z/OS using an all-Zowe codebase.	{c,typescript}	2023	Term 2	https://github.com/zowe/zss		600000	38
438	fe5c060e-420b-4c0f-90ae-389d893c50b6	CNCF - Cortex: Switch Cortex Ruler to query Query Frontend	{"Description: Cortex Ruler queries ingester directly for rule evaluation. This is okay but if Cortex Ruler could query Query Frontend instead for rule evaluation, it can benefit from more features in the Query Frontend like vertical sharding. This also simplifies the Cortex ruler to not embed a querier and uses less resources. For this project, we would like to switch Cortex Ruler to query Query Frontend. You are expected to work with a microservice architecture and write unit tests and end to end tests to make sure the feature works correctly.\n\nExpected Outcome: Cortex Ruler talks to Query Frontend for rules evaluation."}	Description: Cortex Ruler queries ingester directly for rule evaluation. This is okay but if Cortex Ruler could query Query Frontend instead for rule evaluation, it can benefit from more features in the Query Frontend like vertical sharding. This also simplifies the Cortex ruler to not embed a querier and uses less resources. For this project, we would like to switch Cortex Ruler to query Query Frontend. You are expected to work with a microservice architecture and write unit tests and end to end tests to make sure the feature works correctly.\n\nExpected Outcome: Cortex Ruler talks to Query Frontend for rules evaluation.	{go}	2023	Term 1	https://github.com/cortexproject/cortex/issues/5105	https://cortexmetrics.io/	300000	57
139	54adaade-8537-4150-b4ea-988454615ed7	CNCF - Keptn: Improve Prometheus integration and exposure of Prometheus metrics	{"In the current implementation, the Prometheus integration in Keptn lacks customizability and configuration options. Also, Keptn core services should be instrumented to expose Prometheus metrics. The goal of this project is to refactor or rewrite the integration and add Prometheus to Keptn core services."}	In the current implementation, the Prometheus integration in Keptn lacks customizability and configuration options. Also, Keptn core services should be instrumented to expose Prometheus metrics. The goal of this project is to refactor or rewrite the integration and add Prometheus to Keptn core services.	{go,prometheus}	2021	Term 1	https://github.com/keptn-contrib/prometheus-service/issues/53	https://keptn.sh/	300000	73
219	927caaff-2278-4c56-92f5-4b22d0e2c24f	CNCF - OpenEBS: A Kubernetes operator to remove stale PVs of failed statefulset replicas	{"When using StatefulSets with Local Volumes on Ephemeral Nodes - the StatefulSet Pods will get stuck in pending state when the Node is taken out of the cluster. In such cases, manual steps are required to verify that Node is out of the cluster, remove the PV and PVC and delete the failed StatefulSet replica to trigger the re-creation of a new replica with a new PVC","PV on a new node. This enhancement request is to build a generic operator that is driven by a configuration to automate the manual steps."}	When using StatefulSets with Local Volumes on Ephemeral Nodes - the StatefulSet Pods will get stuck in pending state when the Node is taken out of the cluster. In such cases, manual steps are required to verify that Node is out of the cluster, remove the PV and PVC and delete the failed StatefulSet replica to trigger the re-creation of a new replica with a new PVC/PV on a new node. This enhancement request is to build a generic operator that is driven by a configuration to automate the manual steps.	{go,kubernetes}	2021	Term 3	https://github.com/openebs/dynamic-localpv-provisioner/issues/87	https://openebs.io/	300000	46
194	7cd6af42-6f86-4a77-a077-304dc7d82134	CNCF - OpenEBS: Enforcing XFS quotas on OpenEBS hostpath Local PV	{"OpenEBS Local PV hostpath is the most simple to use Local PV option available for Kubernetes today. Many of the applications use XFS filesystem to create Local PVs. This project is to implement XFS project quota on the OpenEBS Local PV subdirectory to restrict pods from exceeding the Quota assigned to them via the PVC request."}	OpenEBS Local PV hostpath is the most simple to use Local PV option available for Kubernetes today. Many of the applications use XFS filesystem to create Local PVs. This project is to implement XFS project quota on the OpenEBS Local PV subdirectory to restrict pods from exceeding the Quota assigned to them via the PVC request.	{go,testing}	2021	Term 2	https://github.com/openebs/dynamic-localpv-provisioner/issues/13	https://openebs.io/	0	46
313	a8b8000d-e30f-4a7e-ab48-210ddde3a1a2	Open Mainframe- Software Discovery Tool Back-end and Infrastructure Enhancements	{"The Software Discovery Tool is a web application written in Python Flask for searching for open source software related to the IBM Z "," s390x hardware architecture. Our mentee will work on improvements to the Python Flask back-end for more reliable generation of data sources on lower resource systems and improvements to reliability of the production deployment on the Linux environment where it runs. Finally, we're always improving our data sources, so this mentorship will introduce the mentee to the world of open source software on the mainframe, and provide a detailed introduction to the communities, organizations, and companies involved."}	The Software Discovery Tool is a web application written in Python Flask for searching for open source software related to the IBM Z / s390x hardware architecture. Our mentee will work on improvements to the Python Flask back-end for more reliable generation of data sources on lower resource systems and improvements to reliability of the production deployment on the Linux environment where it runs. Finally, we're always improving our data sources, so this mentorship will introduce the mentee to the world of open source software on the mainframe, and provide a detailed introduction to the communities, organizations, and companies involved.	{mainframe}	2022	Term 1	https://github.com/openmainframeproject/software-discovery-tool		300000	38
317	3a3d59d9-a6bc-457f-bcc1-97fe919fdbd0	Open Mainframe- COBOL Programming Course Enhancements	{"The COBOL Programming course has seen huge success in course adoption. The summer mentee will go through the COBOL programming course to learn COBOL and help the project:\n\n1. Update current course with ZOWE V2\n2. Edit Getting Started chapter into two - removing setup instructions into an appendix or a separate ‘Setup’ chapter\n3. Add Processing JSON and XML to Advanced chapter\n4. Create a new chapter about CI","CD and Unit Testing with COBOL Check\n         - Investigate how to incorporate COBOL Check into the course\n5. Create a Gitbook of all the updated, new COBOL course content"}	The COBOL Programming course has seen huge success in course adoption. The summer mentee will go through the COBOL programming course to learn COBOL and help the project:\n\n1. Update current course with ZOWE V2\n2. Edit Getting Started chapter into two - removing setup instructions into an appendix or a separate ‘Setup’ chapter\n3. Add Processing JSON and XML to Advanced chapter\n4. Create a new chapter about CI/CD and Unit Testing with COBOL Check\n         - Investigate how to incorporate COBOL Check into the course\n5. Create a Gitbook of all the updated, new COBOL course content	{cobol,programming}	2022	Term 2	https://github.com/openmainframeproject/cobol-programming-course		300000	38
312	c3d3974c-b3a8-4678-990d-d8b4e4e14420	Open Mainframe- IBM Telum AI & ONNX-MLIR Python Toolkit	{"The IBM Telum AI accelerator will be exploited by open source packages such as ONNX and ONNX-MLIR. This project will be to explore and create a python toolkit to simplify model conversion to ONNX and provide python APIs to use the ONNX-MLIR model compiler. Additional capabilities can be explored if time allows. This would be a new project."}	The IBM Telum AI accelerator will be exploited by open source packages such as ONNX and ONNX-MLIR. This project will be to explore and create a python toolkit to simplify model conversion to ONNX and provide python APIs to use the ONNX-MLIR model compiler. Additional capabilities can be explored if time allows. This would be a new project.	{python}	2022	Term 2	https://github.com/openmainframeproject-internship		600000	38
316	ef621693-7645-4d6e-b6e5-f6a1e8bef5ad	Open Mainframe- Portability test of MLOps for s390x	{"MLOps is defining new way of improving many businesses that utilizing machine learning and AI. There are many open tools out there helping data scientists, cloud engineers and application developers to accelerate end-to-end process of AI and Machine Learning - but very limited yet to s390x platform. In this project, the mentee will help identify components and tasks that will require to port certain popular MLOps tools (such as Ray) in open source space and also help estimate the work that will be involved to port to s390x architecture."}	MLOps is defining new way of improving many businesses that utilizing machine learning and AI. There are many open tools out there helping data scientists, cloud engineers and application developers to accelerate end-to-end process of AI and Machine Learning - but very limited yet to s390x platform. In this project, the mentee will help identify components and tasks that will require to port certain popular MLOps tools (such as Ray) in open source space and also help estimate the work that will be involved to port to s390x architecture.	{ai}	2022	Term 2	github.com/openmainframeproject/omp-education		300000	38
351	f5b4f631-934d-4cbc-aa09-c258a6735744	Open Mainframe- Secure Transcript Ledger	{"Develop a blockchain capable of keeping a record of student grades. It will allow input from designated users (Instructors). Each block will contain a course name and ID, a grade, and a student ID (SID). Each SID will be linked with the student's personal information, and this record could also be shared utilizing a smart contract; reducing the risk of access by unauthorized third parties to personal data. With the SID, institutions or individuals will have access to review a complete transcript.\n\nThe successful development of this blockchain will provide a secure platform that improves the application process and times for school transfers, jobs, and scholarships."}	Develop a blockchain capable of keeping a record of student grades. It will allow input from designated users (Instructors). Each block will contain a course name and ID, a grade, and a student ID (SID). Each SID will be linked with the student's personal information, and this record could also be shared utilizing a smart contract; reducing the risk of access by unauthorized third parties to personal data. With the SID, institutions or individuals will have access to review a complete transcript.\n\nThe successful development of this blockchain will provide a secure platform that improves the application process and times for school transfers, jobs, and scholarships.	{blockchain}	2022	Term 2	https://github.com/openmainframeproject-internship		300000	38
314	64785cea-0131-43a3-b6fb-395831d6881a	Open Mainframe- Zowe Interface Scripting	{"Use Zowe CLI to replace a section of a test framework for GenevaERS.  This framework uses Java Code to call FTP to interact with the mainframe.  We want to replace the FTP with Zowe CLI."}	Use Zowe CLI to replace a section of a test framework for GenevaERS.  This framework uses Java Code to call FTP to interact with the mainframe.  We want to replace the FTP with Zowe CLI.	{mainframe,developer}	2022	Term 1	https://github.com/genevaers/community		0	38
511	dcf9b5fd-a454-44de-a0cc-0949619fd7e2	Open Mainframe- Zowe Python SDK Enhancements	{"Finalize team configuration support and make other enhancements to support a v2 release."}	Finalize team configuration support and make other enhancements to support a v2 release.	{python}	2023	Term 2	https://github.com/zowe/zowe-client-python-sdk		600000	38
554	bc81d628-2bb1-42a2-be66-53da1fd9b961	Open Mainframe- Zowe Security Workgroup CVE Publication Automation	{"Mentee will help prepare pipelines in the GitHub Actions to automate processing and publication of found CVEs together with managing the CVEs."}	Mentee will help prepare pipelines in the GitHub Actions to automate processing and publication of found CVEs together with managing the CVEs.	{}	2023	Term 2	https://github.com/zowe/security-reports/		300000	38
448	632ab03f-a970-44ce-b451-fac0a7349f71	CNCF - TAG Contributor Strategy: Mentoring Workspaces II	{"pair.sharing.io is a mentoring "," pair environment used by ii.nz that brings up clusters to co-learn and co-author via tmate+emacs and a live cluster with many features useful to cloud native development. However, while many folks find the ideas useful, it would be good to reach a wider audience by bringing up workspaces w"," VSCode as an alternative to emacs. The request is for a PoC deploying coder.com to CNCF Infrastructure (likely Packet) and bringing over some of the methods of collaboration learned by ii on pair to a wider audience."}	pair.sharing.io is a mentoring / pair environment used by ii.nz that brings up clusters to co-learn and co-author via tmate+emacs and a live cluster with many features useful to cloud native development. However, while many folks find the ideas useful, it would be good to reach a wider audience by bringing up workspaces w/ VSCode as an alternative to emacs. The request is for a PoC deploying coder.com to CNCF Infrastructure (likely Packet) and bringing over some of the methods of collaboration learned by ii on pair to a wider audience.	{shell,terminal,kubernetes}	2023	Term 1	https://github.com/sharingio/pair/issues/173	https://pair.sharing.io/	600000	160
548	97cc38e4-83d9-4f04-842e-86ca2328c8a7	Open Mainframe- COBOL Programming Course - COBOL Check Integration	{"The goal of this summer mentorship is to create a chapter in the COBOL Programming course that focuses on unit testing","debugging COBOL code. The concept of Test Driven Development incorporating the COBOL Check project into the course. This will ensure that we are able to provide our learners with the end to end experience of learning a programming language and the concept of code & test."}	The goal of this summer mentorship is to create a chapter in the COBOL Programming course that focuses on unit testing/debugging COBOL code. The concept of Test Driven Development incorporating the COBOL Check project into the course. This will ensure that we are able to provide our learners with the end to end experience of learning a programming language and the concept of code & test.	{cobol}	2023	Term 2	https://github.com/openmainframeproject/cobol-programming-course		300000	38
546	aca35651-9307-4299-a357-12c3a6d4a468	Open Mainframe- Unix System Services for Zowe Client Java SDK	{"Help complete USS Rest Api wrapper calls for the SDK."}	Help complete USS Rest Api wrapper calls for the SDK.	{java}	2023	Term 2	https://github.com/zowe/zowe-client-java-sdk		600000	38
353	4ec0a092-eb06-48d6-b2db-b3bd8254cf80	Open Mainframe- Zowe CLI Enhancements	{"Work alongside the Zowe CLI squad to implement highly upvoted community enhancements for Zowe CLI! These enhancements can be viewed here - https:","",github.com,zowe,zowe-cli,"issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc. This project will initially focus primarily on enhancements to our popular jobs and files command groups with potential to expand to other areas if time permits. The Zowe CLI has been instrumental in enabling popular distributed tooling to integrate the mainframe platform."}	Work alongside the Zowe CLI squad to implement highly upvoted community enhancements for Zowe CLI! These enhancements can be viewed here - https://github.com/zowe/zowe-cli/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc. This project will initially focus primarily on enhancements to our popular jobs and files command groups with potential to expand to other areas if time permits. The Zowe CLI has been instrumental in enabling popular distributed tooling to integrate the mainframe platform.	{mainframe,developer}	2022	Term 1	https://github.com/zowe/zowe-cli		300000	38
89	00625dae-e9dd-447f-9c97-4a9c7cb7d429	Open Mainframe Project 2020 Mentorship - ADE - Add additional log support	{"ADE - Add additional log support"}	ADE - Add additional log support	{}	2020	Term 2	https://github.com/openmainframeproject-internship/ADE---Add-additional-log-support	https://www.openmainframeproject.org/projects/mentorship-program	600000	2
4	7671f900-a622-4bbc-956b-f4c336704478	Linux kernel Bug Fixing Spring 2021	{"There are many bugs being found in the Linux kernel by automated tools, yet not many people available to help fix these.  This project will help the applicants choose bugs to fix, work through the problem found, and help get the issue resolved in the Linux kernel by producing a fix or working with the community on a fix.\n\nAfter the end of the project, the applicant will have a much deeper knowledge of many different areas of the Linux kernel and hopefully, many bugs fixed and many patches accepted."}	There are many bugs being found in the Linux kernel by automated tools, yet not many people available to help fix these.  This project will help the applicants choose bugs to fix, work through the problem found, and help get the issue resolved in the Linux kernel by producing a fix or working with the community on a fix.\n\nAfter the end of the project, the applicant will have a much deeper knowledge of many different areas of the Linux kernel and hopefully, many bugs fixed and many patches accepted.	{c,linux,shell}	2021	Term 1	https://git.kernel.org	https://kernel.org	3973500	116
88	13ffdfb6-cb76-4ff0-8ac1-c27977b8cb56	Open Mainframe Project 2020 Mentorship - Enabling IBM Z in MLModelScope	{"MLModelScope (https:","",mlmodelscope.org,") is a playground that allows people to try ML tasks with various model types, frameworks and hardware backends. It is jointly developed by IBM and University of Illinois. It is currently one of the platforms that collaborate with MLPerf for ML benchmark workload validation. (IBM participates in MLPerf as a founding members, including contribution from z). MLModelScope enables multiple backends for model training, execution and performance study. Currently IBM Z is enabled."}	MLModelScope (https://mlmodelscope.org/) is a playground that allows people to try ML tasks with various model types, frameworks and hardware backends. It is jointly developed by IBM and University of Illinois. It is currently one of the platforms that collaborate with MLPerf for ML benchmark workload validation. (IBM participates in MLPerf as a founding members, including contribution from z). MLModelScope enables multiple backends for model training, execution and performance study. Currently IBM Z is enabled.	{java,javascript,devops,development}	2019	Term 2	https://github.com/openmainframeproject-internship/Enabling-IBM-Z-in-MLModelScope	https://www.openmainframeproject.org/projects/mentorship-program	600000	2
2	a8eddf2a-2b97-447a-90cc-7e8db0dab269	Open Mainframe Project 2020 Mentorship - InZpect	{"InZpect is next generation dump viewer and inspector that is something of a successor to IPCS. There is a lot of work to do in UI and testing. Plus almost any work on the internals requires lots of knowledge of the operating system. SVCDumps are not highly structured, so the relationships of data and state of the computation must be synthesized from the memory blocks in the dump, primarily. The server is written in java, with some back-end code coming from extensions","usage of the ZSS in Zowe. The client code is in Javascript using React. The tooling is very minimal currently, as is the number of third-party libraries."}	InZpect is next generation dump viewer and inspector that is something of a successor to IPCS. There is a lot of work to do in UI and testing. Plus almost any work on the internals requires lots of knowledge of the operating system. SVCDumps are not highly structured, so the relationships of data and state of the computation must be synthesized from the memory blocks in the dump, primarily. The server is written in java, with some back-end code coming from extensions/usage of the ZSS in Zowe. The client code is in Javascript using React. The tooling is very minimal currently, as is the number of third-party libraries.	{javascript}	2020	Term 2	https://github.com/openmainframeproject-internship/InZpect	https://www.openmainframeproject.org/projects/mentorship-program	600000	2
13	79ce0bff-4a7c-4377-8d14-b0a347346f9c	Hyperledger - Multiple Data Integration to Fabric Climate Accounting Network	{"The Hyperledger Labs blockchain-carbon-accounting project includes a Hyperledger Fabric network for recording the carbon and Greenhouse Gas (GHG) emissions that cause climate change.  Since there are many activities that cause such emissions, the network is designed to accept data from multiple sources of measurements.  In this project, we will demonstrate integrations from measurement sources with blockchain networks by integrating the ThoughtWorks cloud computing emissions calculator, the NREL OpenPath mobile application, and other web- and mobile-based API's sources to turn instrumented readings into emissions measurements.   It will leverage previous projects involving Hyperledger Cactus, Vault security engines, and client security for Hyperledger Fabric."}	The Hyperledger Labs blockchain-carbon-accounting project includes a Hyperledger Fabric network for recording the carbon and Greenhouse Gas (GHG) emissions that cause climate change.  Since there are many activities that cause such emissions, the network is designed to accept data from multiple sources of measurements.  In this project, we will demonstrate integrations from measurement sources with blockchain networks by integrating the ThoughtWorks cloud computing emissions calculator, the NREL OpenPath mobile application, and other web- and mobile-based API's sources to turn instrumented readings into emissions measurements.   It will leverage previous projects involving Hyperledger Cactus, Vault security engines, and client security for Hyperledger Fabric.	{ethereum,solidity}	2022	Term 2	https://github.com/hyperledger-labs/blockchain-carbon-accounting	https://wiki.hyperledger.org/display/INTERN/Multiple+Data+Integration+to+Fabric+Climate+Accounting+Network	0	1
607	e8c0d16a-c263-4a6c-bce7-0b896c925a52	CNCF - Kubevela: Support auto generation of multiple languages SDK from CUE II	{"In KubeVela, we use [CUElang](https:","",cuelang.org,") to code the X-Definition. We want to support auto generation of multiple languages SDK from CUE, so that users can buidling KubeVela Application in their own language. This helps to adoptors to build platform based on KubeVela.\n- Expected Outcome: Support auto generation of multiple languages SDK from CUE, including Java, Typescript ,Python. This capability should be part of vela CLI command."}	In KubeVela, we use [CUElang](https://cuelang.org/) to code the X-Definition. We want to support auto generation of multiple languages SDK from CUE, so that users can buidling KubeVela Application in their own language. This helps to adoptors to build platform based on KubeVela.\n- Expected Outcome: Support auto generation of multiple languages SDK from CUE, including Java, Typescript ,Python. This capability should be part of vela CLI command.	{kubernetes,cuelang,java,python,typescript}	2023	Term 3	https://github.com/kubevela/kubevela/issues/5365	https://kubevela.io/	0	117
436	9b8a3840-1355-4301-894b-7271c597f0cf	CNCF - Kubewarden: Kubewarden policies enhancements	{"Description:  Kubewarden has many policies to validate and mutate Kubernetes resources. Therefore, there are many enhancements to be made on them. However, these improvements are still to be made. Thus, it's necessary to fix the open issues in the policies repositories and implement new policies to add more value to the Kubewarden users. \n\nExpected Outcome: Fix as many open issues in the Kubewarden policies as possible and create new policies requested by the community"}	Description:  Kubewarden has many policies to validate and mutate Kubernetes resources. Therefore, there are many enhancements to be made on them. However, these improvements are still to be made. Thus, it's necessary to fix the open issues in the policies repositories and implement new policies to add more value to the Kubewarden users. \n\nExpected Outcome: Fix as many open issues in the Kubewarden policies as possible and create new policies requested by the community	{go,rust,kubernetes}	2023	Term 1	https://github.com/kubewarden/kubewarden-controller/issues/393	https://www.kubewarden.io	300000	154
109	5cdea024-4cc8-4e48-bc61-3660b5f06978	Linux Kernel PCI Add support for Latency Tolerance Reporting _DSM	{"Latency Tolerance Reporting is used to help manage ASPM (Active State Power Management). Some of the LTR values are platform-dependent and must be learned from platform firmware. Linux currently does not ask firmware for this information. I don't know exactly how this information needs to be incorporated into the ASPM support, but I think it's something we should figure out. See the PCI Firmware Spec, r3.2, sec 4.6.6."}	Latency Tolerance Reporting is used to help manage ASPM (Active State Power Management). Some of the LTR values are platform-dependent and must be learned from platform firmware. Linux currently does not ask firmware for this information. I don't know exactly how this information needs to be incorporated into the ASPM support, but I think it's something we should figure out. See the PCI Firmware Spec, r3.2, sec 4.6.6.	{c,shell}	2020	Term 3	https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/	https://wiki.linuxfoundation.org/lkmp	0	21
177	e5096c52-8c4a-422a-8514-966619805a34	Hyperledger - Blockchain Integration for Climate Emissions Data with Fabric and Cactus	{"The Carbon Accounting and Certification Working Group is developing an Operating System for Climate Action, where data from a Hyperledger Fabric channel is tokenized on an Ethereum network.  We would like to create a more robust integration with Hyperledger Cactus.\n\nLearning Objectives\nAs part of this project, you will learn about\n1) Hyperledger Fabric chain code development\n2) Hyperledger Cactus cross-chain integration\n3) REST APIs\n4) Ethereum and Solidity\n5) Open source development and project management\n\nExpected Outcome\nImplementation of an integration from Hyperledger Fabric utility emissions data channel to issue tokens on an Ethereum network using Hyperledger Cactus.  Documentation and tutorials showing how the integration is done."}	The Carbon Accounting and Certification Working Group is developing an Operating System for Climate Action, where data from a Hyperledger Fabric channel is tokenized on an Ethereum network.  We would like to create a more robust integration with Hyperledger Cactus.\n\nLearning Objectives\nAs part of this project, you will learn about\n1) Hyperledger Fabric chain code development\n2) Hyperledger Cactus cross-chain integration\n3) REST APIs\n4) Ethereum and Solidity\n5) Open source development and project management\n\nExpected Outcome\nImplementation of an integration from Hyperledger Fabric utility emissions data channel to issue tokens on an Ethereum network using Hyperledger Cactus.  Documentation and tutorials showing how the integration is done.	{ethereum,solidity,fabric}	2021	Term 2	https://github.com/hyperledger-labs/blockchain-carbon-accounting	https://wiki.hyperledger.org/display/INTERN/Blockchain+Integration+for+Climate+Emissions+Data+with+Fabric+and+Cactus	300000	1
377	9ac41a72-62f4-48e9-8630-5f9be261e2bf	CNCF - Kyverno: More support for subresources	{"Kyverno lacks the ability to operate on some important subresources like ","scale and ","status in areas such as validation and mutation.\n\nUpstream Issue (URL): \n  - https:","",github.com,kyverno,kyverno,issues,"3118\n  - https:","",github.com,kyverno,kyverno,issues,"2843\n  - https:","",github.com,kyverno,kyverno,issues,4313}	Kyverno lacks the ability to operate on some important subresources like /scale and /status in areas such as validation and mutation.\n\nUpstream Issue (URL): \n  - https://github.com/kyverno/kyverno/issues/3118\n  - https://github.com/kyverno/kyverno/issues/2843\n  - https://github.com/kyverno/kyverno/issues/4313	{go}	2022	Term 3	https://github.com/kyverno/kyverno/issues/3118	https://kyverno.io/	0	17
460	e1ff5120-32e4-44a8-a1be-4e0717ef9ad6	CNCF - Linkerd: Add dynamic profiling to Linkerd Rust controllers	{"Description: The Linkerd control plane includes controllers that are written in Rust. Enable users to dynamically profile the running application can aid significantly in debugging and diagnostics. \n\nExpected Outcome: In an upcoming release of Linkerd the policy controller would expose endpoints (leveraging [pprof](https:","",github.com,tikv,pprof-rs,blob,master,"README.md) or another tool) for profiling controller resource consumption."}	Description: The Linkerd control plane includes controllers that are written in Rust. Enable users to dynamically profile the running application can aid significantly in debugging and diagnostics. \n\nExpected Outcome: In an upcoming release of Linkerd the policy controller would expose endpoints (leveraging [pprof](https://github.com/tikv/pprof-rs/blob/master/README.md) or another tool) for profiling controller resource consumption.	{kubernetes,rust}	2023	Term 1	https://github.com/linkerd/linkerd2/issues/10227	https://linkerd.io	300000	61
134	f06db0d5-537e-4e0f-8ca4-0a471f95a04d	Linux kernel Bug Fixing July-Sept 2020, August-October 2020	{"There are many bugs being found in the Linux kernel by automated tools, yet not many people available to help fix these.  This project will help the applicants choose bugs to fix, work through the problem found, and help get the issue resolved in the Linux kernel by producing a fix or working with the community on a fix.\n\nAfter the end of the project, the applicant will have a much deeper knowledge of many different areas of the Linux kernel and hopefully, many bugs fixed and many patches accepted."}	There are many bugs being found in the Linux kernel by automated tools, yet not many people available to help fix these.  This project will help the applicants choose bugs to fix, work through the problem found, and help get the issue resolved in the Linux kernel by producing a fix or working with the community on a fix.\n\nAfter the end of the project, the applicant will have a much deeper knowledge of many different areas of the Linux kernel and hopefully, many bugs fixed and many patches accepted.	{c,linux,shell}	2020	Term 3	https://git.kernel.org	https://kernel.org	2102500	116
471	f629822d-978e-437d-a617-28a02f428ee7	Linux kernel Bug Fixing Spring Unpaid 2023	{"There are many bugs being found in the Linux kernel by automated tools, yet not many people available to help fix these.  This project will help the applicants choose bugs to fix, work through the problem found, and help get the issue resolved in the Linux kernel by producing a fix or working with the community on a fix.\n\nAfter the end of the project, the applicant will have a much deeper knowledge of many different areas of the Linux kernel and hopefully, many bugs fixed and many patches accepted."}	There are many bugs being found in the Linux kernel by automated tools, yet not many people available to help fix these.  This project will help the applicants choose bugs to fix, work through the problem found, and help get the issue resolved in the Linux kernel by producing a fix or working with the community on a fix.\n\nAfter the end of the project, the applicant will have a much deeper knowledge of many different areas of the Linux kernel and hopefully, many bugs fixed and many patches accepted.	{c,shell,kernel}	2023	Term 1	https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/	https://wiki.linuxfoundation.org/lkmp	0	116
31	7cd84e47-4a70-41a3-b3ca-32989be5f8e5	Hyperledger Iroha - Reworking Consensus API	{"In some of the projects this year we've decided to focus on modularity and decoupling which makes a lot of sense when we talk about open source and interoperability. In our chats we receive questions about decoupling different components – some of the users would like to plug different modules in and out.\n\nTo provide them with that option and make development easier and more secure, we would like to explore the option of following the example of the pluggable consensus in HL Sawtooth. In the best case scenario result, we will be able to reuse consensus modules from Sawtooth in Iroha and vice-versa in the future.\n\nSo, overall, this project will increase HL projects interoperability and make Iroha more flexible."}	In some of the projects this year we've decided to focus on modularity and decoupling which makes a lot of sense when we talk about open source and interoperability. In our chats we receive questions about decoupling different components – some of the users would like to plug different modules in and out.\n\nTo provide them with that option and make development easier and more secure, we would like to explore the option of following the example of the pluggable consensus in HL Sawtooth. In the best case scenario result, we will be able to reuse consensus modules from Sawtooth in Iroha and vice-versa in the future.\n\nSo, overall, this project will increase HL projects interoperability and make Iroha more flexible.	{}	2020	Term 2	https://github.com/hyperledger/iroha	https://wiki.hyperledger.org/display/INTERN/Reworking+HL+Iroha+Consensus+APIhttps://wiki.hyperledger.org/display/HYP/Hyperledger+Code+of+Conduct	420000	110
229	fdaeb9be-5863-4cfa-937d-b5f1fc044f40	CNCF - Kyverno: Security model and processes for Kyverno	{"Improve security model and processes for Kyverno. Document security processes, help define a threat model with risks and mitigation and add best practice processes like publishing signed images."}	Improve security model and processes for Kyverno. Document security processes, help define a threat model with risks and mitigation and add best practice processes like publishing signed images.	{kubernetes,security,documentation}	2021	Term 3	https://github.com/kyverno/kyverno/issues/2250	https://kyverno.io	600000	17
103	4cedc314-8d3b-451a-8fca-d5056dca3c69	LF Networking OPNFV - Barometer CI Development and Test Case Creation	{"Barometer collects system metrics and events to aid in the monitoring of the NFVI and provide means for these metrics and events to be relayed to higher level fault management systems for enforcement and corrective actions. In addition, this information could be fed to analytics systems to enable failure prediction, and can also be used for intelligent workload placement.\n\nBarometer uses collected to collect the system metrics, and works closely with the collected community. Increasing the automated test coverage benefits collected as well as Barometer. The Barometer team has contributed several plugins to collected, and in some cases, specialized hardware is needed to test that the plugins are functional.\n\nMajor work items for the Jerma release are improving the test infrastructure for collected.  This project will involve automating existing tests for individual plugins. The test automation being set up will be one instance of a set of collected community CIs.\n\n"}	Barometer collects system metrics and events to aid in the monitoring of the NFVI and provide means for these metrics and events to be relayed to higher level fault management systems for enforcement and corrective actions. In addition, this information could be fed to analytics systems to enable failure prediction, and can also be used for intelligent workload placement.\n\nBarometer uses collected to collect the system metrics, and works closely with the collected community. Increasing the automated test coverage benefits collected as well as Barometer. The Barometer team has contributed several plugins to collected, and in some cases, specialized hardware is needed to test that the plugins are functional.\n\nMajor work items for the Jerma release are improving the test infrastructure for collected.  This project will involve automating existing tests for individual plugins. The test automation being set up will be one instance of a set of collected community CIs.	{c,python,git}	2020	Term 2	https://git.opnfv.org/	https://wiki.lfnetworking.org/display/LN/Barometer+CI+setup+and+test+case+development	600000	161
523	9749bc0a-04c9-498d-a16c-e66c0930e819	CNCF - ORAS: Design and implement Artifact Explore web portal	{"This project goal is to improve the efficiency of the image developers and users through the artifact explorer tool with ORAS under the hood. This tool helps users to explore and search the content of an artifact or a registry. This doc is to gather ideas for early brainstorming purposes. For users, this tool reduces CLI learning cost and improve efficiency for developers. They don’t need to memorize and type the CLI commands to explore the content of an OCI artifact and registry.\n\n- Expected Outcome:\n   - Provides a web portal to view the content of OCI artifacts from any public registries\n   - Users can drill down into the detailed content of an image manifest or a layer\n   - Users can view the artifact reference graph from the web portal\n   - Users can view and download the supply chain artifacts like the signature, SBOM, attestation \n   - Provides search capabilities to allow users to search container images or OCI artifacts on a central web portal. We can combine it with Artifact Search API capabilities (https:","",docs.google.com,document,d,1rcQROZP31q7BOjoZ977Ok7pt28z_UXfW0vAK3xC0wdI,"edit#heading=h.rx512bvufn5q).\n   - Explore the image’s file system of layer (tentative)"}	This project goal is to improve the efficiency of the image developers and users through the artifact explorer tool with ORAS under the hood. This tool helps users to explore and search the content of an artifact or a registry. This doc is to gather ideas for early brainstorming purposes. For users, this tool reduces CLI learning cost and improve efficiency for developers. They don’t need to memorize and type the CLI commands to explore the content of an OCI artifact and registry.\n\n- Expected Outcome:\n   - Provides a web portal to view the content of OCI artifacts from any public registries\n   - Users can drill down into the detailed content of an image manifest or a layer\n   - Users can view the artifact reference graph from the web portal\n   - Users can view and download the supply chain artifacts like the signature, SBOM, attestation \n   - Provides search capabilities to allow users to search container images or OCI artifacts on a central web portal. We can combine it with Artifact Search API capabilities (https://docs.google.com/document/d/1rcQROZP31q7BOjoZ977Ok7pt28z_UXfW0vAK3xC0wdI/edit#heading=h.rx512bvufn5q).\n   - Explore the image’s file system of layer (tentative)	{html,javascript,css,hugo,docker,figma}	2023	Term 2	https://github.com/oras-project/oras-www/issues/158	https://oras.land	300000	156
216	0406b090-f367-45fd-bcf4-56be6004d267	Hyperledger - Automated Testing for Climate Emissions Ledger	{"The utility emissions channel is a Hyperledger Fabric application which calculates Greenhouse Gas (GHG) emissions from utility bills and stores them on a Fabric ledger.  This can then be used to generate audited emissions for businesses and individuals and help them calculate and offset their total carbon footprint.  Because the application involves looking up data and performing numerical calculations, it's important to have automated tests to make sure that it's done correctly. \n\nAs part of this project, you will develop automated tests for the utility emissions channel which will validate that the application is working as expected.  These tests will verify that the application is looking up the utility bills for the correct customer and time frame, using the correct emissions factors data to calculate GHG emissions, recording them correctly on the Fabric ledger, and finally tokenizing them correctly on an Ethereum network. \n\nThere are currently tests written to submit requests via REST API which connects to the chain code.  The tests could be enhanced and eventually packaged along with deployment scripts so that a fresh testing instance could load all the data and run all the scripts to validate the results.\n\nExpected Outcome\nAt the end of the project, you will have had a lot of fun, helped with climate change, and created an automated test suite to validate the utility emissions channel's emissions calculations."}	The utility emissions channel is a Hyperledger Fabric application which calculates Greenhouse Gas (GHG) emissions from utility bills and stores them on a Fabric ledger.  This can then be used to generate audited emissions for businesses and individuals and help them calculate and offset their total carbon footprint.  Because the application involves looking up data and performing numerical calculations, it's important to have automated tests to make sure that it's done correctly. \n\nAs part of this project, you will develop automated tests for the utility emissions channel which will validate that the application is working as expected.  These tests will verify that the application is looking up the utility bills for the correct customer and time frame, using the correct emissions factors data to calculate GHG emissions, recording them correctly on the Fabric ledger, and finally tokenizing them correctly on an Ethereum network. \n\nThere are currently tests written to submit requests via REST API which connects to the chain code.  The tests could be enhanced and eventually packaged along with deployment scripts so that a fresh testing instance could load all the data and run all the scripts to validate the results.\n\nExpected Outcome\nAt the end of the project, you will have had a lot of fun, helped with climate change, and created an automated test suite to validate the utility emissions channel's emissions calculations.	{fabric}	2021	Term 2	https://github.com/hyperledger-labs/blockchain-carbon-accounting	https://wiki.hyperledger.org/display/INTERN/Automated+Testing+for+Climate+Emissions+Ledger	600000	1
35	f53f3817-c5d4-474d-997f-231b5d243133	Hyperledger - Support NFT standards in Weaver for cross-network asset operations	{"Weaver, a Hyperledger labs project, is a DLT interoperation framework that supports data sharing, asset exchange and asset transfer between blockchain networks built on independent DLTs while preserving core blockchain tenets of decentralization of the participating blockchains and without relying on trusted mediating individuals or networks. True interoperability of tokens among different networks, whether they are built on permissoned or open blockchain technology, requires standards around tokens so that they can be exchanged or transferred across network boundaries  independent of the underlying DLT implementation. The Enterprise Ethereum Alliance (EEA) has made significant strides in this direction by drafting ERC (Ethereum Request for Comments) standards for managing different types of assets. The widely used ERC-20 standard helps manage fungible token types, whereas ERC-721 and ERC-1155 standards help manage non-fungible token (NFT) types and multi-token types respectively.\n\nThe scope of the project covers interoperation support for these three prominent ERC token types across Hyperledger Fabric, Corda and Hyperledger Besu networks. Weaver currently supports interoperation between ERC-20 token types for Hyperledger Besu networks. The goal of this project is to add support for ERC-721 and ERC-1155 standards for interoperation among Hyperledger Besu networks, and extend support for ERC-20, ERC-721 and ERC-1155 token types to interoperate across all three DLT platforms."}	Weaver, a Hyperledger labs project, is a DLT interoperation framework that supports data sharing, asset exchange and asset transfer between blockchain networks built on independent DLTs while preserving core blockchain tenets of decentralization of the participating blockchains and without relying on trusted mediating individuals or networks. True interoperability of tokens among different networks, whether they are built on permissoned or open blockchain technology, requires standards around tokens so that they can be exchanged or transferred across network boundaries  independent of the underlying DLT implementation. The Enterprise Ethereum Alliance (EEA) has made significant strides in this direction by drafting ERC (Ethereum Request for Comments) standards for managing different types of assets. The widely used ERC-20 standard helps manage fungible token types, whereas ERC-721 and ERC-1155 standards help manage non-fungible token (NFT) types and multi-token types respectively.\n\nThe scope of the project covers interoperation support for these three prominent ERC token types across Hyperledger Fabric, Corda and Hyperledger Besu networks. Weaver currently supports interoperation between ERC-20 token types for Hyperledger Besu networks. The goal of this project is to add support for ERC-721 and ERC-1155 standards for interoperation among Hyperledger Besu networks, and extend support for ERC-20, ERC-721 and ERC-1155 token types to interoperate across all three DLT platforms.	{solidity,go}	2022	Term 2	https://github.com/hyperledger-labs/weaver-dlt-interoperability	https://wiki.hyperledger.org/display/INTERN/Support+NFT+standards+in+Weaver+for+cross-network+asset+operations	480000	1
36	0fd19dde-0990-4987-9ab3-2fe9b2ce26b2	CNCF - Kubernetes WG Policy: CIS Benchmarks Policy Report	{"Execute CIS benchmark checks and produce a Policy Report CRD."}	Execute CIS benchmark checks and produce a Policy Report CRD.	{go,json}	2021	Term 1	https://github.com/kubernetes-sigs/wg-policy-prototypes/issues/29	kubernetes.io	300000	75
123	55cdb4a1-76bd-423a-ab48-3bdf1502a171	Open Printing: IPP scan (or virtual MF device) server (Scanner Application)	{"The Internet Printing Protocol (IPP) does not only support printing, but also scanning, as there are many printers which also have a scanner (multi-function (MF) devices). Both CUPS and the developer tool ippserver emulate IPP network printers but not IPP scanners and so they cannot serve as a server to share a local scanner.\n\nThis task is about adding the scan server functionality. If you have a scanner connected locally (and it scans via SANE), share it as an IPP scanner, advertising itself and accepting jobs using the IPP driverless scanning standard. In contrary to SANE-based network scanning clients with any operating system, also phones or IoT devices can scan on your shared scanner.\n\nAlso old hardware can be recycled to a modern MF device.\n\nThis server software will be a so-called Scanner Application, a sample implementation of the future form of scanner drivers, easily packageable in sandboxed, distribution-independent package formats like Snap."}	The Internet Printing Protocol (IPP) does not only support printing, but also scanning, as there are many printers which also have a scanner (multi-function (MF) devices). Both CUPS and the developer tool ippserver emulate IPP network printers but not IPP scanners and so they cannot serve as a server to share a local scanner.\n\nThis task is about adding the scan server functionality. If you have a scanner connected locally (and it scans via SANE), share it as an IPP scanner, advertising itself and accepting jobs using the IPP driverless scanning standard. In contrary to SANE-based network scanning clients with any operating system, also phones or IoT devices can scan on your shared scanner.\n\nAlso old hardware can be recycled to a modern MF device.\n\nThis server software will be a so-called Scanner Application, a sample implementation of the future form of scanner drivers, easily packageable in sandboxed, distribution-independent package formats like Snap.	{c,printing}	2020	Term 3	https://github.com/OpenPrinting	https://openprinting.github.io/	0	69
41	ac33cc23-2fad-4703-b7d6-22202b7f10b3	Integrated cloud native reference stack for edge use cases	{"Integrated Cloud Native (ICN) Blueprint (BP) family intends to address deployment of workloads at large number of edges and public clouds using Kubernetes (K8s) as resource orchestrator at each site and ONAP4K8s as service level orchestrator across sites. \n\nICN BP is an end2end solution that provides a software stack integrating multiple layers of software such as Service orchestrator, Resource Orchestrator, Software and hardware accelerator plugins etc. Objective of ICN BP is to have single stack for Enterprise, Telco and IoT use cases.\nMajor work items where the intern can help are –\n1.\tBug fixes and improving installation scripts\n2.\tKUD integration, tuning, and adding new features\n3.\tMaintaining current CI","CD systems\n4.\tIntegrating Cluster API into ICN local controller"}	Integrated Cloud Native (ICN) Blueprint (BP) family intends to address deployment of workloads at large number of edges and public clouds using Kubernetes (K8s) as resource orchestrator at each site and ONAP4K8s as service level orchestrator across sites. \n\nICN BP is an end2end solution that provides a software stack integrating multiple layers of software such as Service orchestrator, Resource Orchestrator, Software and hardware accelerator plugins etc. Objective of ICN BP is to have single stack for Enterprise, Telco and IoT use cases.\nMajor work items where the intern can help are –\n1.\tBug fixes and improving installation scripts\n2.\tKUD integration, tuning, and adding new features\n3.\tMaintaining current CI/CD systems\n4.\tIntegrating Cluster API into ICN local controller	{}	2020	Term 2	https://github.com/onap/multicloud-k8s/tree/master/kud	https://wiki.akraino.org/display/AK/ICN+-+Metal3+Baremetal+Operator	600000	108
268	47dbcdfb-7468-4f31-8701-a51f705ac87f	CNCF - WasmEdge: Implement typed function references proposal	{"Description: WasmEdge is a lightweight, high-performance, and extensible WebAssembly runtime for cloud native, edge, and decentralized applications. WasmEdge is an official sandbox project hosted by the CNCF. This proposal is one of the requirements for GC proposal. Typed function references proposal adds function references that are typed and can be called directly."}	Description: WasmEdge is a lightweight, high-performance, and extensible WebAssembly runtime for cloud native, edge, and decentralized applications. WasmEdge is an official sandbox project hosted by the CNCF. This proposal is one of the requirements for GC proposal. Typed function references proposal adds function references that are typed and can be called directly.	{}	2022	Term 1	https://github.com/WasmEdge/WasmEdge/issues/1123	https://wasmedge.org/	600000	31
210	46f39522-fef2-4803-9877-b5052ebc2771	Hyperledger Iroha + Cactus - Integration	{"Hyperledger Cactus is a blockchain decentralised integration tool designed to allow users to securely integrate different blockchains started by companies Fujitsu and Accenture. Cactus has pluggable architecture which makes easy to integrate various blockchain by creating plugin, currently plugins for Fabric, Besu, Quorum are implemented. Cactus allows to transfer not only assets but also data between multiple blockchains. On the other hand Iroha (version 1.x) is great with asset management, and has functionality to store data, which makes those two projects a perfect fit!\n\nWith this internship we will not only allow the interoperability between different blockchains but also create a system of Iroha networks that will also demonstrate the integration in examples easy to run by everybody.\n\nThe mentee will be able to learn:\n1) ways of integrating different projects from architectural point of view,\n2) architecture of Iroha (1.x) and Cactus,\n3) work in true spirit of open-source, communicating with both Iroha and Cactus community, joining calls and using other community tools,\n4) writing documentation, so anyone in the community could use the results of their work,\n5)following rules and standards of open-source projects created by hyperledger\n\nExpected Outcome\n1) Documented, ready-to-use integration of Iroha and Cactus,\n2) Documented example of integration between multiple (two and more) iroha's networks with Cactus,\n3)Documented example of integration between Fabric and Iroha"}	Hyperledger Cactus is a blockchain decentralised integration tool designed to allow users to securely integrate different blockchains started by companies Fujitsu and Accenture. Cactus has pluggable architecture which makes easy to integrate various blockchain by creating plugin, currently plugins for Fabric, Besu, Quorum are implemented. Cactus allows to transfer not only assets but also data between multiple blockchains. On the other hand Iroha (version 1.x) is great with asset management, and has functionality to store data, which makes those two projects a perfect fit!\n\nWith this internship we will not only allow the interoperability between different blockchains but also create a system of Iroha networks that will also demonstrate the integration in examples easy to run by everybody.\n\nThe mentee will be able to learn:\n1) ways of integrating different projects from architectural point of view,\n2) architecture of Iroha (1.x) and Cactus,\n3) work in true spirit of open-source, communicating with both Iroha and Cactus community, joining calls and using other community tools,\n4) writing documentation, so anyone in the community could use the results of their work,\n5)following rules and standards of open-source projects created by hyperledger\n\nExpected Outcome\n1) Documented, ready-to-use integration of Iroha and Cactus,\n2) Documented example of integration between multiple (two and more) iroha's networks with Cactus,\n3)Documented example of integration between Fabric and Iroha	{python,java}	2021	Term 2	https://github.com/hyperledger/iroha	https://wiki.hyperledger.org/display/INTERN/HL+Iroha+and+HL+Cactus+Integration	600000	110
324	af03f76c-6253-4efc-b26b-17c522794813	CNCF - KubeArmor: Support for OpenShift	{"KubeArmor is a cloud-native runtime security enforcement system that restricts the behavior (such as process execution, file access, and networking operation) of containers and nodes (VMs) at the system level. This project aims to support KubeArmor on OpenShift. The work will include compatibility analysis of KubeArmor on OpenShift, finding limitations (if any), and eventually testing it on OpenShift."}	KubeArmor is a cloud-native runtime security enforcement system that restricts the behavior (such as process execution, file access, and networking operation) of containers and nodes (VMs) at the system level. This project aims to support KubeArmor on OpenShift. The work will include compatibility analysis of KubeArmor on OpenShift, finding limitations (if any), and eventually testing it on OpenShift.	{kubernetes,openshift}	2022	Term 2	https://github.com/kubearmor/KubeArmor/issues/221	https://kubearmor.io/	300000	9
243	682b8b07-8fb4-4243-8932-cc27ecea2d0b	Linux Kernel: Evaluate and Improve checkpatch.pl	{"The project’s goal is to evaluate results from checkpatch.pl, aggregate its findings and improve checkpatch.pl based on the evaluation. The checkpatch.pl script detects various coding style issues and checks for various stylistic conventions on commit messages and supporting information, e.g., consistent records of  MAINTAINERS, license information, proper file permissions, etc. However, an earlier investigation on checkpatch.pl has shown that many patches in the kernel repository are accepted, even though checkpatch.pl reports findings on them. Some of those checkpatch.pl errors and warnings are “false positives”.\nThe work in this project shall be driven by a continuous, highly iterative cycle of evaluation on large sets of commits and patches, machine-assisted aggregation and clustering to identify specific community-local conventions, and adjustment and configuration of the checkpatch.pl script. The repeated evaluation will prioritize the specific changes to checkpatch.pl."}	The project’s goal is to evaluate results from checkpatch.pl, aggregate its findings and improve checkpatch.pl based on the evaluation. The checkpatch.pl script detects various coding style issues and checks for various stylistic conventions on commit messages and supporting information, e.g., consistent records of  MAINTAINERS, license information, proper file permissions, etc. However, an earlier investigation on checkpatch.pl has shown that many patches in the kernel repository are accepted, even though checkpatch.pl reports findings on them. Some of those checkpatch.pl errors and warnings are “false positives”.\nThe work in this project shall be driven by a continuous, highly iterative cycle of evaluation on large sets of commits and patches, machine-assisted aggregation and clustering to identify specific community-local conventions, and adjustment and configuration of the checkpatch.pl script. The repeated evaluation will prioritize the specific changes to checkpatch.pl.	{perl,c,bash}	2020	Term 3	https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/		600000	116
29	a5d7c85a-e8c5-4c25-a4b8-593cc8e2e508	XDP redirect: Fixes and performance improvements	{"The eXpress Data Path (XDP) feature in the kernel enabled high performance\nprogrammable packet processing using BPF. This is achieved by way of the XDP\nhook for attaching BPF programs to a very early stage of packet processing in\nthe network drivers. This allows very high packet processing performance, and\nXDP programs have access to the full packet data and can filter or steer packets\nusing kernel helpers and different return codes.\n\nThe XDP_REDIRECT return code is the most flexible of the return codes and is\nused to direct packets to other places in the system, including other network\ninterfaces, other CPUs, or straight to userspace. While the redirect feature is\nconceptually simple from the BPF user's point of view, the implementation of it\nin the kernel uses various tricks to improve performance as packets traverse\nthe different parts of the kernel.\n\nThis project aims to improve various outstanding issues of the XDP redirect\nfacility. The exact tasks taken on will be up to the mentee's particular\ninterests and skill set, and can include implementing missing pieces of\nfunctionality, performance analysis and improvements as well as adding kernel\nselftests. We have a list of issues to tackle which includes things like:\n\n- Implementing missing redirect features for Generic XDP\n- Improving performance through bulking of operations on redirect\n- Adding selftests for redirect map types\n- Improving the usability of BPF samples using redirect"}	The eXpress Data Path (XDP) feature in the kernel enabled high performance\nprogrammable packet processing using BPF. This is achieved by way of the XDP\nhook for attaching BPF programs to a very early stage of packet processing in\nthe network drivers. This allows very high packet processing performance, and\nXDP programs have access to the full packet data and can filter or steer packets\nusing kernel helpers and different return codes.\n\nThe XDP_REDIRECT return code is the most flexible of the return codes and is\nused to direct packets to other places in the system, including other network\ninterfaces, other CPUs, or straight to userspace. While the redirect feature is\nconceptually simple from the BPF user's point of view, the implementation of it\nin the kernel uses various tricks to improve performance as packets traverse\nthe different parts of the kernel.\n\nThis project aims to improve various outstanding issues of the XDP redirect\nfacility. The exact tasks taken on will be up to the mentee's particular\ninterests and skill set, and can include implementing missing pieces of\nfunctionality, performance analysis and improvements as well as adding kernel\nselftests. We have a list of issues to tackle which includes things like:\n\n- Implementing missing redirect features for Generic XDP\n- Improving performance through bulking of operations on redirect\n- Adding selftests for redirect map types\n- Improving the usability of BPF samples using redirect	{c,shell,kernel}	2021	Term 1	https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/	https://wiki.linuxfoundation.org/lkmp	300000	21
240	95074422-1740-4cf8-bcce-917798c3f840	CNCF - WasmEdge: Support WASI-NN proposal	{"Machine Learning is a big topic nowadays. WasmEdge already provides [a set of TensorFlow host functions](https:","",github.com,second-state,"wasmedge_tensorflow_interface) to enable the ML inference in WebAssembly. However, these TensorFlow host functions are defined by us and they are just a Wasm function binding from the [TensorFlow C API](https:","",github.com,tensorflow,tensorflow,blob,master,tensorflow,c,"c_api.h). Here comes a standard, the [WASI-NN proposal](https:","",github.com,WebAssembly,"wasi-nn) provides a new way to perform neural network inferencing by using a runtime-provided implementation that can leverage host native optimizations, CPU multi-threading, or powerful hardware devices such as GPUs or TPUs."}	Machine Learning is a big topic nowadays. WasmEdge already provides [a set of TensorFlow host functions](https://github.com/second-state/wasmedge_tensorflow_interface) to enable the ML inference in WebAssembly. However, these TensorFlow host functions are defined by us and they are just a Wasm function binding from the [TensorFlow C API](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/c/c_api.h). Here comes a standard, the [WASI-NN proposal](https://github.com/WebAssembly/wasi-nn) provides a new way to perform neural network inferencing by using a runtime-provided implementation that can leverage host native optimizations, CPU multi-threading, or powerful hardware devices such as GPUs or TPUs.	{rust}	2021	Term 3	https://github.com/WasmEdge/WasmEdge/issues/343	https://wasmedge.org/	300000	31
218	98f6b50e-d097-42ca-b42f-3f51e6f91f0b	CNCF - Buildpacks: Update Builder implementation to 0.1 Builder Spec	{"In Cloud Native Buildpacks, [Builders](https:","",buildpacks.io,docs,concepts,components,builder,") are distributed OCI images that act as the complete context for the building of an application. They came into existance by necessity and have risen to be an essential concept that now needs a specification. As part of this project, you will help us make the necessary changes to [pack](https:","",github.com,buildpacks,"pack) to adhere to the specification as well as _finalize_ the specification itself. This will have to be done with consideration to existing implementations in effort to prevent unintentionally breaking anyone's existing workflow."}	In Cloud Native Buildpacks, [Builders](https://buildpacks.io/docs/concepts/components/builder/) are distributed OCI images that act as the complete context for the building of an application. They came into existance by necessity and have risen to be an essential concept that now needs a specification. As part of this project, you will help us make the necessary changes to [pack](https://github.com/buildpacks/pack) to adhere to the specification as well as _finalize_ the specification itself. This will have to be done with consideration to existing implementations in effort to prevent unintentionally breaking anyone's existing workflow.	{go,docker}	2021	Term 3	https://github.com/buildpacks/pack/issues/945	https://buildpacks.io/	300000	27
39	386ea576-b9a6-4fad-b76d-f954daefbf1f	CNCF - Buildpacks: Web Redesign of Feature Comparison	{"Cloud Native Buildpacks is a specification and set of tools that help you take source code and convert them into OCI images. Sound familiar? Maybe you've heard of Docker, Source-2-Image, Kaniko, etc. Well you are not alone. The goal of this project is to refactor our existing [\\"features\\" page](https:","",buildpacks.io,features,"#comparison) to provide an easier to comprehend comparison across other similar solutions. Through this project, you'll research each alternative, learn how they compare and aim to provide that information to the users in an easy to digest format. This will include designing and implementing a better format to compare projects and their features side-by-side."}	Cloud Native Buildpacks is a specification and set of tools that help you take source code and convert them into OCI images. Sound familiar? Maybe you've heard of Docker, Source-2-Image, Kaniko, etc. Well you are not alone. The goal of this project is to refactor our existing ["features" page](https://buildpacks.io/features/#comparison) to provide an easier to comprehend comparison across other similar solutions. Through this project, you'll research each alternative, learn how they compare and aim to provide that information to the users in an easy to digest format. This will include designing and implementing a better format to compare projects and their features side-by-side.	{containers,javascript,css,html,design}	2021	Term 3	https://github.com/buildpacks/pack/issues/389	https://buildpacks.io/	300000	27
250	cd8810bf-e2cf-43fa-b727-3d2b08021434	CDP 2-Way Communications	{"ClusterDuck Protocol is a mesh communication firmware that utilizes LoRa radios and makes it easy to set up emergency networks. The current implementation only allows for information to flow in one direction. While this works well, the experience is not great for end users who need to know if help is on the way. Therefore we want to add 2-Way communication so that disaster survivors can chat with emergency response teams."}	ClusterDuck Protocol is a mesh communication firmware that utilizes LoRa radios and makes it easy to set up emergency networks. The current implementation only allows for information to flow in one direction. While this works well, the experience is not great for end users who need to know if help is on the way. Therefore we want to add 2-Way communication so that disaster survivors can chat with emergency response teams.	{arduino}	2022	Term 1	https://github.com/Call-for-Code/ClusterDuck-Protocol	https://clusterduckprotocol.org/	300000	122
136	5dd87db8-769b-4647-97c1-2bc4ab571808	CNCF - Chaos Mesh: Chaos Engineering as a Service	{"Description: Chaos Mesh is not like Chaos Engineering as a Service now:\n- Poor observability: the result of chaos experiments are not easy to observe and judge, the users need to check whether the Chaos effects by manual.\n- Chaosd(for physic node) is too simple: only supports command line operation, does not support task scheduling and life cycle management.\n- The costs of learning operation and maintenance are high: the maintenance of Chaos Mesh and Chaosd are not unified.\n- It should be a unified place to manage Chaos experiments for multiple platforms and multiple clusters, and can see the monitoring data of the experiment."}	Description: Chaos Mesh is not like Chaos Engineering as a Service now:\n- Poor observability: the result of chaos experiments are not easy to observe and judge, the users need to check whether the Chaos effects by manual.\n- Chaosd(for physic node) is too simple: only supports command line operation, does not support task scheduling and life cycle management.\n- The costs of learning operation and maintenance are high: the maintenance of Chaos Mesh and Chaosd are not unified.\n- It should be a unified place to manage Chaos experiments for multiple platforms and multiple clusters, and can see the monitoring data of the experiment.	{go}	2021	Term 1	https://github.com/chaos-mesh/chaos-mesh/issues/1462	https://chaos-mesh.org/	300000	81
154	1d49ebad-29fa-430c-8492-44f29ee61cc1	CNCF - Chaos Mesh: Enriching AWS chaos	{"We have already made a technical previewing implementation for AWS Chaos, it could inject some simple chaos now, such as stop","restart the EC2. And we want to make it more stable and structured. And there is another direction of AWS chaos: AWS service failure. It might be useful for testing infrastructure automation tools. Basically, there are two things that we want to do: - enriching e2e test cases using localstack - more chaos by simulating AWS service failure by hijacking awscli request to a modified localstack."}	We have already made a technical previewing implementation for AWS Chaos, it could inject some simple chaos now, such as stop/restart the EC2. And we want to make it more stable and structured. And there is another direction of AWS chaos: AWS service failure. It might be useful for testing infrastructure automation tools. Basically, there are two things that we want to do: - enriching e2e test cases using localstack - more chaos by simulating AWS service failure by hijacking awscli request to a modified localstack.	{go,python}	2021	Term 1	https://github.com/chaos-mesh/chaos-mesh/issues/1472	https://chaos-mesh.org/	300000	81
252	09847d84-5d14-4c05-8644-57cdde5b6466	CNCF - Chaos Mesh: Interactive Katacoda Playground for Chaos Experiment Examples	{"Chaos Mesh is a powerful chaos engineering platform for Kubernetes. There is a Katacoda playground as interactive tutorial, and we want to build more Katacoda scenarios as the minimum examples for each certain type of chaos experiment. The basic work would be to create new katacoda scenarios as referred to by https:","",github.com,chaos-mesh,chaos-mesh,tree,master,"examples, and build small applications (with well-built observability) as the target of chaos experiment if required. This project will not require you to dive deep into the hard-core parts of Chaos Mesh, instead, it would be a tour of learning and exploring Chaos Mesh."}	Chaos Mesh is a powerful chaos engineering platform for Kubernetes. There is a Katacoda playground as interactive tutorial, and we want to build more Katacoda scenarios as the minimum examples for each certain type of chaos experiment. The basic work would be to create new katacoda scenarios as referred to by https://github.com/chaos-mesh/chaos-mesh/tree/master/examples, and build small applications (with well-built observability) as the target of chaos experiment if required. This project will not require you to dive deep into the hard-core parts of Chaos Mesh, instead, it would be a tour of learning and exploring Chaos Mesh.	{go,kubernetes,shell}	2022	Term 1	https://github.com/chaos-mesh/chaos-mesh/issues/2842	https://chaos-mesh.org/	600000	81
221	8db683b0-0273-4a83-9ed9-4c33ee2cfcf0	CNCF - Chaos Mesh: Monitoring Metrics about Chaos Mesh	{"Observability is very important for each application, we want to monitor more things of Chaos Mesh components, enrich the metrics for both logic patterns and performance data. We want to let users could watch the status of Chaos Mesh on grafana dashboard, and developers could using time-series metrics for debugging and profiling."}	Observability is very important for each application, we want to monitor more things of Chaos Mesh components, enrich the metrics for both logic patterns and performance data. We want to let users could watch the status of Chaos Mesh on grafana dashboard, and developers could using time-series metrics for debugging and profiling.	{go,prometheus,grafana}	2021	Term 3	https://github.com/chaos-mesh/chaos-mesh/issues/2198	https://chaos-mesh.org/	300000	81
125	12a8bfec-ba3f-496e-bc26-118e9f5eebe6	Chaos Mesh	{"A Chaos Engineering Platform for Kubernetes."}	A Chaos Engineering Platform for Kubernetes.	{kubernetes,go}	2021	Term 1	https://github.com/chaos-mesh/chaos-mesh	https://github.com/chaos-mesh/chaos-mesh	600000	71
44	680e32e5-d056-46fa-a94d-4af453d4e81d	CNCF - Cilium: Improving Security posture of the Cilium/Hubble/Tetragon release process	{"To be able to improve the Security posture of the Cilium family’s open source projects (Cilium, Hubble, Tetragon), we would need to create signed SBOMs, signed release artifacts for each open source project and automate all of these steps.\n\nhttps:","",github.com,cilium,cilium,issues,"19282\nhttps:","",github.com,cilium,cilium,issues,"20712\nhttps:","",github.com,cilium,cilium,issues,20850}	To be able to improve the Security posture of the Cilium family’s open source projects (Cilium, Hubble, Tetragon), we would need to create signed SBOMs, signed release artifacts for each open source project and automate all of these steps.\n\nhttps://github.com/cilium/cilium/issues/19282\nhttps://github.com/cilium/cilium/issues/20712\nhttps://github.com/cilium/cilium/issues/20850	{go,kubernetes,docker,security}	2022	Term 3	https://github.com/cilium/cilium/issues/19282	https://cilium.io/	0	28
518	659fe584-68e6-46bf-bd13-12653ef60268	CNCF - Cilium: Tetragon Implement a Kubernetes operator to maintain pod IP to pod metadata mapping	{"Tetragon currently depends on Cilium to look up pod information by their IP addresses. The goal of this project is to remove this Cilium dependency by implementing a Kubernetes operator that provides this information. The idea is for this operator to maintain a new custom resource that provide a mapping from IPs to the small subset of pod information that Tetragon needs.\n\n- Expected Outcome:\n  - A Kubernetes operator that maintains IP to pod info mapping used by Tetragon.\n  - The operator should be installable via Helm as a Kubernetes deployment.\n  - Replace Cilium dependency in the code base with this new custom resource.\n  - Some performance benchmarks in a high pod churn environment."}	Tetragon currently depends on Cilium to look up pod information by their IP addresses. The goal of this project is to remove this Cilium dependency by implementing a Kubernetes operator that provides this information. The idea is for this operator to maintain a new custom resource that provide a mapping from IPs to the small subset of pod information that Tetragon needs.\n\n- Expected Outcome:\n  - A Kubernetes operator that maintains IP to pod info mapping used by Tetragon.\n  - The operator should be installable via Helm as a Kubernetes deployment.\n  - Replace Cilium dependency in the code base with this new custom resource.\n  - Some performance benchmarks in a high pod churn environment.	{go,kubernetes}	2023	Term 2	https://github.com/cilium/tetragon/issues/794	https://cilium.io/	300000	28
432	81a0e506-1c05-45fa-90c4-6bde8bdc0e61	CNCF - Cilium: Website Use Cases pages	{"Cilium would like to have use case pages built out on its website to make it easy for people to find the information and relevant content to the problems they are trying to solve with Cilium.\n\nExpected Outcome: The mentee will read through relevant docs, blogs, case studies, user stories, and labs to understand the use cases which will drive the content for each of the pages being built. The finished product will be a new use cases section on the Cilium website."}	Cilium would like to have use case pages built out on its website to make it easy for people to find the information and relevant content to the problems they are trying to solve with Cilium.\n\nExpected Outcome: The mentee will read through relevant docs, blogs, case studies, user stories, and labs to understand the use cases which will drive the content for each of the pages being built. The finished product will be a new use cases section on the Cilium website.	{css,javascript}	2023	Term 1	https://github.com/cilium/cilium.io/issues/226	https://cilium.io/	0	28
423	ee387e1b-de4e-4c1e-9bef-0239a2e9ca40	CNCF - Cloud Native Buildpacks: Multi-Architecture Builds Support	{"The rise of ARM processors has created new binary targets for pre-compiled executables. Additionally, there are tales of widespread use of operating systems that aren't linux? In the ideal case a `pack` user could create a build for an abritrary architecture and operating system, regardless of the host system they used to run the command.\n\nExpected outcome: Improved multi-architecture (including ARM) and multi-os \\"cross-compilation\\" support in [pack](https:","",github.com,buildpacks,pack,")\n\nUpstream Issue (URL): https:","",github.com,buildpacks,pack,issues,"1459 and https:","",github.com,buildpacks,pack,issues,1460}	The rise of ARM processors has created new binary targets for pre-compiled executables. Additionally, there are tales of widespread use of operating systems that aren't linux? In the ideal case a `pack` user could create a build for an abritrary architecture and operating system, regardless of the host system they used to run the command.\n\nExpected outcome: Improved multi-architecture (including ARM) and multi-os "cross-compilation" support in [pack](https://github.com/buildpacks/pack/)\n\nUpstream Issue (URL): https://github.com/buildpacks/pack/issues/1459 and https://github.com/buildpacks/pack/issues/1460	{go,git,buildpacks}	2023	Term 1	https://github.com/buildpacks/pack/issues/1459	https://buildpacks.io	300000	82
27	d70e1f9e-abde-403f-8389-52a122301500	Cloud native measurement, reporting and validation of carbon emissions	{"Help us fight climate change through nature-based solutions!\n\nThe Full Lands INtegration Tool (FLINT) is a platform for estimating greenhouse gas emissions at local, national and global scales.\n\nThis project aims to build a cloud deployment framework for rapid deployment of FLINT implementations. This framework will be used in a continuous deployment pipeline (CD) for integration testing and the delivery of FLINT as a service (FLINTcloud) for demonstration purposes. \n\nWe hope to offer an easy entry point for new users to evaluate the FLINT platform and provide a blueprint for new users to roll their own FLINTcloud deployments. This is critical to help drive the adoption of FLINT and help scale up the monitoring, reporting and validation of carbon emissions and sequestration from land use and land use change around the world.\n\nThis will involve:\n1.\tPublishing a design for the complete FLINTcloud solution, detailing the required components, technologies and endpoints to be exposed. \n2.\tExposing the core FLINT routines by wrapping the existing command line interface in a REST API.\n3.\tCreating a simple script to install the prerequisite libraries, FLINT and its modules as a standalone microservice.\n4.\tCreating the deployment framework and setting up a CD pipeline on commodity cloud hardware, favouring automation wherever possible (e.g. infrastructure-as-code). \n5.\tProviding new documentation for the cloud deployment procedure."}	Help us fight climate change through nature-based solutions!\n\nThe Full Lands INtegration Tool (FLINT) is a platform for estimating greenhouse gas emissions at local, national and global scales.\n\nThis project aims to build a cloud deployment framework for rapid deployment of FLINT implementations. This framework will be used in a continuous deployment pipeline (CD) for integration testing and the delivery of FLINT as a service (FLINTcloud) for demonstration purposes. \n\nWe hope to offer an easy entry point for new users to evaluate the FLINT platform and provide a blueprint for new users to roll their own FLINTcloud deployments. This is critical to help drive the adoption of FLINT and help scale up the monitoring, reporting and validation of carbon emissions and sequestration from land use and land use change around the world.\n\nThis will involve:\n1.\tPublishing a design for the complete FLINTcloud solution, detailing the required components, technologies and endpoints to be exposed. \n2.\tExposing the core FLINT routines by wrapping the existing command line interface in a REST API.\n3.\tCreating a simple script to install the prerequisite libraries, FLINT and its modules as a standalone microservice.\n4.\tCreating the deployment framework and setting up a CD pipeline on commodity cloud hardware, favouring automation wherever possible (e.g. infrastructure-as-code). \n5.\tProviding new documentation for the cloud deployment procedure.	{kubernetes,terraform,docker,documentation,python,javascript}	2021	Term 1	https://github.com/moja-global/FLINT.Cloud	moja.global	3000000	108
202	9ddced83-279c-460d-869a-3f92e9c98c2c	CNCF: Kyverno - Test mutate and generate policies via the Kyverno CLI	{"Kyverno is a Kubernetes native policy manager that also can be used in a CI","CD pipeline. This project will extend the Kyverno command line tool to support mutate and generate rules and add more E2E","Unit Tests and offer test reports based on the results."}	Kyverno is a Kubernetes native policy manager that also can be used in a CI/CD pipeline. This project will extend the Kyverno command line tool to support mutate and generate rules and add more E2E/Unit Tests and offer test reports based on the results.	{go,testing}	2021	Term 2	https://kyverno.io	https://github.com/kyverno/kyverno/issues/1821	540000	108
199	de95a031-903b-4129-b34f-f39611fd176c	CNCF: Tremor - Modular sub-queries in tremor-query	{"Currently tremor supports composition through composing pipelines together, through function composition and through allowing references to query operator definitions and constants in externalizable modules that can be loaded via a module path.\n\n  It would be excellent if the modularity in tremor extended fully to the query language so that distinct subgraphs could be modularized and consumed by multiple queries to optimise for reuse of flow oriented logic in tremor.\n\n  This would require extending module support in the tremor query language to support sub-graph definitions with parameters that can be declared and used as part of a higher level query.\n\n  Modules in tremor-query in their current state: <https:","",docs.tremor.rs,tremor-query,modules,">\n\n  This project idea involves designing the sub-graph module syntax and semantics and implementing changes to the lexer, grammar, optimizers and runtime. It is most suited to candidates who are interested in programming language evolution and design."}	Currently tremor supports composition through composing pipelines together, through function composition and through allowing references to query operator definitions and constants in externalizable modules that can be loaded via a module path.\n\n  It would be excellent if the modularity in tremor extended fully to the query language so that distinct subgraphs could be modularized and consumed by multiple queries to optimise for reuse of flow oriented logic in tremor.\n\n  This would require extending module support in the tremor query language to support sub-graph definitions with parameters that can be declared and used as part of a higher level query.\n\n  Modules in tremor-query in their current state: <https://docs.tremor.rs/tremor-query/modules/>\n\n  This project idea involves designing the sub-graph module syntax and semantics and implementing changes to the lexer, grammar, optimizers and runtime. It is most suited to candidates who are interested in programming language evolution and design.	{rust}	2021	Term 2	https://github.com/tremor-rs/tremor-runtime/issues/940	https://www.tremor.rs/	300000	108
532	c45cc842-278f-4663-9ff4-deecc3fc040d	CNCF - CNCF Landscape: UX UI improvement II	{"With your collaboration, we aim to analyze findings and meaningful information (quantitative and qualitative data) and run a series of ideation rounds. We will create user personas, empathy maps, and other UX deliverables that will be the foundation to lay out a set of solutions to improve the current way to search, navigate and find relevant information on the Landscape.\n- Expected Outcome: Creation user personas, empathy maps, and other UX deliverables."}	With your collaboration, we aim to analyze findings and meaningful information (quantitative and qualitative data) and run a series of ideation rounds. We will create user personas, empathy maps, and other UX deliverables that will be the foundation to lay out a set of solutions to improve the current way to search, navigate and find relevant information on the Landscape.\n- Expected Outcome: Creation user personas, empathy maps, and other UX deliverables.	{figma,prototyping}	2023	Term 2	https://github.com/cncf/landscape/issues/2467	https://landscape.cncf.io	300000	160
435	96080e3d-83e2-46ed-928c-b6e7f3154bf3	CNCF - CNCF TAG Network: Representing Kubernetes ontology in MeshModel	{"Network topologies and graph databases go hand-in-hand. The OpenAPI specifications for Kubernetes provides taxonomy, but augmenting a graph data model with formalized ontologies enables any number of capabilities, one of the more straightforward is the inferencing requisite for natural language processing, and consequently, a human-centric query "," response interaction becomes becomes possible. More importantly, more advanced systems can be built when a graph data model of connected systems is upgraded to be a knowledge semantic graph. Deliverables (among other items):\n\n- MeshModel capabilities browser\n- Import","export of MeshModel models and components as OCI images\n- augmentation of cuelang-based component generator"}	Network topologies and graph databases go hand-in-hand. The OpenAPI specifications for Kubernetes provides taxonomy, but augmenting a graph data model with formalized ontologies enables any number of capabilities, one of the more straightforward is the inferencing requisite for natural language processing, and consequently, a human-centric query / response interaction becomes becomes possible. More importantly, more advanced systems can be built when a graph data model of connected systems is upgraded to be a knowledge semantic graph. Deliverables (among other items):\n\n- MeshModel capabilities browser\n- Import/export of MeshModel models and components as OCI images\n- augmentation of cuelang-based component generator	{cuelang,golang,oci}	2023	Term 1	https://github.com/cncf/tag-network/issues/24	https://github.com/cncf/tag-network	300000	160
385	2f5582f4-6cfa-41af-88d2-2bfdd8768756	CNCF - TAG Contributor Strategy: Mentoring Workspaces	{"pair.sharing.io is a mentoring "," pair environment used by ii.nz that brings up clusters to co-learn and co-author via tmate+emacs and a live cluster with many features useful to cloud native development. However, while many folks find the ideas useful, it would be good to reach a wider audience by bringing up workspaces w"," VSCode as an alternative to emacs. The request is for a PoC deploying coder.com to CNCF Infrastructure (likely Packet) and bringing over some of the methods of collaboration learned by ii on pair to a wider audience."}	pair.sharing.io is a mentoring / pair environment used by ii.nz that brings up clusters to co-learn and co-author via tmate+emacs and a live cluster with many features useful to cloud native development. However, while many folks find the ideas useful, it would be good to reach a wider audience by bringing up workspaces w/ VSCode as an alternative to emacs. The request is for a PoC deploying coder.com to CNCF Infrastructure (likely Packet) and bringing over some of the methods of collaboration learned by ii on pair to a wider audience.	{shell,terminal,kubernetes}	2022	Term 3	https://github.com/sharingio/pair/issues/173	https://pair.sharing.io/	300000	160
366	df449a23-ac20-4ee9-8a2c-e0e5d08ba727	CNCF - TAG Network and Observability: Kubernetes ontology and subgraph module design	{"Network topologies and graph databases go hand-in-hand. The OpenAPI specifications for Kubernetes provides taxonomy, but augmenting a graph data model with formalized ontologies enables any number of capabilities, one of the more straightforward is the inferencing requisite for natural language processing, and consequently, a human-centric query "," response interaction becomes becomes possible. More importantly, more advanced systems can be built when a graph data model of connected systems is upgraded to be a knowledge semantic graph. Deliverables (among other items):\n\n- a Kubernetes ontology using OWL as a popular (and mature) way of doing this.\n- a cuelang-based component generator"}	Network topologies and graph databases go hand-in-hand. The OpenAPI specifications for Kubernetes provides taxonomy, but augmenting a graph data model with formalized ontologies enables any number of capabilities, one of the more straightforward is the inferencing requisite for natural language processing, and consequently, a human-centric query / response interaction becomes becomes possible. More importantly, more advanced systems can be built when a graph data model of connected systems is upgraded to be a knowledge semantic graph. Deliverables (among other items):\n\n- a Kubernetes ontology using OWL as a popular (and mature) way of doing this.\n- a cuelang-based component generator	{cuelang,go}	2022	Term 3	https://github.com/cncf/tag-network/issues/21	https://github.com/cncf/tag-network	0	160
244	8b542e66-e8e7-44fa-bb64-4ef88f815eb1	CNCF - Service Mesh Interface: Conformance Program	{"Ensure that a service mesh is properly configured and that its behavior conforms to official SMI specifications. Advance the definition of conformance tests and the test framework, Meshery (see [design specification](https:","",docs.google.com,document,d,1HL8Sk7NSLLj-9PRqoHYVIGyU6fZxUQFotrxbmfFtjwc,edit)).}	Ensure that a service mesh is properly configured and that its behavior conforms to official SMI specifications. Advance the definition of conformance tests and the test framework, Meshery (see [design specification](https://docs.google.com/document/d/1HL8Sk7NSLLj-9PRqoHYVIGyU6fZxUQFotrxbmfFtjwc/edit)).	{go,react}	2021	Term 3	https://smi-spec.io/	https://github.com/servicemeshinterface/smi-spec/issues/70	300000	119
291	74cd9106-ed71-427c-a4ec-1dafe39a73c9	CNCF - Service Mesh Interface: Conformance Program (extended)	{"Ensure that a service mesh is properly configured and that its behavior conforms to official SMI specifications. Advance the definition of conformance tests and the test framework, Meshery (see [design specification](https:","",docs.google.com,document,d,1HL8Sk7NSLLj-9PRqoHYVIGyU6fZxUQFotrxbmfFtjwc,edit)).}	Ensure that a service mesh is properly configured and that its behavior conforms to official SMI specifications. Advance the definition of conformance tests and the test framework, Meshery (see [design specification](https://docs.google.com/document/d/1HL8Sk7NSLLj-9PRqoHYVIGyU6fZxUQFotrxbmfFtjwc/edit)).	{go,react}	2022	Term 1	https://smi-spec.io/	https://github.com/servicemeshinterface/smi-spec/issues/70	300000	119
207	1866546e-6945-4388-bbdc-76a6e6570c23	COBOL Programming Course updates	{"The mentee will work on updating the Getting Started chapter with updates from Zowe, add new and updated content to the Advanced chapter, review issues, and work on incorporating those that will add value to the course offering and be part of the release team, releasing the next version of the course"}	The mentee will work on updating the Getting Started chapter with updates from Zowe, add new and updated content to the Advanced chapter, review issues, and work on incorporating those that will add value to the course offering and be part of the release team, releasing the next version of the course	{cobol}	2021	Term 2	https://github.com/openmainframeproject/cobol-programming-course	https://www.openmainframeproject.org/projects/cobolprogrammingcourse	660000	18
132	7deb8e48-6797-4115-bb96-9cf4ccee022e	Codeuino Mentorship	{"Codeuino is a volunteer-driven, open-source, social networking software development organization that desires the potential to change the way other communities and individuals use and create open-source social-environment tools to better align with end-user expectations, giving prospectus to various other organizations, users and activists to use the social environments built within Codeuino in a custom way.\nDonut is an open-source, feature-rich, highly flexible and privacy-friendly, social networking platform built for community-oriented collaboration in a customized way. It has been built on the Node.js framework allowing an essential impetus to provide custom and friendly rich widgets and an expansive library of modules to make communication and collaboration easy and successful. With a powerful module system, you can customize this platform by using third party tools, writing your own, or integrating other software."}	Codeuino is a volunteer-driven, open-source, social networking software development organization that desires the potential to change the way other communities and individuals use and create open-source social-environment tools to better align with end-user expectations, giving prospectus to various other organizations, users and activists to use the social environments built within Codeuino in a custom way.\nDonut is an open-source, feature-rich, highly flexible and privacy-friendly, social networking platform built for community-oriented collaboration in a customized way. It has been built on the Node.js framework allowing an essential impetus to provide custom and friendly rich widgets and an expansive library of modules to make communication and collaboration easy and successful. With a powerful module system, you can customize this platform by using third party tools, writing your own, or integrating other software.	{javascript,jquery,json,react,mongodb,oops}	2020	Term 2	https://github.com/codeuino	http://codeuino.org	900000	77
451	3dbf9c76-1147-4d49-b4dc-a982061f0b9c	Confidential Computing Fellowship	{"As organizations from different sectors move their computing workloads across multiple environments, from on-premises to public Cloud to Edge, they require greater assurances that their sensitive code and data are protected. Confidential Computing protects data in use by performing computation in a hardware-based Trusted Execution Environment (TEE).\n\nTo learn more about the Confidential Computing Fellowship, please check the guide below:\n\nhttps:","",enarx.dev,docs,Fellowship,"Introduction\n\nAlso, please join the #mentorship chat channel at:\n\nhttps:","",chat.enarx.dev,channel,mentorship}	As organizations from different sectors move their computing workloads across multiple environments, from on-premises to public Cloud to Edge, they require greater assurances that their sensitive code and data are protected. Confidential Computing protects data in use by performing computation in a hardware-based Trusted Execution Environment (TEE).\n\nTo learn more about the Confidential Computing Fellowship, please check the guide below:\n\nhttps://enarx.dev/docs/Fellowship/Introduction\n\nAlso, please join the #mentorship chat channel at:\n\nhttps://chat.enarx.dev/channel/mentorship	{rust,git,linux,security,webassembly}	2022	Term 1	https://github.com/confidential-computing/	https://confidentialcomputing.io	900000	157
185	beb3a680-3f78-4716-b072-7f547cf417ab	CNCF - CoreDNS: Add ACME protocol support for certificate management with DNS	{"CoreDNS (https:","",github.com,coredns,"coredns) is a cloud-native DNS server with a focus on service discovery. While best known as the default DNS server for Kubernetes, CoreDNS is capable of handle many other scenarios within or outside of Kubernetes clusters to make easy infrastructure management. One such case is certificate management. This project is to provide ACME protocol support so that it is possible to have automatic certificate management through CoreDNS. More details and discussions are available in https:","",github.com,coredns,coredns,issues,3460.}	CoreDNS (https://github.com/coredns/coredns) is a cloud-native DNS server with a focus on service discovery. While best known as the default DNS server for Kubernetes, CoreDNS is capable of handle many other scenarios within or outside of Kubernetes clusters to make easy infrastructure management. One such case is certificate management. This project is to provide ACME protocol support so that it is possible to have automatic certificate management through CoreDNS. More details and discussions are available in https://github.com/coredns/coredns/issues/3460.	{go}	2021	Term 2	https://github.com/coredns/coredns/issues/3460	https://coredns.io/	360000	104
515	dd10bf62-53d1-4a96-bea2-65bbb78bd10e	CNCF - CoreDNS: Add DNS-over-QUIC (DoQ) and/or DNS-over-HTTP/3 (DoH3) support	{"DNS-over-QUIC (DoQ) and DNS-over-HTTP","3 (DoH3) are relatively new protocols for transmitting DNS queries with security and privacy. Additionally, DoQ and DoH3 also offers other benefits such as improved latency and better error detection. The goal of this proposal is to add DoQ and","or DoH3 support to CoreDNS.\n\nExpected Outcome: An implementation of DoQ or DoH3 for CoreDNS. A stretch goal of adding both DoQ and DoH3 is also within scope.\n- Recommended Skills: Golang, DNS.\n- Mentor(s): Yong Tang @yongtang (yong.tang.github@outlook.com); Chris O'Haver @chrisohaver (cohaver@infoblox.com)\n- Upstream Issue (URL): https:","",github.com,coredns,coredns,issues,"5583, https:","",github.com,coredns,coredns,issues,5539}	DNS-over-QUIC (DoQ) and DNS-over-HTTP/3 (DoH3) are relatively new protocols for transmitting DNS queries with security and privacy. Additionally, DoQ and DoH3 also offers other benefits such as improved latency and better error detection. The goal of this proposal is to add DoQ and/or DoH3 support to CoreDNS.\n\nExpected Outcome: An implementation of DoQ or DoH3 for CoreDNS. A stretch goal of adding both DoQ and DoH3 is also within scope.\n- Recommended Skills: Golang, DNS.\n- Mentor(s): Yong Tang @yongtang (yong.tang.github@outlook.com); Chris O'Haver @chrisohaver (cohaver@infoblox.com)\n- Upstream Issue (URL): https://github.com/coredns/coredns/issues/5583, https://github.com/coredns/coredns/issues/5539	{go,dns}	2023	Term 2	https://github.com/coredns/coredns/issues/5583	https://coredns.io/	360000	104
417	184ccb3e-6abe-4bf9-9659-b42b5c07c5a5	CNCF - Cortex: API to import Prometheus & Thanos blocks	{"Description: For users who want to migrate from Prometheus to Cortex, currently it is supported via a tool called [Thanosconvert](https:","",cortexmetrics.io,docs,blocks-storage,migrate-storage-from-thanos-and-prometheus,"#when-migrating-from-prometheus). However, having this feature as part of the tool is limited in some usecase like SaaS because users usually don’t have permissions to access their storage layer directly. It would be nice to extend this feature into an API so that users can import their Prometheus TSDB compatible blocks for easier migration.\n\nExpected Outcome: An API that imports Prometheus blocks into Cortex."}	Description: For users who want to migrate from Prometheus to Cortex, currently it is supported via a tool called [Thanosconvert](https://cortexmetrics.io/docs/blocks-storage/migrate-storage-from-thanos-and-prometheus/#when-migrating-from-prometheus). However, having this feature as part of the tool is limited in some usecase like SaaS because users usually don’t have permissions to access their storage layer directly. It would be nice to extend this feature into an API so that users can import their Prometheus TSDB compatible blocks for easier migration.\n\nExpected Outcome: An API that imports Prometheus blocks into Cortex.	{go,prometheus,thanos}	2023	Term 1	https://github.com/cortexproject/cortex/issues/4956	https://cortexmetrics.io/	300000	57
186	faccbbff-f651-4bc5-a825-be0f395315c9	CNCF - Cortex: Cue support and validation for the Cortex config	{"- Description: [Cortex](https:","",github.com,cortexproject,"cortex) is a\n  cloud-native Prometheus compatible monitoring system. It is made up of a set\n  of microservices that can be composed into an architecture that fits multiple\n  use cases. However, this level of flexibility can lead to complexity in the\n  configuration file. One way to handle this complexity is first-class\n  validation support for the config. This is where [Cue](https:","",cuelang.org,")\n  comes in. Cue provides data validation as a language feature and has solid\n  support for Go. We think enabling Cortex to be configured using Cue and\n  creating a Cue specification for the Cortex configuration file and other file\n  types specific to Cortex would be a good step forward in improving the\n  usability of the project."}	- Description: [Cortex](https://github.com/cortexproject/cortex) is a\n  cloud-native Prometheus compatible monitoring system. It is made up of a set\n  of microservices that can be composed into an architecture that fits multiple\n  use cases. However, this level of flexibility can lead to complexity in the\n  configuration file. One way to handle this complexity is first-class\n  validation support for the config. This is where [Cue](https://cuelang.org/)\n  comes in. Cue provides data validation as a language feature and has solid\n  support for Go. We think enabling Cortex to be configured using Cue and\n  creating a Cue specification for the Cortex configuration file and other file\n  types specific to Cortex would be a good step forward in improving the\n  usability of the project.	{go}	2021	Term 2	https://github.com/cortexproject/cortex/issues/4095	https://cortexmetrics.io/	300000	57
425	820f9269-ddef-44e9-bf77-95a8d2444c1e	CNCF - Cortex: Experimental Auth Gateway	{"Description: Cortex server has a simple authentication mechanism (X-Scope-OrgId) but users can’t use the multi tenancy features out of the box without complicated proxy configuration. It’s hard to support all the different authentication mechanisms used by different companies but plan to have a simple but opinionated auth-gateway that provides value out of the box.\n\nExpected Outcome: A new experimental cortex component called auth-gateway that validates tenants requests and proxies valid requests to distributors and query-frontend."}	Description: Cortex server has a simple authentication mechanism (X-Scope-OrgId) but users can’t use the multi tenancy features out of the box without complicated proxy configuration. It’s hard to support all the different authentication mechanisms used by different companies but plan to have a simple but opinionated auth-gateway that provides value out of the box.\n\nExpected Outcome: A new experimental cortex component called auth-gateway that validates tenants requests and proxies valid requests to distributors and query-frontend.	{go}	2023	Term 1	https://github.com/cortexproject/cortex/issues/5106	https://cortexmetrics.io/	300000	57
87	38bb5e81-2bca-42f5-b31d-3bcd2da732d3	Cortex	{"Cortex provides horizontally scalable, multi-tenant, long term storage for Prometheus metrics when used as a remote write destination, and a horizontally scalable, Prometheus-compatible query API."}	Cortex provides horizontally scalable, multi-tenant, long term storage for Prometheus metrics when used as a remote write destination, and a horizontally scalable, Prometheus-compatible query API.	{go}	2020	Term 1	https://github.com/cortexproject/cortex	https://cortexmetrics.io/	550000	57
618	99274b1a-694b-4af5-b7ca-39311f38a646	CNCF - CRI-O: Add additional log drivers to conmon-rs	{"conmon-rs is a container monitor written in Rust, used by CRI-O to monitor a container's lifecycle. Part of its responsibilities is log forwarding--taking the output of the container and writing that output to various places. Currently, conmon-rs supports one format: the one required by the Kubernetes CRI. The goal of this proposal is to add new log formats from the list of standardized ones, like JSON, Splunk, Journald.\n- Expected outcome: A JSON log driver and Journald log driver are added to conmon-rs. A stretch goal of adding a Splunk log driver is also within scope."}	conmon-rs is a container monitor written in Rust, used by CRI-O to monitor a container's lifecycle. Part of its responsibilities is log forwarding--taking the output of the container and writing that output to various places. Currently, conmon-rs supports one format: the one required by the Kubernetes CRI. The goal of this proposal is to add new log formats from the list of standardized ones, like JSON, Splunk, Journald.\n- Expected outcome: A JSON log driver and Journald log driver are added to conmon-rs. A stretch goal of adding a Splunk log driver is also within scope.	{rust,containers}	2023	Term 3	https://github.com/containers/conmon-rs/issues/1126	https://cri-o.io	0	183
620	cb189d71-3943-450a-9d5f-d71bd66d73c9	CNCF - CRI-O: CRI stats KEP	{"[CRI stats KEP](https:","",github.com,kubernetes,enhancements,issues,"2371) is an effort to take the container stats and metrics collection from cAdvisor and move it to the CRI implementations. CRI-O will soon have support for stats and metrics collected through CRI, but work needs to be done to verify and validate these fields, and make sure their collection is performant as possible.\n- Expected outcome: A test suite verifying the correctness of CRI-O's stats and metrics collection, as well as data verifying performance regressions are minimal at worst."}	[CRI stats KEP](https://github.com/kubernetes/enhancements/issues/2371) is an effort to take the container stats and metrics collection from cAdvisor and move it to the CRI implementations. CRI-O will soon have support for stats and metrics collected through CRI, but work needs to be done to verify and validate these fields, and make sure their collection is performant as possible.\n- Expected outcome: A test suite verifying the correctness of CRI-O's stats and metrics collection, as well as data verifying performance regressions are minimal at worst.	{containers,go}	2023	Term 3	https://github.com/cri-o/cri-o/issues/7175	https://cri-o.io	0	183
156	2e2239cf-2964-434f-a5c9-676b857fc29a	CNCF - Crossplane: Automated end-to-end testing infrastructure	{"Crossplane provides a broad library of Kubernetes custom resources that let you orchestrate systems external to Kubernetes. These include AWS S3 buckets, GCP CloudSQL instances, Azure Cosmos tables, plain old SQL databases, Helm releases, and Dominos pizzas. We call these 'managed resources'. Crossplane's goal is to allow platform teams to build their own custom resources that are in turn composed of these primitives without needing to write Kubernetes controllers in Go. Crossplane currently has extensive unit testing, but not much in the way of automated integration","e2e tests. We have a very broad surface area to test (we have around a hundred controllers that interact with cloud providers) and would like to establish some integration testing best practices so that the community can easily contribute integration tests when they work on Crossplane."}	Crossplane provides a broad library of Kubernetes custom resources that let you orchestrate systems external to Kubernetes. These include AWS S3 buckets, GCP CloudSQL instances, Azure Cosmos tables, plain old SQL databases, Helm releases, and Dominos pizzas. We call these 'managed resources'. Crossplane's goal is to allow platform teams to build their own custom resources that are in turn composed of these primitives without needing to write Kubernetes controllers in Go. Crossplane currently has extensive unit testing, but not much in the way of automated integration/e2e tests. We have a very broad surface area to test (we have around a hundred controllers that interact with cloud providers) and would like to establish some integration testing best practices so that the community can easily contribute integration tests when they work on Crossplane.	{go}	2021	Term 1	https://github.com/crossplane/crossplane/issues/1033	https://crossplane.io/	300000	91
344	c6e63427-09d9-42e5-b2af-c9c66e57881a	CNCF - Crossplane: Document and add automated testing for pulling packages from private registries	{"Crossplane supports pulling packages from private registries through a variety of mechanisms, including IRSA, Workload Identity, and packagePullSecrets. There are a wide variety of environments in which a user is pulling a private package, which can lead to confusion about which to use, the precedence with which each is invoked, etc. This can lead to issues such as crossplane","crossplane#2876, where I suspect that we are resolving to Application Default Credentials, when we actually want to be using the provided packagePullSecret."}	Crossplane supports pulling packages from private registries through a variety of mechanisms, including IRSA, Workload Identity, and packagePullSecrets. There are a wide variety of environments in which a user is pulling a private package, which can lead to confusion about which to use, the precedence with which each is invoked, etc. This can lead to issues such as crossplane/crossplane#2876, where I suspect that we are resolving to Application Default Credentials, when we actually want to be using the provided packagePullSecret.	{go}	2022	Term 2	https://github.com/crossplane/crossplane/issues/2913	https://crossplane.io	300000	91
329	06fe8a5b-c3b9-4d12-975f-19f5ec49d006	CNCF - Crossplane: Report breaking changes in CustomResourceDefinition schemas for Pull Requests	{"As Crossplane ecosystem expands, it's not unusual anymore to have 100s of CustomResourceDefinitions in a provider. A big part of this is thanks to code generation tooling we've been building. However, the changes we make with those tools may result in thousands of lines of changes and it's hard to tell whether there is a breaking change in given CustomResourceDefinition in those PRs. Today, we're checking manually to see if there is a breaking change and mark the PR as such so that the notice ends up in the release notes. We can have a GH Action that processes diff in package","crds folder and report breaking changes in the OpenAPI v3 Schemas of CRDs. It'd comment on the PR and automatically label it with breaking-change. This tool would be useful for the whole Kubernetes community that works to extend Kubernetes with CustomResourceDefinitions. We should document the variety of ways to pull packages from popular private registries, and also have testing in place to ensure we don't break any of the mechanisms from release to release."}	As Crossplane ecosystem expands, it's not unusual anymore to have 100s of CustomResourceDefinitions in a provider. A big part of this is thanks to code generation tooling we've been building. However, the changes we make with those tools may result in thousands of lines of changes and it's hard to tell whether there is a breaking change in given CustomResourceDefinition in those PRs. Today, we're checking manually to see if there is a breaking change and mark the PR as such so that the notice ends up in the release notes. We can have a GH Action that processes diff in package/crds folder and report breaking changes in the OpenAPI v3 Schemas of CRDs. It'd comment on the PR and automatically label it with breaking-change. This tool would be useful for the whole Kubernetes community that works to extend Kubernetes with CustomResourceDefinitions. We should document the variety of ways to pull packages from popular private registries, and also have testing in place to ensure we don't break any of the mechanisms from release to release.	{go}	2022	Term 2	https://github.com/crossplane/crossplane/issues/2863	https://crossplane.io/	300000	91
295	7da32609-7541-44ce-9534-804cad2ff219	Design and implement a Linux abstraction layer for protected read-only memory	{"To store immutable data safely, we wish to provide a Linux API for a constant block which is initialized on Linux init and is henceforth limited to read-only access by users (root or non-privileged).  The project will focus on design of 3 types of solutions, and implementation of one model only:\n- Pure software (architecture independent)\n- Combined software "," hardware dependent\n- Pure architecture (hardware) dependent\n\nLearning Objectives\n- An opportunity to work with and learn from experts in Linux kernel development and designers of safety critical systems.\n- An entry point into Linux kernel development.\n- Bridging the gap from academic "," “toy” applications, to programming in the real world, with its constraints and expectations. \n\nExpected Outcomes\n- In-depth analysis of the problem and solution space.\n- Provide detailed design for each of the 3 models, including advantages and limitations of each model.\n- Implement one of the designs, with a clear justification for the choice."}	To store immutable data safely, we wish to provide a Linux API for a constant block which is initialized on Linux init and is henceforth limited to read-only access by users (root or non-privileged).  The project will focus on design of 3 types of solutions, and implementation of one model only:\n- Pure software (architecture independent)\n- Combined software / hardware dependent\n- Pure architecture (hardware) dependent\n\nLearning Objectives\n- An opportunity to work with and learn from experts in Linux kernel development and designers of safety critical systems.\n- An entry point into Linux kernel development.\n- Bridging the gap from academic / “toy” applications, to programming in the real world, with its constraints and expectations. \n\nExpected Outcomes\n- In-depth analysis of the problem and solution space.\n- Provide detailed design for each of the 3 models, including advantages and limitations of each model.\n- Implement one of the designs, with a clear justification for the choice.	{linux,programming}	2022	Term 2	https://github.com/torvalds/linux		600000	133
335	ef552046-cf54-4fe1-ac31-0a1014210e15	CNCF - Devfile: Add Compose file support in the spec API	{"Devfiles are YAML files that define development environment running in the cloud. The main part of a Devfile is the components section and specify the containers required to code, build and test an application. The Devfile can either include those containers defintions or reference external files such as Dockerfiles or Kubernetes manifests. The Compose file is a popular format in open source development projects to define runtime environments for testing the application but those cannot be referenced by a Devfile yet. The goal is to update the API specification to allow referencing a Compose file from a Devfile and to implement the support in the Devfile library."}	Devfiles are YAML files that define development environment running in the cloud. The main part of a Devfile is the components section and specify the containers required to code, build and test an application. The Devfile can either include those containers defintions or reference external files such as Dockerfiles or Kubernetes manifests. The Compose file is a popular format in open source development projects to define runtime environments for testing the application but those cannot be referenced by a Devfile yet. The goal is to update the API specification to allow referencing a Compose file from a Devfile and to implement the support in the Devfile library.	{go,compose,kubernetes}	2022	Term 2	https://github.com/devfile/api/issues/501	https://devfile.io/	300000	12
18	3feec75a-3d80-476a-83ab-89ee90f48aad	CNCF - Devfile: Add some syntax sugar to speficy the components that are deployed at startup and tho	{"Devfiles are YAML files that define development environment running in the cloud. The main part of a Devfile is the components section and specify the containers required to code, build and test an application. Some components, such as those to code and build the application, need to be deployed as soon as development environment is provisioned. Others instead are supposed to be started later, usually when a command is triggered by the developer to test the applicaiton she is working on (a database for example). The current definition of the latter type of coponents is complicated and not self explanatory. The goal of this project is to add a new component field to specify if the component should be included at startup or not."}	Devfiles are YAML files that define development environment running in the cloud. The main part of a Devfile is the components section and specify the containers required to code, build and test an application. Some components, such as those to code and build the application, need to be deployed as soon as development environment is provisioned. Others instead are supposed to be started later, usually when a command is triggered by the developer to test the applicaiton she is working on (a database for example). The current definition of the latter type of coponents is complicated and not self explanatory. The goal of this project is to add a new component field to specify if the component should be included at startup or not.	{go,kubernetes}	2022	Term 2	https://github.com/devfile/api/issues/852	https://devfile.io/	300000	12
498	143b8fc5-5e32-4013-b8a6-a63adef6307a	Analysis of eBPF (extended Berkley Packet Filter) Verifier	{"To make eBPF programs “safe”, the Linux kernel validates all eBPF code before loading.  However, the current validator has many known limitations, leading to rejection of working programs.  \n  \nFocus in this project will be:\n- In-depth analysis and review of the eBPF validator, and its use to validate eBPF programs.  \n- Code enhancements to the validator to improve usability.\n- Identify use cases for kernel profiling in safety critical applications\n\nAdditional information:\n- https:","",elisaworkshopfall2021.sched.com,event,p18L,"ebpf-verifier-lessons-learned-for-safety-elana-copperman-mobileye-intel?iframe=no\n- https:","",ish-ar.io,ebpf-dive-into-the-verifier,"\n- https:","",github.com,torvalds,linux,blob,master,kernel,bpf,"verifier.c\n\nExpected Outcomes:\nAnalysis of eBPF verifier source code and design, proposals for improvement and application for safety-critical applications"}	To make eBPF programs “safe”, the Linux kernel validates all eBPF code before loading.  However, the current validator has many known limitations, leading to rejection of working programs.  \n  \nFocus in this project will be:\n- In-depth analysis and review of the eBPF validator, and its use to validate eBPF programs.  \n- Code enhancements to the validator to improve usability.\n- Identify use cases for kernel profiling in safety critical applications\n\nAdditional information:\n- https://elisaworkshopfall2021.sched.com/event/p18L/ebpf-verifier-lessons-learned-for-safety-elana-copperman-mobileye-intel?iframe=no\n- https://ish-ar.io/ebpf-dive-into-the-verifier/\n- https://github.com/torvalds/linux/blob/master/kernel/bpf/verifier.c\n\nExpected Outcomes:\nAnalysis of eBPF verifier source code and design, proposals for improvement and application for safety-critical applications	{c,linux}	2022	Term 1	https://github.com/torvalds/linux/blob/master/kernel/bpf/verifier.c	https://elisa.tech/	1140000	36
56	33e091d5-a371-487a-8b7a-930287bbb130	Code Coverage Metrics for GLibC	{"ISO 26262, requires the measurement of structural coverage (=code coverage) and explains the importance of this method for the development of safety-related SW.\n \nThere are commercial and open-source tools that measure code coverage for user space application and for the Linux kernel but an important part is missing – code coverage for GLibC. This is a crucial gap for a safety case of Linux because many of the kernel’s core services (memory management, thread management, locking) are implemented in GLibC with complex code.\n \nCurrently building glibc with gcc and code coverage instrumentation is not possible. The last reference on the internet to this problem is from seven years ago (https:","",libc-help.sourceware.narkive.com,WT9CfAx8,"glibc-build-with-gcov-support).\n \nThe aim of this project is to find a way to perform code coverage (C0, C1 and C2) on GlibC.\n \nVarious solutions can be explored:\n1. \tPatches to gcc code\n2. \tPatches to glibc code\n3. \tModification of glibc code at compile time (macros or GCC C Plugin or compiler parameters)\n4. \tA new tool\n \n \nWhat’s in it for you?\nUnderstand code coverage metrics and how they are measured\nGain a deep understanding of the internals of the GCC compiler\nAn opportunity to work with and learn from experts in Linux kernel development and designers of safety critical systems.\nAn entry point into Linux kernel development."}	ISO 26262, requires the measurement of structural coverage (=code coverage) and explains the importance of this method for the development of safety-related SW.\n \nThere are commercial and open-source tools that measure code coverage for user space application and for the Linux kernel but an important part is missing – code coverage for GLibC. This is a crucial gap for a safety case of Linux because many of the kernel’s core services (memory management, thread management, locking) are implemented in GLibC with complex code.\n \nCurrently building glibc with gcc and code coverage instrumentation is not possible. The last reference on the internet to this problem is from seven years ago (https://libc-help.sourceware.narkive.com/WT9CfAx8/glibc-build-with-gcov-support).\n \nThe aim of this project is to find a way to perform code coverage (C0, C1 and C2) on GlibC.\n \nVarious solutions can be explored:\n1. \tPatches to gcc code\n2. \tPatches to glibc code\n3. \tModification of glibc code at compile time (macros or GCC C Plugin or compiler parameters)\n4. \tA new tool\n \n \nWhat’s in it for you?\nUnderstand code coverage metrics and how they are measured\nGain a deep understanding of the internals of the GCC compiler\nAn opportunity to work with and learn from experts in Linux kernel development and designers of safety critical systems.\nAn entry point into Linux kernel development.	{compiler,linux,kernel,english}	2021	Term 1	https://www.kernel.org/	https://www.kernel.org/	300000	36
182	4e97be2e-271c-445b-8c5d-a59ac85d35fb	GenevaERS Demo System	{"Assist in development of the initial GenevaERS demo system, including building views, test processes, procedures to allow initial users to download and use GenevaERS successfully."}	Assist in development of the initial GenevaERS demo system, including building views, test processes, procedures to allow initial users to download and use GenevaERS successfully.	{java}	2021	Term 2	https://github.com/genevaers/community	https://genevaers.org/	300000	102
122	903b8a9d-b4f5-48be-865f-193f8a048885	GraphQL Mentorship	{"GraphQL is a query language for APIs and a runtime for fulfilling those queries with your existing data. GraphQL provides a complete and understandable description of the data in your API, gives clients the power to ask for exactly what they need and nothing more, makes it easier to evolve APIs over time, and enables powerful developer tools."}	GraphQL is a query language for APIs and a runtime for fulfilling those queries with your existing data. GraphQL provides a complete and understandable description of the data in your API, gives clients the power to ask for exactly what they need and nothing more, makes it easier to evolve APIs over time, and enables powerful developer tools.	{graphql}	2020	Term 3	https://github.com/graphql	https://graphql.org	300000	68
215	b96ed4f4-a4e4-477a-abbf-156f417f933e	Environmentally Sensitive Growth Module and Online Courses for Forest Greenhouse Gas Estimates	{"Climate change is the biggest challenge of our time. We need all the help we can get to reduce emissions in every sector. The land sector is particularly important because there are not only emissions but also to potential for removals of greenhouse gases from the atmosphere. The Full Lands INtegration Tool (FLINT) is a platform for estimating greenhouse gas emissions and removals from land. This project:\n\nDevelops new modules that represent the impacts of environmental changes on growth and mortality rates. Data from South Korea is used as a pilot. Scientifically, the key challenge is to translate information about tree-level responses to environmental changes to stand-level responses. \n\nDevelops online training in response to COVID-19 to replace classroom training. A coherent approach to online instruction needs to be designed and the existing materials need to be turned into online ready versions."}	Climate change is the biggest challenge of our time. We need all the help we can get to reduce emissions in every sector. The land sector is particularly important because there are not only emissions but also to potential for removals of greenhouse gases from the atmosphere. The Full Lands INtegration Tool (FLINT) is a platform for estimating greenhouse gas emissions and removals from land. This project:\n\nDevelops new modules that represent the impacts of environmental changes on growth and mortality rates. Data from South Korea is used as a pilot. Scientifically, the key challenge is to translate information about tree-level responses to environmental changes to stand-level responses. \n\nDevelops online training in response to COVID-19 to replace classroom training. A coherent approach to online instruction needs to be designed and the existing materials need to be turned into online ready versions.	{r,moodle}	2020	Term 3	https://github.com/moja-global/LINUX.FLINT.Environmentally_sensitive_forest_module	moja.global	780000	115
359	16d86d3e-57cd-4d15-8b59-25089f3ec6fc	Google Season of Documentation	{"Help us fight climate change through nature-based solutions!\n\nThe Full Lands INtegration Tool (FLINT) is a platform for estimating greenhouse gas emissions at local, national and global scales.\n\nThis project aims to improve moja global's documentation by building a community website and adopting consistent standards across our projects."}	Help us fight climate change through nature-based solutions!\n\nThe Full Lands INtegration Tool (FLINT) is a platform for estimating greenhouse gas emissions at local, national and global scales.\n\nThis project aims to improve moja global's documentation by building a community website and adopting consistent standards across our projects.	{documentation}	2022	Term 2	https://github.com/moja-global/mentorship/tree/main/google-season-of-docs	moja.global	1215000	140
393	926665ac-9b96-45aa-bb11-5d99096be870	Using Machine Learning to Predict Deforestation	{"Climate change is the biggest challenge of our time. We need all the help we can get to reduce emissions in every sector. Forest are particularly important as they can remove CO2 from the atmosphere. \n\nMoja global's software Full Lands INtegration Tool (FLINT) estimates greenhouse gas fluxes from land use. \n\nProgress means that emissions today are lower than the baseline, i.e. emissions we would have observed had we done nothing. A baseline  requires predicting the rate of deforestation in case we had done nothing. \n\nSo we need a formula linked to local proxies that predicts deforestation locally, but that can be aggregated to the level of a state, province or country without losing consistency. Proxies can be location of hamlets, roads, agriculture, recent deforestation, etc. Most approaches use social research and linear programming to develop an algorithm that predicts deforestation. \n\nThis project proposes to develop the algorithm based on machine learning."}	Climate change is the biggest challenge of our time. We need all the help we can get to reduce emissions in every sector. Forest are particularly important as they can remove CO2 from the atmosphere. \n\nMoja global's software Full Lands INtegration Tool (FLINT) estimates greenhouse gas fluxes from land use. \n\nProgress means that emissions today are lower than the baseline, i.e. emissions we would have observed had we done nothing. A baseline  requires predicting the rate of deforestation in case we had done nothing. \n\nSo we need a formula linked to local proxies that predicts deforestation locally, but that can be aggregated to the level of a state, province or country without losing consistency. Proxies can be location of hamlets, roads, agriculture, recent deforestation, etc. Most approaches use social research and linear programming to develop an algorithm that predicts deforestation. \n\nThis project proposes to develop the algorithm based on machine learning.	{statistics}	2020	Term 3	https://github.com/moja-global/predictive-deforestation	https://moja.global/	1950500	140
455	7e8cb88a-5b37-471c-8db8-e11907b5a661	CNCF - Harbor: An official Golang API client and CLI for Harbor	{"Description: Design, plan and implement an Golang API client for Harbor\n\nExpected Outcome: Working golang harbor API client which can be used in the CI","CD implementations which compliments the Web UI, well documented and with the coresponding architectural diagrams under the Harbor org(not necessary to be complete functionality)"}	Description: Design, plan and implement an Golang API client for Harbor\n\nExpected Outcome: Working golang harbor API client which can be used in the CI/CD implementations which compliments the Web UI, well documented and with the coresponding architectural diagrams under the Harbor org(not necessary to be complete functionality)	{golang}	2023	Term 1	https://github.com/search?q=Harbor%20CLI&type=repositories	https://goharbor.io	300000	158
454	7ea4c506-c830-4a15-be4a-600d2dfe3f44	CNCF - Harbor: Implement per project for the whole instance vulnerability overview	{"Description: Design, plan and implement an and UI and backend to be able to visualize per project and","or for the registry vulnerability overview which will allow better security audits and vulenrability mitigation \n\nExpected Outcome: Addition to the Web UI which can be used to represent in full for the whole Harbor instance or per project the vulnerability status of the images, which will allow Harbor admin or project admin to get an overview of the existing vulnerabilities on in the images, also to provide capability to export the data via the CVE exporter so it can be consumed in 3rd party tools(not necessary to be complete functionality)\n\nUpstream Issues: \nhttps:","",github.com,goharbor,harbor,issues,"16680\nhttps:","",github.com,goharbor,harbor,issues,"10496\nhttps:","",dso.docker.com,explore?search=pkgs}	Description: Design, plan and implement an and UI and backend to be able to visualize per project and/or for the registry vulnerability overview which will allow better security audits and vulenrability mitigation \n\nExpected Outcome: Addition to the Web UI which can be used to represent in full for the whole Harbor instance or per project the vulnerability status of the images, which will allow Harbor admin or project admin to get an overview of the existing vulnerabilities on in the images, also to provide capability to export the data via the CVE exporter so it can be consumed in 3rd party tools(not necessary to be complete functionality)\n\nUpstream Issues: \nhttps://github.com/goharbor/harbor/issues/16680\nhttps://github.com/goharbor/harbor/issues/10496\nhttps://dso.docker.com/explore?search=pkgs	{angular,golang,javascript,clarity}	2023	Term 1	https://github.com/goharbor/harbor/issues/16680	https://goharbor.io	300000	158
203	b282b36d-dbf4-43df-bb67-7626898176b7	Hypereldger - Support Clique for Besu on HL Labs BAF	{"Hyperledger Labs Blockchain Automation Framework(BAF) is a tool to deploy different DLT platforms automatically on a given Kubernetes cluster. BAF supports multi-cloud and multi-DLT deployments, and already supports HL Fabric, HL Besu, Quorum, R3 Corda. For HL Besu, currently only IBFT2 Consensus is supported by BAF.\n\nWith this internship program we want to support the Clique consensus for Besu, so that BAF can be used to deploy and operate a HL Besu network with Clique consensus. This will also include upgrading BAF to support the latest stable Besu version.\n\nThe mentee will be able to learn:\n1) Production grade architecture.\n2) evOps in Blockchain Development.\n3) Architecture of Besu and BAF.\n4) work in true spirit of open-source, communicating with both Besu and BAF community, joining calls and using other community tools.\n5) writing documentation, so anyone in the community could use the results of their work.\n6) following rules and standards of open-source projects created by Hyperledger.\n\nExpected Outcome\n1) Documented, ready-to-use Besu Clique consensus on BAF.\n2) Documented upgrade of Besu to latest stable on BAF."}	Hyperledger Labs Blockchain Automation Framework(BAF) is a tool to deploy different DLT platforms automatically on a given Kubernetes cluster. BAF supports multi-cloud and multi-DLT deployments, and already supports HL Fabric, HL Besu, Quorum, R3 Corda. For HL Besu, currently only IBFT2 Consensus is supported by BAF.\n\nWith this internship program we want to support the Clique consensus for Besu, so that BAF can be used to deploy and operate a HL Besu network with Clique consensus. This will also include upgrading BAF to support the latest stable Besu version.\n\nThe mentee will be able to learn:\n1) Production grade architecture.\n2) evOps in Blockchain Development.\n3) Architecture of Besu and BAF.\n4) work in true spirit of open-source, communicating with both Besu and BAF community, joining calls and using other community tools.\n5) writing documentation, so anyone in the community could use the results of their work.\n6) following rules and standards of open-source projects created by Hyperledger.\n\nExpected Outcome\n1) Documented, ready-to-use Besu Clique consensus on BAF.\n2) Documented upgrade of Besu to latest stable on BAF.	{ansible,molecule,helmcharts,kubernetes,gitops}	2021	Term 2	https://github.com/hyperledger-labs/blockchain-automation-framework	https://wiki.hyperledger.org/display/INTERN/Support+Clique+for+Besu+on+HL+Labs+BAF	300000	110
183	067650de-5dbb-4bbd-a5b3-fc3310fa230f	Hyperledger Cactus - Cactus-samples - Business Logic Plugins for Hyperledger Cactus	{"The Hyperledger Greenhouse,  powered by the Hyperledger Foundation, provides several Business Blockchain Frameworks & Tools, some of which compose the state of the art private blockchains. In particular, Hyperledger Fabric is amongst the most famous permissioned blockchains; Hyperledger Indy tackles the first big initiative to promote decentralized identity support; Hyperledger Caliper is a comprehensive blockchain-testing framework; Hyperledger Quilt enables a specific type of blockchain interoperability - enables payments across any payment network — fiat or crypto - using the Interledger protocol.\n\nCactus is a blockchain integration tool designed to allow users to securely integrate different blockchains, promoting blockchain interoperability. This way, Hyperledger Cactus aims to provide Decentralized, Secure, and Adaptable Integration between Blockchain Networks. Hyperledger Cactus is currently undergoing a major refactoring effort to enable the desired to-be architecture which will enable plug-in based collaborative development to increase the breadth of use cases & Ledgers supported.\n\nTo fully explore Cactus’ capabilities, and to promote the project’s adoption, one needs several business logic plugins that realize cross-blockchain use cases (similarly to fabric-samples). Many are proposed on the whitepaper, but few are implemented."}	The Hyperledger Greenhouse,  powered by the Hyperledger Foundation, provides several Business Blockchain Frameworks & Tools, some of which compose the state of the art private blockchains. In particular, Hyperledger Fabric is amongst the most famous permissioned blockchains; Hyperledger Indy tackles the first big initiative to promote decentralized identity support; Hyperledger Caliper is a comprehensive blockchain-testing framework; Hyperledger Quilt enables a specific type of blockchain interoperability - enables payments across any payment network — fiat or crypto - using the Interledger protocol.\n\nCactus is a blockchain integration tool designed to allow users to securely integrate different blockchains, promoting blockchain interoperability. This way, Hyperledger Cactus aims to provide Decentralized, Secure, and Adaptable Integration between Blockchain Networks. Hyperledger Cactus is currently undergoing a major refactoring effort to enable the desired to-be architecture which will enable plug-in based collaborative development to increase the breadth of use cases & Ledgers supported.\n\nTo fully explore Cactus’ capabilities, and to promote the project’s adoption, one needs several business logic plugins that realize cross-blockchain use cases (similarly to fabric-samples). Many are proposed on the whitepaper, but few are implemented.	{blockchain,typescript,research,teamwork}	2021	Term 2	https://github.com/hyperledger/cactus	https://wiki.hyperledger.org/display/INTERN/Cactus-samples+-+Business+Logic+Plugins+for+Hyperledger+Cactus	300000	110
172	d43582b5-21b6-427f-b4cb-bf79afc12b34	Hyperledger Caliper - Declarative workload behavior definition for Hyperledger Caliper	{"Hyperledger Caliper is a general-purpose benchmarking tool with the goal of mitigating the aforementioned aspects of performance benchmarking. It allows the implementation","plug-in of custom workload behaviors to meet the diverse criteria of a wide range of business scenarios.\n\nImplementing a workload module for Caliper currently has some limitations and usability obstacles.\n\nThe goal of the project is to provide means for defining complex workload behaviors in Caliper in a declarative manner, using solely","mostly configuration files.\n\nThis should be accomplished with a general built-in workload module implementation that acts according to a configuration-based workload specification.\n\nExpected Outcome\nThe mentee must complete the following tasks by the end of the internship:\n1) Survey the workload definition capabilities of relevant workload generators","benchmark tools and typical workload requirements of relevant standards or existing workloads.\n2) Specify","design a flexible and extendable YAML-based configuration schema for the declarative definition of complex workloads.\n3) Implement a built-in Caliper workload module capable of generating requests based on the above configuration.\n4) Thoroughly test the implemented module.\n5) Provide developer and user documentation for the schema and the implemented module.\n6) Port some of the Caliper microbenchmarks to the declarative schema as proof of concept."}	Hyperledger Caliper is a general-purpose benchmarking tool with the goal of mitigating the aforementioned aspects of performance benchmarking. It allows the implementation/plug-in of custom workload behaviors to meet the diverse criteria of a wide range of business scenarios.\n\nImplementing a workload module for Caliper currently has some limitations and usability obstacles.\n\nThe goal of the project is to provide means for defining complex workload behaviors in Caliper in a declarative manner, using solely/mostly configuration files.\n\nThis should be accomplished with a general built-in workload module implementation that acts according to a configuration-based workload specification.\n\nExpected Outcome\nThe mentee must complete the following tasks by the end of the internship:\n1) Survey the workload definition capabilities of relevant workload generators/benchmark tools and typical workload requirements of relevant standards or existing workloads.\n2) Specify/design a flexible and extendable YAML-based configuration schema for the declarative definition of complex workloads.\n3) Implement a built-in Caliper workload module capable of generating requests based on the above configuration.\n4) Thoroughly test the implemented module.\n5) Provide developer and user documentation for the schema and the implemented module.\n6) Port some of the Caliper microbenchmarks to the declarative schema as proof of concept.	{git,javascript}	2021	Term 2	https://github.com/hyperledger/caliper	https://wiki.hyperledger.org/display/INTERN/Declarative+workload+behavior+definition+for+Hyperledger+Caliper	300000	110
116	c13790aa-d331-4c1f-9d21-071eba77229c	Hyperledger Cello - Operate and Govern Blockchain Networks in Decentralized Way	{"Blockchain is helpful for collaborative applications, however, it is difficult to manage because of the decentralized features. Another challenge is how to efficiently govern a blockchain network among various organizations.\n\nHyperledger Cello is aiming to resolve the following challenges, 1) Facilitate creation of blockchain network, can help user without blockchain background to setup their network quickly. 2) Security, need to implement mechanism to protect key pairs. 3) Cross organization communication, can connect blockchain networks among multiple organizations. This project targets to design and implement a practical operational system equipping with decentralized functionalities to solve above challenges, based on Hyperledger Cello code base."}	Blockchain is helpful for collaborative applications, however, it is difficult to manage because of the decentralized features. Another challenge is how to efficiently govern a blockchain network among various organizations.\n\nHyperledger Cello is aiming to resolve the following challenges, 1) Facilitate creation of blockchain network, can help user without blockchain background to setup their network quickly. 2) Security, need to implement mechanism to protect key pairs. 3) Cross organization communication, can connect blockchain networks among multiple organizations. This project targets to design and implement a practical operational system equipping with decentralized functionalities to solve above challenges, based on Hyperledger Cello code base.	{python,django,cloud,docker,kubernetes,javascript,css,html}	2020	Term 2	https://github.com/hyperledger/cello	https://wiki.hyperledger.org/pages/viewpage.action?pageId=29035308	300000	110
113	1ccc33e0-ea10-4d82-8032-ae55ce5efec7	Hyperledger Fabric - Distributed Citizens Pulse Platform For State/City Council	{"Introduction and Problem Statement:\n\nMany city councils want to know the voice of citizens whenever a change is implemented. Usually when a city approves a plan, there are always a small minority affected","opposing it.The city always wants feedback from the citizens on their decisions and they never have a proper channel to do so. Though the usage of online forums are common in developed and developing countries,  its usually challenging to get public agree on a common platform as the platforms are controlled by individual companies","government bodies. Also, public is worried about privacy and safety of the platform. Further, there are also situations like some NGOs who host the platform have financial challenges and hence discontinue their services.\n\nProposed Solution:\n\nIn a distributed platform like Hyperledger, its possible to decentralize the ownership. This gets to a situation where the platform is by the people and for the people. Also, due to the immutable nature of the Hyperledger, there is one source of truth existing in the system. The Endorsement policies handle the privacy part and Certificate-Authorities provide safe and\nsecure platform. When the Hyperledger components are shared among the NGOs",government,Universities,"private-bodies, the ownership is distributed."}	Introduction and Problem Statement:\n\nMany city councils want to know the voice of citizens whenever a change is implemented. Usually when a city approves a plan, there are always a small minority affected/opposing it.The city always wants feedback from the citizens on their decisions and they never have a proper channel to do so. Though the usage of online forums are common in developed and developing countries,  its usually challenging to get public agree on a common platform as the platforms are controlled by individual companies/government bodies. Also, public is worried about privacy and safety of the platform. Further, there are also situations like some NGOs who host the platform have financial challenges and hence discontinue their services.\n\nProposed Solution:\n\nIn a distributed platform like Hyperledger, its possible to decentralize the ownership. This gets to a situation where the platform is by the people and for the people. Also, due to the immutable nature of the Hyperledger, there is one source of truth existing in the system. The Endorsement policies handle the privacy part and Certificate-Authorities provide safe and\nsecure platform. When the Hyperledger components are shared among the NGOs/government/Universities/private-bodies, the ownership is distributed.	{golang,java,bash,docker}	2020	Term 2	https://github.com/hyperledger/fabric	https://wiki.hyperledger.org/pages/viewpage.action?pageId=29036032	300000	110
118	a1fc2d2b-8d37-41a8-897d-6f8a01249ddc	Hyperledger Indy -Secure DID Registry on Github/Gitlab for Hyperledger Frameworks	{"Develop a command line utility that can create a secure DID Registry on Github or a Gitlab .\n\nOn the Bash shell this utility will enable commands that makes use of Github or Gitlab API for doing basic CRUD operations on  a  GIT repository based DID Registry."}	Develop a command line utility that can create a secure DID Registry on Github or a Gitlab .\n\nOn the Bash shell this utility will enable commands that makes use of Github or Gitlab API for doing basic CRUD operations on  a  GIT repository based DID Registry.	{git,github,gitlab,python}	2020	Term 2	https://github.com/hyperledger/indy-sdk/	https://wiki.hyperledger.org/pages/viewpage.action?pageId=31195277	660000	110
503	06a25b14-a56e-4294-a89d-05f1dc74106c	Hyperledger - Iroha 1: extend queries with optional arguments	{"Hyperledger Iroha 1 has a great set of commands and queries, but some useful commands and queries are missing. Iroha 1 blockchain can be the core of banking systems because it supports transactions, multiple currencies, etc. Nevertheless, advanced filtering of transactions is only partially available in Iroha 1, and it's necessary for banking systems. Currently, to filter transactions, we need to return many of them and filter with our code - it is much heavier for both Iroha's node and client application.\n\nDuring the internship project, queries should be extended with optional arguments.\n\nThe mentee will be able to learn the following:\n- the architecture of Iroha (1.x),\n- work in the true spirit of open-source, communicating with the Iroha community, joining calls, etc.\n- writing documentation,\n- following rules of the open-source code of Hyperledger Iroha\n- creating automatic tests of code\n\nExpected Outcome\n- Implementation according to description and plan of documentation and tests.\n- Added the possibility of running modified queries from Iroha's client libraries.\n- Present changes during the Iroha bi-weekly meeting at the end of the internship."}	Hyperledger Iroha 1 has a great set of commands and queries, but some useful commands and queries are missing. Iroha 1 blockchain can be the core of banking systems because it supports transactions, multiple currencies, etc. Nevertheless, advanced filtering of transactions is only partially available in Iroha 1, and it's necessary for banking systems. Currently, to filter transactions, we need to return many of them and filter with our code - it is much heavier for both Iroha's node and client application.\n\nDuring the internship project, queries should be extended with optional arguments.\n\nThe mentee will be able to learn the following:\n- the architecture of Iroha (1.x),\n- work in the true spirit of open-source, communicating with the Iroha community, joining calls, etc.\n- writing documentation,\n- following rules of the open-source code of Hyperledger Iroha\n- creating automatic tests of code\n\nExpected Outcome\n- Implementation according to description and plan of documentation and tests.\n- Added the possibility of running modified queries from Iroha's client libraries.\n- Present changes during the Iroha bi-weekly meeting at the end of the internship.	{git,protobuf,cmake,python,java,javascript,json,postgresql,rocksdb}	2023	Term 2	https://github.com/hyperledger/iroha	https://wiki.hyperledger.org/display/INTERN/Iroha+1%3A+extend+queries+with+optional+arguments	300000	110
105	d567eb49-9e69-49d3-b858-8d60657869b3	Hyperledger Iroha - Making Data Model Modular for Interoperability with Other Projects	{"Hyperledger is a big project hosting different frameworks and tools. In Iroha team we've been working for some time now on integrating other projects into Iroha and Iroha into other projects. During that process, we've received good feedback and an important question: what can Iroha give other projects? To answer that, we've decided to mentor this project, including some research of different projects to create the most interoperable API solution and practical work with C++ code.\n\nWe will try to derive a common subset of data model and executor interfaces from different HL projects in order to make the most compatible solution.\n\nDecoupling data model (commands and queries) will allow both reuse of it in different projects and easier work in case someone decides to change something in Iroha code itself (to customise it for their own project, for example, by easily plugging a new data model piece in). Some of the features might only be needed to some users, but not everyone. \n"}	Hyperledger is a big project hosting different frameworks and tools. In Iroha team we've been working for some time now on integrating other projects into Iroha and Iroha into other projects. During that process, we've received good feedback and an important question: what can Iroha give other projects? To answer that, we've decided to mentor this project, including some research of different projects to create the most interoperable API solution and practical work with C++ code.\n\nWe will try to derive a common subset of data model and executor interfaces from different HL projects in order to make the most compatible solution.\n\nDecoupling data model (commands and queries) will allow both reuse of it in different projects and easier work in case someone decides to change something in Iroha code itself (to customise it for their own project, for example, by easily plugging a new data model piece in). Some of the features might only be needed to some users, but not everyone.	{research}	2020	Term 2	https://github.com/hyperledger/iroha	https://wiki.hyperledger.org/display/INTERN/Making+HL+Iroha+Data+Model+Modular+for+Interoperability+with+Other+Projects	300000	110
11	ab5d101c-3791-4336-888c-e6c9ffaa7e72	Hyperledger Solang -Create A New Solidity Language Server (SLS) Using Solang Compiler	{"Solidity is a language for Smart Contracts used by the Ethereum Solidity compiler. Solang is a new implementation of the Solidity compiler, which can target wasm on more blockchains. Solang is written in rust and uses a generated Solidity parser.\n\nModern IDEs provide syntax highlighting, symbol definition, errors, warnings etc. This is often done via the Language Server Protocol. There are no good implementations of a language server for Solidity, however using the parser and resolver of Solang, this can change.\n\nThe idea of this mentorship is to extend the Solang Compiler project so it can run as a language server, that implements the Language Server Protocol."}	Solidity is a language for Smart Contracts used by the Ethereum Solidity compiler. Solang is a new implementation of the Solidity compiler, which can target wasm on more blockchains. Solang is written in rust and uses a generated Solidity parser.\n\nModern IDEs provide syntax highlighting, symbol definition, errors, warnings etc. This is often done via the Language Server Protocol. There are no good implementations of a language server for Solidity, however using the parser and resolver of Solang, this can change.\n\nThe idea of this mentorship is to extend the Solang Compiler project so it can run as a language server, that implements the Language Server Protocol.	{rust,compiler}	2020	Term 2	https://github.com/hyperledger-labs/solang	https://wiki.hyperledger.org/display/INTERN/Create+a+new+Solidity+Language+Server+%28SLS%29+using+Solang+Compiler	300000	110
168	eed9b6a5-8fc9-415c-8dd0-053cbc9e2278	Hyperledger Solang - Implement two compiler passes for the Solang Solidity Compiler	{"The Solang Solidity Compiler is a new compiler project, which compiles Solidity to wasm or bpf. The compiler does not yet detect if a variable is used before it is assigned, or detecting that a value set is never used. For example:\n\nFor example:\ncontract test {\n  function f(bool condition) public return (int) {\n    int x;\n    int y = 5;\n    if (condition) {\n      x = 102;\n    }\n\n    return x;\n  }\n}\nThis code has two problems: y is never used, and the value of x might be returned without it being given a value. In compiler theory this is done with reaching definitions. Solang already has an implementation of reaching definitions for the constant propagation pass, which will have to be generalized. Unused variables should be removed from the generated code.\n\nSecondly, implement a common subexpression elimination pass, which also uses the same reaching definitions implementation, to ensure subexpressions can safely be substituted.\n\nLearning Objectives\n1)  learn how to be a positive collaborator and contributor in an active open source project.\n2) Learn how to work within the Hyperledger open source ecosystem and culture.\n3) Understand smart contracts and the Solidity language\n4) Gain a greater understanding of compiler optimizations and compiler theory\n\nExpected Outcome\n1) Solang gives errors when undefined variables are used\n2) Solang gives warnings when variables are not used, and removes them from the generated code\n3) Solang does a common subexpression elimination pass"}	The Solang Solidity Compiler is a new compiler project, which compiles Solidity to wasm or bpf. The compiler does not yet detect if a variable is used before it is assigned, or detecting that a value set is never used. For example:\n\nFor example:\ncontract test {\n  function f(bool condition) public return (int) {\n    int x;\n    int y = 5;\n    if (condition) {\n      x = 102;\n    }\n\n    return x;\n  }\n}\nThis code has two problems: y is never used, and the value of x might be returned without it being given a value. In compiler theory this is done with reaching definitions. Solang already has an implementation of reaching definitions for the constant propagation pass, which will have to be generalized. Unused variables should be removed from the generated code.\n\nSecondly, implement a common subexpression elimination pass, which also uses the same reaching definitions implementation, to ensure subexpressions can safely be substituted.\n\nLearning Objectives\n1)  learn how to be a positive collaborator and contributor in an active open source project.\n2) Learn how to work within the Hyperledger open source ecosystem and culture.\n3) Understand smart contracts and the Solidity language\n4) Gain a greater understanding of compiler optimizations and compiler theory\n\nExpected Outcome\n1) Solang gives errors when undefined variables are used\n2) Solang gives warnings when variables are not used, and removes them from the generated code\n3) Solang does a common subexpression elimination pass	{rust,compiler}	2021	Term 2	https://github.com/hyperledger-labs/solang	https://wiki.hyperledger.org/display/INTERN/Implement+two+compiler+passes+for+the+Solang+Solidity+Compiler	360000	110
20	7b9d5a1b-60f8-449e-abd4-143025447c7a	Hyperledger Umbra - Adding Network Fuzzing Capabilities	{"Hyperledger Umbra is a Hyperledger Lab designed to run unmodified versions of our DLT platforms (e.g. Hyperledger Fabric, Hyperledger Iroha) under a simulated environment with software defined networking for the purposes of running experiments (e.g. scaling experiments, consensus algorithm development, etc) and security audits. Currently Hyperledger Umbra can run unmodified Hyperledger Fabric Docker images and execute a full Fabric network under simulation. Umbra is written in Python and has code for doing virtual network switches and connections. This project is to extend that code to allow for network fuzzing capabilities. The goal is to be able to introduce packet drops, packet delay, packet reordering, as well as unsolicited packets with random","known-bad data. The purpose it be able to test Hyperledger Fabric's resilience to general network \\"weather\\" and intentional attacks coming from the network."}	Hyperledger Umbra is a Hyperledger Lab designed to run unmodified versions of our DLT platforms (e.g. Hyperledger Fabric, Hyperledger Iroha) under a simulated environment with software defined networking for the purposes of running experiments (e.g. scaling experiments, consensus algorithm development, etc) and security audits. Currently Hyperledger Umbra can run unmodified Hyperledger Fabric Docker images and execute a full Fabric network under simulation. Umbra is written in Python and has code for doing virtual network switches and connections. This project is to extend that code to allow for network fuzzing capabilities. The goal is to be able to introduce packet drops, packet delay, packet reordering, as well as unsolicited packets with random/known-bad data. The purpose it be able to test Hyperledger Fabric's resilience to general network "weather" and intentional attacks coming from the network.	{python,networking,docker}	2020	Term 2	https://github.com/hyperledger-labs/umbra	https://wiki.hyperledger.org/display/INTERN/Adding+Network+Fuzzing+Capabilities+to+Hyperledger+Umbra	660000	110
10	6970f6e1-a365-4ab7-9f6e-9a26b8b4a145	Hyperledger Umbra - Build a University Course on Hyperledger Fabric	{"A growing part of our community is our partnership with academia. Hyperledger Fabric and other Hyperledger projects are being incorporated in to research and education in universities more and more all over the world. Hyperledger can aid the growth by preparing study materials, sample lab experiments and quizzes and tests that teach computer science students about distributed systems and consensus algorithms and cryptography. Hyperledger Umbra has streamlined the process of setting up a full Hyperledger Fabric network running on a single computer. It provides an ideal environment for students to learn Hyperledger Fabric administration as well as use it for running lab experiments and learning exercises. This mentorship is focused on producing a \\"classroom-in-a-box\\" for an introductory computer science course on distributed systems and algorithms that can be offered to universities that wish to use Hyperledger Fabric in their curriculum."}	A growing part of our community is our partnership with academia. Hyperledger Fabric and other Hyperledger projects are being incorporated in to research and education in universities more and more all over the world. Hyperledger can aid the growth by preparing study materials, sample lab experiments and quizzes and tests that teach computer science students about distributed systems and consensus algorithms and cryptography. Hyperledger Umbra has streamlined the process of setting up a full Hyperledger Fabric network running on a single computer. It provides an ideal environment for students to learn Hyperledger Fabric administration as well as use it for running lab experiments and learning exercises. This mentorship is focused on producing a "classroom-in-a-box" for an introductory computer science course on distributed systems and algorithms that can be offered to universities that wish to use Hyperledger Fabric in their curriculum.	{linux,docker}	2020	Term 2	https://github.com/hyperledger-labs/umbra	https://wiki.hyperledger.org/display/INTERN/Build+a+university+course+on+Hyperledger+Fabric+using+Hyperledger+Umbra	420000	110
117	b3878c5c-ec58-43c0-8462-e4ac487f2f64	Hyperledger Umbra - Scaling Experiments	{"For the past two years, the Hyperledger Umbra lab has been worked on by mentees to develop the capability of running the unmodified Hyperledger Fabric docker images under a simulated network environment for the purposes of doing experiments on running Fabric networks in a controlled way. At the end of last summer, Hyperledger Umbra achieved the initial goal of running Fabric networks under simulation. Hyperledger Umbra is useful for doing network-level security fuzzing as well as network scaling experiments and this mentorship is focused on designing and executing scaling experiments.\n\nHyperledger Fabric networks are designed to scale up fairly easily, however nobody has actually tried scaling Hyperledger Fabric to many hundreds and thousands of nodes and done analysis on how it affects the characteristics of the distributed system such as time to reach consensus, time to run an election for block creation, etc. The mentee chosen for this project will work with their mentor to design a plan for setting up, executing, gathering and analyzing data, and reporting the results of scaling experiments run at different scales."}	For the past two years, the Hyperledger Umbra lab has been worked on by mentees to develop the capability of running the unmodified Hyperledger Fabric docker images under a simulated network environment for the purposes of doing experiments on running Fabric networks in a controlled way. At the end of last summer, Hyperledger Umbra achieved the initial goal of running Fabric networks under simulation. Hyperledger Umbra is useful for doing network-level security fuzzing as well as network scaling experiments and this mentorship is focused on designing and executing scaling experiments.\n\nHyperledger Fabric networks are designed to scale up fairly easily, however nobody has actually tried scaling Hyperledger Fabric to many hundreds and thousands of nodes and done analysis on how it affects the characteristics of the distributed system such as time to reach consensus, time to run an election for block creation, etc. The mentee chosen for this project will work with their mentor to design a plan for setting up, executing, gathering and analyzing data, and reporting the results of scaling experiments run at different scales.	{docker,python,aws,kubernetes}	2020	Term 2	https://github.com/hyperledger-labs/umbra	https://wiki.hyperledger.org/display/INTERN/Scaling+Experiments+with+Hyperledger+Umbra	360000	110
477	f3cedb60-dfb8-41ed-a086-7e0f09a22a77	Hyperledger - An object-key-value mapper for Hyperledger Fabric	{"In Hyperledger Fabric, there can be a significant mismatch between the simple key-value ledger storage abstraction and the data representation style used for developing chaincode – i.e., Java has classes and objects, not keys and values. Currently, there aren’t really good tools for facilitating the “object-key-value mapping” (at least anything approaching classic Object-Relational Mapping – ORM). This not only complicates chaincode development, but a less than systematic approach with the mapping can lead to performance problems (through logically unnecessary MVCC conflict transaction invalidations). Additionally, an explicit object-oriented ledger data model would enable imposing data-centric constraints on the ledger content, either for runtime checking or development time verification and validation.\n\nThe goal of the mentorship is to design and implement an object-key-value mapper with the following functionality:\n\n- Generating key-value storage models from UML ledger data models\n- Application of storage strategies during the mapping (as we explored in the report referenced below)\n- Generating a chaincode-internal Java data access","persistence layer, “parameterized” by the storage model\n- Demonstration on a representative example (e.g., our earlier work on faithfully implementing TPC-C to Fabric)\n- (If we have time): declaring OCL (Object Constraint Language) constraints on the models and enforcing them in the data access layer"}	In Hyperledger Fabric, there can be a significant mismatch between the simple key-value ledger storage abstraction and the data representation style used for developing chaincode – i.e., Java has classes and objects, not keys and values. Currently, there aren’t really good tools for facilitating the “object-key-value mapping” (at least anything approaching classic Object-Relational Mapping – ORM). This not only complicates chaincode development, but a less than systematic approach with the mapping can lead to performance problems (through logically unnecessary MVCC conflict transaction invalidations). Additionally, an explicit object-oriented ledger data model would enable imposing data-centric constraints on the ledger content, either for runtime checking or development time verification and validation.\n\nThe goal of the mentorship is to design and implement an object-key-value mapper with the following functionality:\n\n- Generating key-value storage models from UML ledger data models\n- Application of storage strategies during the mapping (as we explored in the report referenced below)\n- Generating a chaincode-internal Java data access/persistence layer, “parameterized” by the storage model\n- Demonstration on a representative example (e.g., our earlier work on faithfully implementing TPC-C to Fabric)\n- (If we have time): declaring OCL (Object Constraint Language) constraints on the models and enforcing them in the data access layer	{git,java}	2023	Term 2	https://github.com/hyperledger	https://wiki.hyperledger.org/display/INTERN/An+object-key-value+mapper+for+Hyperledger+Fabric	300000	1
482	a11b88b0-0947-47d1-8831-1bcee6a9d7a4	Hyperledger - aries-vcx based message mediator	{"Implement message mediator service written in Rust with pick-up v2 protocol support on top of aries-vcx crate.\n\nLearning Objectives\n- Learn about self sovereign identity and Aries protocols\n- Learn about writing scalable server applications\n- Learn Rust language\n- Learn Github Actions and CI process in general\n\nExpected Outcome\n- Web service for Aries message mediation \n- CI jobs to test the implementation"}	Implement message mediator service written in Rust with pick-up v2 protocol support on top of aries-vcx crate.\n\nLearning Objectives\n- Learn about self sovereign identity and Aries protocols\n- Learn about writing scalable server applications\n- Learn Rust language\n- Learn Github Actions and CI process in general\n\nExpected Outcome\n- Web service for Aries message mediation \n- CI jobs to test the implementation	{programming}	2023	Term 2	https://github.com/hyperledger/aries-vcx/tree/main/aries_vcx	https://wiki.hyperledger.org/display/INTERN/aries-vcx+based+message+mediator	300000	1
34	f1bab2bd-b96d-4ed4-a6ce-8516d0b91a0d	Hyperledger - Automated Fault-Tolerant HTLC for Cross-Chain Atomic Asset Exchange	{"The Hash Time Locked Contract (HTLC) is a well-known pattern for dis-intermediated asset swaps, i.e., as a mechanism to exchange assets between two parties in two different blockchain networks in an atomic manner without a trusted mediator (or trusted third party). HTLC support has been implemented in the Hyperledger Weaver Labs project, with mechanisms to lock, claim, and unlock assets supported in both the contract and higher application layers. At present, the protocol requires several manual steps by the application users carrying out the exchange or agents acting on their behalf. We would like to automate this process as much as possible by augmenting the relay module in Weaver to communicate events and trigger transactions in foreign networks; this requires both research and software development. Further, HTLC is known to have flaws that could violate system integrity properties, because it relies on parties to execute certain actions by a certain timeout or forever lose the opportunity to carry out those actions; this makes the basic protocol intolerant to system crashes. Research is needed to (provably) determine the best way to make the HTLC protocol fault tolerant, using the features of the blockchain networks maintaining the assets and the Weaver relays mediating the exchange. Augmenting the existing HTLC mechanism within Weaver will provide practical validation of this research."}	The Hash Time Locked Contract (HTLC) is a well-known pattern for dis-intermediated asset swaps, i.e., as a mechanism to exchange assets between two parties in two different blockchain networks in an atomic manner without a trusted mediator (or trusted third party). HTLC support has been implemented in the Hyperledger Weaver Labs project, with mechanisms to lock, claim, and unlock assets supported in both the contract and higher application layers. At present, the protocol requires several manual steps by the application users carrying out the exchange or agents acting on their behalf. We would like to automate this process as much as possible by augmenting the relay module in Weaver to communicate events and trigger transactions in foreign networks; this requires both research and software development. Further, HTLC is known to have flaws that could violate system integrity properties, because it relies on parties to execute certain actions by a certain timeout or forever lose the opportunity to carry out those actions; this makes the basic protocol intolerant to system crashes. Research is needed to (provably) determine the best way to make the HTLC protocol fault tolerant, using the features of the blockchain networks maintaining the assets and the Weaver relays mediating the exchange. Augmenting the existing HTLC mechanism within Weaver will provide practical validation of this research.	{rust,go}	2022	Term 2	https://github.com/hyperledger-labs/weaver-dlt-interoperability	https://wiki.hyperledger.org/display/INTERN/Automated+Fault-Tolerant+HTLC+for+Cross-Chain+Atomic+Asset+Exchange	600000	1
485	9ac44821-a844-4e8c-b6d3-764788d237a3	Hyperledger - Automated gateways through smart contracts	{"The objective of this project is to improve the interoperability of different blockchain networks by developing smart contract gateways that can interact with multiple networks. These gateways will automate, trace, and secure the process of submitting transactions to other blockchain networks.\n\nTo achieve this goal, the project will involve creating smart contract gateways for various blockchain networks, including those using the same platform, as well as gateways to submit transactions to heterogeneous networks. The initial focus will be on developing gateway smart contracts for connecting multiple separate Hyperledger Fabric networks, and the next step will be to create gateways to connect Hyperledger Fabric to other heterogeneous networks, such as Hyperledger Indy and Besu.\n\nThe project's primary objective is to evaluate system-level and large-scale integrations, which will be accomplished by setting up an extensive network of blockchains using Hyperledger Fabric as the source blockchain and other networks as destination networks.\n\nOverall, the project aims to enhance the efficiency, security, and traceability of cross-blockchain transaction submissions, thereby improving the overall functionality of blockchain technology."}	The objective of this project is to improve the interoperability of different blockchain networks by developing smart contract gateways that can interact with multiple networks. These gateways will automate, trace, and secure the process of submitting transactions to other blockchain networks.\n\nTo achieve this goal, the project will involve creating smart contract gateways for various blockchain networks, including those using the same platform, as well as gateways to submit transactions to heterogeneous networks. The initial focus will be on developing gateway smart contracts for connecting multiple separate Hyperledger Fabric networks, and the next step will be to create gateways to connect Hyperledger Fabric to other heterogeneous networks, such as Hyperledger Indy and Besu.\n\nThe project's primary objective is to evaluate system-level and large-scale integrations, which will be accomplished by setting up an extensive network of blockchains using Hyperledger Fabric as the source blockchain and other networks as destination networks.\n\nOverall, the project aims to enhance the efficiency, security, and traceability of cross-blockchain transaction submissions, thereby improving the overall functionality of blockchain technology.	{blockchain}	2023	Term 2	https://github.com/hyperledger	https://wiki.hyperledger.org/display/INTERN/Automated+gateways+through+smart+contracts	600000	1
551	700fe3a8-04a9-4aed-8264-bd353aea56ee	Hyperledger - Benchmarking Cross-Chain Bridges	{"Cross-chain bridges are a vital part of the blockchain landscape as they enable data and value to move seamlessly across different blockchains. Security is of utmost importance in this area, as these bridges can be vulnerable to malicious attacks and manipulation. In addition to securing bridges, measuring their performance and scalability is key to understanding if one is meeting the highest standards and is capable of encountering users' needs. This is mandatory to achieve mass adoption of the technology by single users and businesses.\n\nThere have been some efforts to design and implement interoperability solutions across different blockchains, some of which are within the Hyperledger ecosystem (see links in the next section). Furthermore, preliminary research has been conducted to benchmark interoperability solutions, however, we feel there is currently a lack of studies that thoroughly compare bridges – used both within and between Hyperledger and public blockchains used in the industry. We aim to address this gap and conduct such evaluation studies, mainly through benchmarking.\n\nThe goal of this project is to benchmark cross-chain solutions, an essential step for measuring the performance of any solution. As the adoption of cross-chain bridges grows it is increasingly important to have benchmarks to measure cross-chain solutions' performance and security."}	Cross-chain bridges are a vital part of the blockchain landscape as they enable data and value to move seamlessly across different blockchains. Security is of utmost importance in this area, as these bridges can be vulnerable to malicious attacks and manipulation. In addition to securing bridges, measuring their performance and scalability is key to understanding if one is meeting the highest standards and is capable of encountering users' needs. This is mandatory to achieve mass adoption of the technology by single users and businesses.\n\nThere have been some efforts to design and implement interoperability solutions across different blockchains, some of which are within the Hyperledger ecosystem (see links in the next section). Furthermore, preliminary research has been conducted to benchmark interoperability solutions, however, we feel there is currently a lack of studies that thoroughly compare bridges – used both within and between Hyperledger and public blockchains used in the industry. We aim to address this gap and conduct such evaluation studies, mainly through benchmarking.\n\nThe goal of this project is to benchmark cross-chain solutions, an essential step for measuring the performance of any solution. As the adoption of cross-chain bridges grows it is increasingly important to have benchmarks to measure cross-chain solutions' performance and security.	{blockchain,typescript,git,docker,linux}	2023	Term 2	https://github.com/hyperledger	https://wiki.hyperledger.org/display/INTERN/Benchmarking+Cross-Chain+Bridges	600000	1
307	e6802f1d-f8c0-453e-8f3a-6ac87594b97d	Hyperledger - Blockchain Network Operation in a Decentralized Way	{"Now most users can easily deploy a blockchain network. On this basis, they need to manage the blockchain more flexibly and safely. Hyperledger cello will implement a fully decentralized BaaS. Each organization can independently deploy and manage its own baas, including their own private keys. Among organizations that have deployed cello, blockchain networks can be flexibly established.\n\nThrough learning and development work, complete the design and development of some interfaces and front-end pages of the platform"}	Now most users can easily deploy a blockchain network. On this basis, they need to manage the blockchain more flexibly and safely. Hyperledger cello will implement a fully decentralized BaaS. Each organization can independently deploy and manage its own baas, including their own private keys. Among organizations that have deployed cello, blockchain networks can be flexibly established.\n\nThrough learning and development work, complete the design and development of some interfaces and front-end pages of the platform	{python,html,javascript,django}	2022	Term 2	https://github.com/hyperledger/cello	https://wiki.hyperledger.org/display/INTERN/Blockchain+Network+Operation+in+a+Decentralized+Way	600000	1
496	97876956-061e-44c5-b55e-938aca2b6440	Hyperledger - Cactus and Hedera Hashgraph integration	{"Hyperledger Cactus is a blockchain decentralised integration tool designed to allow users to securely integrate different blockchains started by companies Fujitsu and Accenture. Cactus has pluggable architecture which makes easy to integrate various blockchain by creating plugin, currently plugins for Fabric, Besu, Quorum are implemented. Cactus allows to transfer not only assets but also data between multiple blockchains.\n\nHedera Hashgraph is a DLT built on top of an asynchronous BFT algorithm, providing smart contract, token, and consensus services to end users. The flexibility\n\n- Document ready-to-use integration of Hedera and Cactu,\n- Document example of integration using Hedera's Testnet and Hedera's Previewnet\n- Document example of integration between another blockchain and Hedera using Cactus"}	Hyperledger Cactus is a blockchain decentralised integration tool designed to allow users to securely integrate different blockchains started by companies Fujitsu and Accenture. Cactus has pluggable architecture which makes easy to integrate various blockchain by creating plugin, currently plugins for Fabric, Besu, Quorum are implemented. Cactus allows to transfer not only assets but also data between multiple blockchains.\n\nHedera Hashgraph is a DLT built on top of an asynchronous BFT algorithm, providing smart contract, token, and consensus services to end users. The flexibility\n\n- Document ready-to-use integration of Hedera and Cactu,\n- Document example of integration using Hedera's Testnet and Hedera's Previewnet\n- Document example of integration between another blockchain and Hedera using Cactus	{go,java,javascript,rust,kotlin}	2022	Term 2	https://github.com/hyperledger/cactus	https://wiki.hyperledger.org/display/INTERN/Hyperleger+Cactus+and+Hedera+Hashgraph+Integration	300000	1
390	1b293202-895d-4b5e-88a5-d3c62fb33461	Hyperledger - Client Connector for Hyperledger Besu (Ethereum)	{"Develop a connector that provides both synchronous and asynchronous modes of interacting with a running Hyperledger Besu node. The connector would act as an interface between an enterprise application and the Hyperledger Besu node for data ingestions and it could provide event subscription options.\n\nThe scope of the project would also include an end-to-end test on a sample network."}	Develop a connector that provides both synchronous and asynchronous modes of interacting with a running Hyperledger Besu node. The connector would act as an interface between an enterprise application and the Hyperledger Besu node for data ingestions and it could provide event subscription options.\n\nThe scope of the project would also include an end-to-end test on a sample network.	{java,blockchain,container}	2022	Term 2	https://github.com/hyperledger/besu	https://wiki.hyperledger.org/display/INTERN/Client+Connector+for+Hyperledger+Besu	300000	1
298	07d439ac-2c7d-409c-994a-bcb5ede81c2e	Hyperledger - Cross-Chain State Modelling and Analysis	{"The emergence of blockchain interoperability is reducing the risk of investing in blockchain by avoiding vendor lock-in, leveraging interoperation with off-chain systems, and providing a truly open ecosystem, enabling a network of blockchains. \n\nIn particular, in the technical report we wrote, we designed a survey aimed to understand the socio-technical challenges that blockchain interoperability poses, with the goal of offering a unique opportunity to link the underlying technology with human experience and values. We discovered that end-users are particularly concerned with visualizing and analyzing cross-chain cost and throughput, with some indication that measuring energetic expenditure in the cross-chain setting would also be valuable.\n\nFollowing that survey, we implemented a simple PoC that obtains transaction receipts from different blockchains and aggregates them in a unified repository. This project aims to build on top of what was done and create a cross-chain model (taking into account what end-users what to analyze from such model) automatically from input cross-chain transactions.\n\nThis project will take a cross-chain use case,  create a model and allow end-users to extract relevant metrics, promoting a better understatement of the business logic running on blockchain infrastructures, and of interoperability."}	The emergence of blockchain interoperability is reducing the risk of investing in blockchain by avoiding vendor lock-in, leveraging interoperation with off-chain systems, and providing a truly open ecosystem, enabling a network of blockchains. \n\nIn particular, in the technical report we wrote, we designed a survey aimed to understand the socio-technical challenges that blockchain interoperability poses, with the goal of offering a unique opportunity to link the underlying technology with human experience and values. We discovered that end-users are particularly concerned with visualizing and analyzing cross-chain cost and throughput, with some indication that measuring energetic expenditure in the cross-chain setting would also be valuable.\n\nFollowing that survey, we implemented a simple PoC that obtains transaction receipts from different blockchains and aggregates them in a unified repository. This project aims to build on top of what was done and create a cross-chain model (taking into account what end-users what to analyze from such model) automatically from input cross-chain transactions.\n\nThis project will take a cross-chain use case,  create a model and allow end-users to extract relevant metrics, promoting a better understatement of the business logic running on blockchain infrastructures, and of interoperability.	{blockchain,research,typescript,docker,collaborative}	2022	Term 2	https://github.com/hyperledger/fabric	https://wiki.hyperledger.org/display/INTERN/Cross-Chain+State+Modelling+and+Analysis	540000	1
299	9621a27e-4e49-4868-8f4d-9444674f9b30	Hyperledger - Demonstrate Interoperability using Hyperledger Bevel and Cactus	{"Hyperledger Cactus support ledger Interoperability but use a local deployment for testing; Hyperledger Bevel supports production-worthy deployments. This project aims to support Cactus deployment using Bevel to demonstrate production-like usage of Hyperledger Cactus. The steps will be following:\n\n1. Deploy a Hyperledger Fabric network using Bevel on a Managed Kubernetes cluster\n2. Deploy a GoQuorum network using Bevel on a Managed Kubernetes cluster (can be the same cluster for simplicity).\n3. Make changes in Hyperledger Bevel code to deploy the Cactus connectors in both the above networks.\n4. Run Cactus test cases."}	Hyperledger Cactus support ledger Interoperability but use a local deployment for testing; Hyperledger Bevel supports production-worthy deployments. This project aims to support Cactus deployment using Bevel to demonstrate production-like usage of Hyperledger Cactus. The steps will be following:\n\n1. Deploy a Hyperledger Fabric network using Bevel on a Managed Kubernetes cluster\n2. Deploy a GoQuorum network using Bevel on a Managed Kubernetes cluster (can be the same cluster for simplicity).\n3. Make changes in Hyperledger Bevel code to deploy the Cactus connectors in both the above networks.\n4. Run Cactus test cases.	{goquorum,ansible,helm,kubernetes}	2022	Term 2	https://github.com/hyperledger/bevel	https://wiki.hyperledger.org/display/INTERN/Demonstrate+Interoperability+using+Hyperledger+Bevel+and+Cactus	300000	1
14	a30e2f43-9f92-4013-b238-952bd97cb8ea	Hyperledger - Deploy Carbon Accounting Network with Bevel	{"The Hyperledger Labs blockchain-carbon-accounting project has progressed significantly, and we're looking to move to production deployment of it.  The deployment of a Hyperledger Fabric, however, has been a difficult and time consuming process in the past.  Working with the maintainers of Hyperledger Bevel, we would like to streamline the deployment processes for both Fabric and Besu networks.  Our goal is to allow any organization to join the networks using its own cloud infrastructure.\n\nAt the end of the project, we are looking for:\n\n- Successful deployment of blockchain-carbon-accounting across multiple cloud networks, such as Digital Ocean, AWS, and Azure.\n- Scripts for further maintenance of the networks.\n- Detailed tutorials and learning materials which would benefit both blockchain-carbon-accounting developers and the broader Fabric, Besu, and Bevel communities."}	The Hyperledger Labs blockchain-carbon-accounting project has progressed significantly, and we're looking to move to production deployment of it.  The deployment of a Hyperledger Fabric, however, has been a difficult and time consuming process in the past.  Working with the maintainers of Hyperledger Bevel, we would like to streamline the deployment processes for both Fabric and Besu networks.  Our goal is to allow any organization to join the networks using its own cloud infrastructure.\n\nAt the end of the project, we are looking for:\n\n- Successful deployment of blockchain-carbon-accounting across multiple cloud networks, such as Digital Ocean, AWS, and Azure.\n- Scripts for further maintenance of the networks.\n- Detailed tutorials and learning materials which would benefit both blockchain-carbon-accounting developers and the broader Fabric, Besu, and Bevel communities.	{ansible,python}	2022	Term 2	https://github.com/hyperledger-labs/blockchain-carbon-accounting	https://wiki.hyperledger.org/display/INTERN/Deploy+Carbon+Accounting+Network+with+Bevel	300000	1
474	10f6c86f-993c-47f3-967c-a10a0d386a8f	Hyperledger - Design/Development of a mini game to explore decentralized Identity & Payments	{"This project is meant to serve as a proof-of-concept for implementing decentralized identity and payment rails in the gaming (and by extension a future metaverse) ecosystem. This project has the following distinctive goals: \n\n1. Implement a mini game project as a proof-of-concept (POC) that demonstrates use of decentralized identity and payment systems using Hyperledger chains such as Indy & Aries for Identity and Firefly for digital assets. \n\n2. Conduct research on existing methods used for identity and payments and how use of Hyperledger blockchains can provide utility functions in such ecosystems\n\n3. Conduct research and coding on how this game project can implement digital wallets for storing credentials and digital assets using design principles and recommendations from the Open Wallet Foundation"}	This project is meant to serve as a proof-of-concept for implementing decentralized identity and payment rails in the gaming (and by extension a future metaverse) ecosystem. This project has the following distinctive goals: \n\n1. Implement a mini game project as a proof-of-concept (POC) that demonstrates use of decentralized identity and payment systems using Hyperledger chains such as Indy & Aries for Identity and Firefly for digital assets. \n\n2. Conduct research on existing methods used for identity and payments and how use of Hyperledger blockchains can provide utility functions in such ecosystems\n\n3. Conduct research and coding on how this game project can implement digital wallets for storing credentials and digital assets using design principles and recommendations from the Open Wallet Foundation	{blockchain,devops}	2023	Term 2	https://github.com/hyperledger	https://wiki.hyperledger.org/pages/viewpage.action?pageId=80778595	600000	1
492	18b2b8f1-77aa-4c4e-89ff-e1027c55b7ee	Hyperledger - Documentation Standards	{"To establish consistent and comprehensive documentation standards for the Hyperledger Foundation open-source blockchain platforms. Our goal is to provide developers, users, and other stakeholders with high-quality technical documentation that is accurate, accessible, and easy to understand. Our end goal is to ensure that the technical documentation for the Hyperledger community is of the highest quality, making it easier for developers to build on these platforms and for users to understand and utilize them effectively.\n\nLearning Objectives\n\n- Develop an understanding of best practices for creating open source documentation\n- Learn how to collaborate with a community to develop documentation guidelines and templates\n- Gain experience in technical writing and editing\n- Learn how to use documentation tools and templates\n\nExpected Outcome\n\n- A set of documentation guidelines and templates that can be used by all Hyperledger projects\n- A style guide for Hyperledger documentation\n- A list of recommended documentation tools and templates\n- A plan for integrating the new guidelines and templates into existing Hyperledger projects\n- A series of blog posts or tutorials on how to use the new guidelines and templates"}	To establish consistent and comprehensive documentation standards for the Hyperledger Foundation open-source blockchain platforms. Our goal is to provide developers, users, and other stakeholders with high-quality technical documentation that is accurate, accessible, and easy to understand. Our end goal is to ensure that the technical documentation for the Hyperledger community is of the highest quality, making it easier for developers to build on these platforms and for users to understand and utilize them effectively.\n\nLearning Objectives\n\n- Develop an understanding of best practices for creating open source documentation\n- Learn how to collaborate with a community to develop documentation guidelines and templates\n- Gain experience in technical writing and editing\n- Learn how to use documentation tools and templates\n\nExpected Outcome\n\n- A set of documentation guidelines and templates that can be used by all Hyperledger projects\n- A style guide for Hyperledger documentation\n- A list of recommended documentation tools and templates\n- A plan for integrating the new guidelines and templates into existing Hyperledger projects\n- A series of blog posts or tutorials on how to use the new guidelines and templates	{communcation,blockchain}	2023	Term 2	https://github.com/hyperledger/	https://wiki.hyperledger.org/display/INTERN/Hyperledger+Documentation+Standards	300000	1
297	48010218-ec43-44ea-aafd-d4b92625df22	Hyperledger - DRman Utility to provision and administer DID based verifiable credential registries	{"The project  DRman has an end goal of developing a stand-alone utility to provision Verifiable Credential Registries [VCR]. The utility can also be used to administer the registry containing credentials issued by the organizations.  The functionalities like creation, verification, modification, and revocation of credentials, form the core of this project as it enables the administrator to manage the VCR with ease. \n\nThe significant aspects to be considered here are Creation, Onboarding (Enable","Restrict Access) of members, and Management(add",update,"revoke) of DIDs.\n\na) Creation (of DID Registry): Function to create a DID Registry for an organization on Git. The automation is already done for this part and can be tracked at https:","",github.com,DIDman,DRman,issues,"3.\n\nb) Onboarding: Function to add enable","restrict access to members of an organization to a repository (e.g., using a combination of GitHub private tokens, GitHub username & users DID). \n\nc) Manage: Function to list APIs that are needed to add",update,"revoke access DID’s or (DID Document) saved as files on the repo.\n\nThis summer, the project holds a variety of interesting automation issues to be addressed that focus more on onboarding and management of the organizations that intend to add themselves to the DID-based Verifiable credential registry."}	The project  DRman has an end goal of developing a stand-alone utility to provision Verifiable Credential Registries [VCR]. The utility can also be used to administer the registry containing credentials issued by the organizations.  The functionalities like creation, verification, modification, and revocation of credentials, form the core of this project as it enables the administrator to manage the VCR with ease. \n\nThe significant aspects to be considered here are Creation, Onboarding (Enable/Restrict Access) of members, and Management(add/update/revoke) of DIDs.\n\na) Creation (of DID Registry): Function to create a DID Registry for an organization on Git. The automation is already done for this part and can be tracked at https://github.com/DIDman/DRman/issues/3.\n\nb) Onboarding: Function to add enable/restrict access to members of an organization to a repository (e.g., using a combination of GitHub private tokens, GitHub username & users DID). \n\nc) Manage: Function to list APIs that are needed to add/update/revoke access DID’s or (DID Document) saved as files on the repo.\n\nThis summer, the project holds a variety of interesting automation issues to be addressed that focus more on onboarding and management of the organizations that intend to add themselves to the DID-based Verifiable credential registry.	{shell,rust}	2022	Term 2	https://github.com/DIDman/DRman/issues	https://wiki.hyperledger.org/display/INTERN/DRman+%3A+Utility+to+provision+and+administer++DID+based+Verifiable+credential+registries+for+Hyperledger+Aries+framework	300000	1
319	320b069b-f8af-4579-9693-4873fee3c91f	Hyperledger - Enable Kubernetes Operators support for Fablo	{"Fablo is a tool to generate the Hyperledger Fabric blockchain network and run it on Docker. It's main goal is to provide a super-easy start with Hyperledger Fabric.\n\nIt has a declarative approach - a whole network is described in a single file. Right now only the Docker environment is supported and we want add support for Kubertnetes Operators.\n\nParticipating in the project will allow you to gain significant experience with Hyperledger Fabric network lifecycle and Kubernetes Operators. You will work with a \\"translation\\" of the current Docker-backed flow to Kubernetes.\n\nThe expertise gained with working on the project is a good start in a broad range of professional careers related with Hyperledger Fabric and","or Kubernetes. It is also a great warmup before Certificated Fabric Administrator and for Kubernetes certifications.\n\nFablo generates Docker compose file and some bash scripts for automation of the network configuration. We want to add an option to generate network configuration for Kubernetes Operators. We expect following outcome:\n\n- Fablo is able to generate YAMLs needed for K8S deployment. IIt can be implemented as a single YAML file deployed by single organization (minimal). It can be also split for multiple organizations for independent deployments (nice to have)\n- Fablo supports current Docker features for K8S (like, up, down, prune, chaincode upgrade.). The scope of supported features is flexible and depends on the work required for adding basic support for K8S."}	Fablo is a tool to generate the Hyperledger Fabric blockchain network and run it on Docker. It's main goal is to provide a super-easy start with Hyperledger Fabric.\n\nIt has a declarative approach - a whole network is described in a single file. Right now only the Docker environment is supported and we want add support for Kubertnetes Operators.\n\nParticipating in the project will allow you to gain significant experience with Hyperledger Fabric network lifecycle and Kubernetes Operators. You will work with a "translation" of the current Docker-backed flow to Kubernetes.\n\nThe expertise gained with working on the project is a good start in a broad range of professional careers related with Hyperledger Fabric and/or Kubernetes. It is also a great warmup before Certificated Fabric Administrator and for Kubernetes certifications.\n\nFablo generates Docker compose file and some bash scripts for automation of the network configuration. We want to add an option to generate network configuration for Kubernetes Operators. We expect following outcome:\n\n- Fablo is able to generate YAMLs needed for K8S deployment. IIt can be implemented as a single YAML file deployed by single organization (minimal). It can be also split for multiple organizations for independent deployments (nice to have)\n- Fablo supports current Docker features for K8S (like, up, down, prune, chaincode upgrade.). The scope of supported features is flexible and depends on the work required for adding basic support for K8S.	{linux,bash,typescript,yaml,docker}	2022	Term 2	https://github.com/hyperledger-labs/fablo	https://wiki.hyperledger.org/display/INTERN/Enable+Kubernetes+Operators+support+for+Fablo	300000	1
354	6be356a9-f851-463c-ad0b-9f1f629cd540	Hyperledger - Expand Minifabric with k8s operator support	{"Minifabric supports deploy Hyperledger Fabric in both docker and K8S environments, however, it currently does not support Hyperledger Fabric running as k8s operators. Making Minifabric deploying Hyperledger Fabric onto K8S and running as K8S operators will allow hyperledger fabric network nodes with taking advantages of K8S operator benefit. This work will involve the participators to develop K8S operators, deploying K8S operator controllers and managing K8S operator life cycles, at the same time, the participators will be able to learn Fabric and Fabric deployment model. This is a great opportunity for anyone who would like to know K8S and know Hyperledger Fabric."}	Minifabric supports deploy Hyperledger Fabric in both docker and K8S environments, however, it currently does not support Hyperledger Fabric running as k8s operators. Making Minifabric deploying Hyperledger Fabric onto K8S and running as K8S operators will allow hyperledger fabric network nodes with taking advantages of K8S operator benefit. This work will involve the participators to develop K8S operators, deploying K8S operator controllers and managing K8S operator life cycles, at the same time, the participators will be able to learn Fabric and Fabric deployment model. This is a great opportunity for anyone who would like to know K8S and know Hyperledger Fabric.	{go,yaml,ansible}	2022	Term 2	https://github.com/hyperledger-labs/minifabric	https://wiki.hyperledger.org/display/INTERN/Expand+Minifabric+with+k8s+operator+support	600000	1
40	9abcdd9a-a3fc-417d-a876-ea862c72e594	Hyperledger - Extend existing Iroha - Cactus integration	{"Hyperledger Cactus is a blockchain decentralized integration tool designed to allow users to securely integrate different blockchains started by companies Fujitsu and Accenture. Cactus has pluggable architecture which makes easy to integrate various blockchain by creating plugin, currently plugins for Fabric, Besu, Quorum are implemented. Cactus allows to transfer not only assets but also data between multiple blockchains. On the other hand Iroha (version 1.x) is great with asset management, and has functionality to store data, which makes those two projects a perfect fit!\n\nIn previous HL Summer Mentorship Project the integration was successfully made (link). As a result of the project commands and queries from HL Iroha were integrated, it was Iroha 1.2, but after that year multiple features and changes was added to Iroha (e.g. Healthcheck). What is more - not all nice to have features was integrated (list of missing features). It is also important to make the configuration flexible to make its possible to set up connection between two different blockchain networks (e.g. Iroha → Iroha).\n\nIn short, the project's goal is to:\n\n1. Implement missing features + tests + documentation\n2. Update Iroha version to newest in the integration.\n3. Make the plugin configurable\n4. Create tutorial demonstrating how to set up connection between multiple networks (at least Iroha - Iroha networks)"}	Hyperledger Cactus is a blockchain decentralized integration tool designed to allow users to securely integrate different blockchains started by companies Fujitsu and Accenture. Cactus has pluggable architecture which makes easy to integrate various blockchain by creating plugin, currently plugins for Fabric, Besu, Quorum are implemented. Cactus allows to transfer not only assets but also data between multiple blockchains. On the other hand Iroha (version 1.x) is great with asset management, and has functionality to store data, which makes those two projects a perfect fit!\n\nIn previous HL Summer Mentorship Project the integration was successfully made (link). As a result of the project commands and queries from HL Iroha were integrated, it was Iroha 1.2, but after that year multiple features and changes was added to Iroha (e.g. Healthcheck). What is more - not all nice to have features was integrated (list of missing features). It is also important to make the configuration flexible to make its possible to set up connection between two different blockchain networks (e.g. Iroha → Iroha).\n\nIn short, the project's goal is to:\n\n1. Implement missing features + tests + documentation\n2. Update Iroha version to newest in the integration.\n3. Make the plugin configurable\n4. Create tutorial demonstrating how to set up connection between multiple networks (at least Iroha - Iroha networks)	{javascript,openapi}	2022	Term 2	https://github.com/hyperledger/iroha	https://wiki.hyperledger.org/display/INTERN/Extend+existing+Iroha+-+Cactus+integration	300000	1
38	880772af-dafe-4721-9182-168827c22843	Hyperledger - Fabric-Ethereum token bridging	{"One of the key use cases of blockchain integration is asset bridging: in essence, \\"locking\\" an asset (typically, a native coin or token) in a smart contract on its authoritative ledger and making available corresponding, newly minted (wrapped",shadow,"...) assets on another. By now, bridging is supported by quite mature solutions in the cryptoworld; however, the same is not true for \\"consortial\\" distributed ledger technologies. At the same time, such functionality can be expected to become an important requirement in the not too distant future\n\n- Analysing the implementation approaches of standard \\"token models\\" (ERC20, ERC-721, ... - as far as this has been already done) in native Fabric chaincode and creating conceptual mappings between standard Ethereum token types and Fabric-native assets (including, but not limited to, the different authentication and authorization approaches).\n- Creating a brief review of the bridging approaches and mature technologies widely used in the cryptoworld.\n- Performing a requirement analysis for bridging assets from Fabric to Ethereum-based networks (and back), taking into account that the parties performing the bridging have to be explicitly given permission to do so by an authority and may be subject to regulatory requirements.\n- Based on the available open-source components, designing and prototyping a Fabric-Ethereum bridge fulfilling the requirements\n\nAs the proposed project bridges multiple fields, creating a centralized solution."}	One of the key use cases of blockchain integration is asset bridging: in essence, "locking" an asset (typically, a native coin or token) in a smart contract on its authoritative ledger and making available corresponding, newly minted (wrapped/shadow/...) assets on another. By now, bridging is supported by quite mature solutions in the cryptoworld; however, the same is not true for "consortial" distributed ledger technologies. At the same time, such functionality can be expected to become an important requirement in the not too distant future\n\n- Analysing the implementation approaches of standard "token models" (ERC20, ERC-721, ... - as far as this has been already done) in native Fabric chaincode and creating conceptual mappings between standard Ethereum token types and Fabric-native assets (including, but not limited to, the different authentication and authorization approaches).\n- Creating a brief review of the bridging approaches and mature technologies widely used in the cryptoworld.\n- Performing a requirement analysis for bridging assets from Fabric to Ethereum-based networks (and back), taking into account that the parties performing the bridging have to be explicitly given permission to do so by an authority and may be subject to regulatory requirements.\n- Based on the available open-source components, designing and prototyping a Fabric-Ethereum bridge fulfilling the requirements\n\nAs the proposed project bridges multiple fields, creating a centralized solution.	{ethereum,solidity,git,docker}	2022	Term 2	https://github.com/hyperledger/cactus/blob/main/whitepaper/whitepaper.md	https://wiki.hyperledger.org/display/INTERN/Fabric-Ethereum+token+bridging	420000	1
57	0f909209-0e75-48cf-8ab8-e8e6b04e5db0	Hyperledger - Git Commit Signing with DID's, Part Deux	{"In the summer of 2019, Hyperledger ran a mentorship geared towards getting the Git version control tool to understand and use cryptographic credentials in decentralized identity (DID) documents to sign and verify commits. The root cause analysis led the project in the direction of creating a patch set for Git that enabled Git to use any signing tool more easily rather than just its existing support for GnuPG. Work is still ongoing to get those patches landed into Git and out in the wild. This mentorship anticipates the completion of that work and extends the previous work with the construction of a software application that can be called by Git to sign","verify commits using credentials stored in DID docs."}	In the summer of 2019, Hyperledger ran a mentorship geared towards getting the Git version control tool to understand and use cryptographic credentials in decentralized identity (DID) documents to sign and verify commits. The root cause analysis led the project in the direction of creating a patch set for Git that enabled Git to use any signing tool more easily rather than just its existing support for GnuPG. Work is still ongoing to get those patches landed into Git and out in the wild. This mentorship anticipates the completion of that work and extends the previous work with the construction of a software application that can be called by Git to sign/verify commits using credentials stored in DID docs.	{python,c,rust}	2020	Term 2	https://github.com/hyperledger/indy-sdk/	https://wiki.hyperledger.org/display/INTERN/Git+Commit+Signing+with+DID%27s%2C+Part+Deux	600000	1
178	6ddbd86c-8d00-4eee-9f56-6f69c3b3eaa5	Hyperledger - Global Scouting of DLT / Blockchain Educational Opportunities	{"The Learning Materials and Development Working Group at Hyperledger is looking for two enthusiastic mentees willing to scout and map existing opportunities to learn about DLT and Blockchain around the globe.\n\nOur Wiki Page is the entry point to the Galaxy of Hyperledger. Our team keeps a Resource Library of Hyperledger Projects and Tools, Use Cases, Key Terms, Learning Materials for Special Interest Groups and Working Groups, Templates, Meetups, Labs, Marketing, and News. As well as a sinopsis of current events, announcements, community events, requirements for Best Practice Badges, GitHub resources. We proof-read edX Blockchain courses for Linux Foundation (Hyperledger). We have 50+ registered and recorded meeting agendas.\n\nWe want to identify and contact potential allies to link our inner Hyperledger Universe with the rest of the world. We need scouts to explore, register and map the outer territories beyond our community.\n\nMentees shall learn how to structure a taxonomy of DLT "," Blockchain Educational Opportunities, where to find them, and how to analyze their objectives and products.\n\nThey shall build a comprehensive Global Directory of DLT "," Blockchain Educational Programs, register their websites, identify their working teams, make initial contact, and identify their potential interest to become part of an Open Source Educational Alliance  with LMDWG, Hyperledger, and the Linux Foundation."}	The Learning Materials and Development Working Group at Hyperledger is looking for two enthusiastic mentees willing to scout and map existing opportunities to learn about DLT and Blockchain around the globe.\n\nOur Wiki Page is the entry point to the Galaxy of Hyperledger. Our team keeps a Resource Library of Hyperledger Projects and Tools, Use Cases, Key Terms, Learning Materials for Special Interest Groups and Working Groups, Templates, Meetups, Labs, Marketing, and News. As well as a sinopsis of current events, announcements, community events, requirements for Best Practice Badges, GitHub resources. We proof-read edX Blockchain courses for Linux Foundation (Hyperledger). We have 50+ registered and recorded meeting agendas.\n\nWe want to identify and contact potential allies to link our inner Hyperledger Universe with the rest of the world. We need scouts to explore, register and map the outer territories beyond our community.\n\nMentees shall learn how to structure a taxonomy of DLT / Blockchain Educational Opportunities, where to find them, and how to analyze their objectives and products.\n\nThey shall build a comprehensive Global Directory of DLT / Blockchain Educational Programs, register their websites, identify their working teams, make initial contact, and identify their potential interest to become part of an Open Source Educational Alliance  with LMDWG, Hyperledger, and the Linux Foundation.	{communication,scouting,mapping,dlt}	2021	Term 2	https://wiki.hyperledger.org/display/LMDWG/Learning+Materials+Development+Working+Group	https://wiki.hyperledger.org/pages/viewpage.action?pageId=41594697	600000	1
304	b1fb80d6-730a-4fd6-86c2-34365bea8e85	Hyperledger - GVCR: Secure Verifiable Credential Registries (VCR) for GitHub & GitLab	{"As conceptualized and standardized by the W3C, the Verifiable Credentials protocol is one of the three pillars of Self-Sovereign Identity, together with the Decentralized Identifiers protocol (DIDs) and Distributed Ledger Technology (or Blockchain). The project aims to design and build a verifiable credential registry (VCR) on GitHub repository, namely GitHub-based Verifiable Credential Registry (GVCR), by leveraging existing GitHub APIs, and other open-source tools provided by other Hyperledger projects, such as Aries, Indy, and Ursa. The basic architecture is already built. For more details about the conceptional design and workflows, please refer to the GitHub repository GitHub-VCR."}	As conceptualized and standardized by the W3C, the Verifiable Credentials protocol is one of the three pillars of Self-Sovereign Identity, together with the Decentralized Identifiers protocol (DIDs) and Distributed Ledger Technology (or Blockchain). The project aims to design and build a verifiable credential registry (VCR) on GitHub repository, namely GitHub-based Verifiable Credential Registry (GVCR), by leveraging existing GitHub APIs, and other open-source tools provided by other Hyperledger projects, such as Aries, Indy, and Ursa. The basic architecture is already built. For more details about the conceptional design and workflows, please refer to the GitHub repository GitHub-VCR.	{shell,git}	2022	Term 2	https://github.com/DIDman/github-vcr	https://wiki.hyperledger.org/pages/viewpage.action?pageId=62242656	300000	1
488	7874611c-245f-4b34-b922-dcfd65a50295	Hyperledger - Hyperledger Bevel documentation redesign	{"A documentation serves best when understood and designed based on user needs. Although Hyperledger Bevel current documentation is comprehensive and robust, it still requires improvement and can be much more effective and useful if designed based on user and community needs. The project scopes to structure the existing documentation of Hyperledger Bevel and redesign it from the perspective of user personas and their deployment needs. It would equally benefit product users and developers, and increase usability and adoption.\n\nLearning Objectives\n- Hyperledger Bevel concepts, features and operations\n- Documentation techniques and best practices\n- Hyperledger Bevel community, users and use-cases \n\nExpected Outcome\n- Complete redesigned of documentation based on the decided structure and theme"}	A documentation serves best when understood and designed based on user needs. Although Hyperledger Bevel current documentation is comprehensive and robust, it still requires improvement and can be much more effective and useful if designed based on user and community needs. The project scopes to structure the existing documentation of Hyperledger Bevel and redesign it from the perspective of user personas and their deployment needs. It would equally benefit product users and developers, and increase usability and adoption.\n\nLearning Objectives\n- Hyperledger Bevel concepts, features and operations\n- Documentation techniques and best practices\n- Hyperledger Bevel community, users and use-cases \n\nExpected Outcome\n- Complete redesigned of documentation based on the decided structure and theme	{markdown,python}	2023	Term 2	https://github.com/hyperledger/bevel	https://wiki.hyperledger.org/display/INTERN/Bevel%3A+Documentation+redesign	300000	1
306	f004af8d-b69c-4bb1-ba92-105f751c453a	Hyperledger - Identity Mixer Support for Fabric Gateway SDK for Java and Client API for Java	{"Idemix is a cryptographic protocol suite, which provides strong authentication as well as privacy-preserving features such as anonymity, the ability to transact without revealing the identity of the transactor, and unlinkability, the ability of a single identity to send multiple transactions without revealing that the transactions were sent by the same identity. More details here - https:","",hyperledger-fabric.readthedocs.io,en,release-2.2,"idemix.html\n\nCurrent JavaSDK doesn't support to store Idemix credentials in Hyperledger Fabric Wallet. Objective of this mentorship program to support wallet to store idimix identities.\n\n- Ability to store Idemix Identity in a Wallet by using Fabric Gateway SDK for Java\n- Ability to store Idemix Identity in a Wallet by using Fabric Gateway Client API for Java\n- Ability to use the Stored Idemix Identity from a Wallet to fire transaction to the Hyperledger Fabric Network in both Fabric Gateway SDK for Java and Fabric Gateway Client API for Java\n- Write Unit Tests and Add Examples in fabric-samples Repository"}	Idemix is a cryptographic protocol suite, which provides strong authentication as well as privacy-preserving features such as anonymity, the ability to transact without revealing the identity of the transactor, and unlinkability, the ability of a single identity to send multiple transactions without revealing that the transactions were sent by the same identity. More details here - https://hyperledger-fabric.readthedocs.io/en/release-2.2/idemix.html\n\nCurrent JavaSDK doesn't support to store Idemix credentials in Hyperledger Fabric Wallet. Objective of this mentorship program to support wallet to store idimix identities.\n\n- Ability to store Idemix Identity in a Wallet by using Fabric Gateway SDK for Java\n- Ability to store Idemix Identity in a Wallet by using Fabric Gateway Client API for Java\n- Ability to use the Stored Idemix Identity from a Wallet to fire transaction to the Hyperledger Fabric Network in both Fabric Gateway SDK for Java and Fabric Gateway Client API for Java\n- Write Unit Tests and Add Examples in fabric-samples Repository	{java}	2022	Term 2	https://github.com/hyperledger/fabric	https://wiki.hyperledger.org/display/INTERN/Identity+Mixer+Support+for+both+Fabric+Gateway+SDK+for+Java+and+Fabric+Gateway+Client+API+for+Java	300000	1
475	495a745c-5ebc-49a7-8eca-fee07a4618ec	Hyperledger - Implement a CLI for node interactions in Hyperledger Solang	{"Hyperledger Solang is a Solidity compiler written in Rust. It currently targets Substrate contracts pallet (Polkadot) and Solana smart contract runtimes. After successful compilation, users need to upload the compiled contract artifacts to a blockchain node, so it can be interacted with. At the time of writing, this needs to be done manually via third-party web front-ends. Which results in an inefficient and tiring process during iterative contract development.\n\nThe goal of this mentorship is to implement a Command Line Interface (CLI) for Solang that can be used for node interactions instead. This includes uploading and deploying contracts on-chain as well as submitting transactions to contracts."}	Hyperledger Solang is a Solidity compiler written in Rust. It currently targets Substrate contracts pallet (Polkadot) and Solana smart contract runtimes. After successful compilation, users need to upload the compiled contract artifacts to a blockchain node, so it can be interacted with. At the time of writing, this needs to be done manually via third-party web front-ends. Which results in an inefficient and tiring process during iterative contract development.\n\nThe goal of this mentorship is to implement a Command Line Interface (CLI) for Solang that can be used for node interactions instead. This includes uploading and deploying contracts on-chain as well as submitting transactions to contracts.	{blockchain,rust,git}	2023	Term 2	https://github.com/hyperledger/solang	https://wiki.hyperledger.org/display/INTERN/Implement+a+CLI+for+node+interactions+in+Hyperledger+Solang	300000	1
612	93dffb9b-dcaa-41ad-b4fe-53a5accb89b5	Hyperledger - Implement an SSA intermediate representation for the Solang compiler	{"Solang is an open-source Solidity compiler written in Rust. It generates an intermediate representation in a Control Flow Graph (CFG) for each function in a Solidity contract during compilation. The CFG contains high level expressions and instructions, which help in some aspects of code analysis and optimization, but are not suitable for more advanced optimization passes, such as liveness analysis and partial redundancy elimination.\n\nIn this project, mentees will have hands-on experience implementing a modern Single Static Assignment intermediate representation for the Solang compiler. This addition will bring many advantages for the compiler, such as the possibility to break down complex constructs into lower level instructions and the facilitation of code analysis. We propose for the mentorship the introduction of a new intermediate representation to live between the existing one (the CFG representation - read more about it at the Additional Information section below) and the LLVM-IR. It should feature three-address code in a Single Static Assignment (SSA) minimal form. The construction of such a form happens in two steps:\n\nPhi function insertion\nVariable renaming\nThe objective is to decouple the types and expressions we have used for the AST from the existing CFG intermediate representation. In addition, the new low level intermediate code must contain assertions that ensure the instructions are created with valid operands and expressions."}	Solang is an open-source Solidity compiler written in Rust. It generates an intermediate representation in a Control Flow Graph (CFG) for each function in a Solidity contract during compilation. The CFG contains high level expressions and instructions, which help in some aspects of code analysis and optimization, but are not suitable for more advanced optimization passes, such as liveness analysis and partial redundancy elimination.\n\nIn this project, mentees will have hands-on experience implementing a modern Single Static Assignment intermediate representation for the Solang compiler. This addition will bring many advantages for the compiler, such as the possibility to break down complex constructs into lower level instructions and the facilitation of code analysis. We propose for the mentorship the introduction of a new intermediate representation to live between the existing one (the CFG representation - read more about it at the Additional Information section below) and the LLVM-IR. It should feature three-address code in a Single Static Assignment (SSA) minimal form. The construction of such a form happens in two steps:\n\nPhi function insertion\nVariable renaming\nThe objective is to decouple the types and expressions we have used for the AST from the existing CFG intermediate representation. In addition, the new low level intermediate code must contain assertions that ensure the instructions are created with valid operands and expressions.	{compiler,rust}	2023	Term 2	https://github.com/hyperledger/solang	https://wiki.hyperledger.org/display/INTERN/Implement+iroha-cpp+library+for+Hyperledger+Iroha+1	360000	1
205	51f4fceb-aeb4-490f-a525-4a547112ebaf	Hyperledger - Implement cross chain contract invocation using 'ServiceMesh' way	{"Since permissioned blockchain has been adopted widely in various industries, the need to integrate different permissioned blockchains rises up recently. The two core problems of this case are chain contract interoperability and transaction atomic. \n\nMany frameworks and solutions proposed currently are a bit complex. We introduced a lightweight protocol and reference implementation inspired by the ‘ServiceMesh’ pattern in the microservice integration area. Combine with a stateless off-chain relay and an extended on-chain contract development kit as a ‘SideCar’ for different permissioned blockchains. \n\nContract developers can focus on writing business logic since the on-chain contract development kit uses an AOP(aspect-oriented programming) approach to delegate the contract interoperability. In addition it also provides a lock API for locking contract state which can be used in two-phase commit cases. The off-chain relay uses an event based architecture to coordinate different permissioned blockchains.\n\nLearning Outcome\n1) Implement the on-chain contract development kit for Hyperledger fabric using JAVA.\n2) Implement the on-chain contract development kit for Consensys quorum or Hyperledger Besu using Solidity.\n3) Implement the off-chain relay plugin for Consensys quorum or Hyperledger Besu."}	Since permissioned blockchain has been adopted widely in various industries, the need to integrate different permissioned blockchains rises up recently. The two core problems of this case are chain contract interoperability and transaction atomic. \n\nMany frameworks and solutions proposed currently are a bit complex. We introduced a lightweight protocol and reference implementation inspired by the ‘ServiceMesh’ pattern in the microservice integration area. Combine with a stateless off-chain relay and an extended on-chain contract development kit as a ‘SideCar’ for different permissioned blockchains. \n\nContract developers can focus on writing business logic since the on-chain contract development kit uses an AOP(aspect-oriented programming) approach to delegate the contract interoperability. In addition it also provides a lock API for locking contract state which can be used in two-phase commit cases. The off-chain relay uses an event based architecture to coordinate different permissioned blockchains.\n\nLearning Outcome\n1) Implement the on-chain contract development kit for Hyperledger fabric using JAVA.\n2) Implement the on-chain contract development kit for Consensys quorum or Hyperledger Besu using Solidity.\n3) Implement the off-chain relay plugin for Consensys quorum or Hyperledger Besu.	{go,java,solidity,fabric,besu,quorum}	2021	Term 2	https://github.com/GrapeBaBa/mesher	https://wiki.hyperledger.org/display/INTERN/Implement+cross+chain+contract+invocation+using+ServiceMesh+way	300000	1
608	ccd3fc5f-a0a9-441f-bd4c-5caae8ab6509	CNCF - Kubescape: Upgrade the documentation publishing pipeline for Kubescape controls	{"Kubescape's control library includes more than 200 controls, tests that codify Kubernetes best practices derived from the most prevalent security frameworks in the industry. Metadata in the controls is used to generate documentation pages in the ARMO website. This project will update this automation to make this control documentation available on kubescape.io.\n- Expected Outcome: A full set of documentation for Kubescape controls on kubescape.io. Stretch goals include better README-style documentation inside the repository, and documentation pages on how the controls, frameworks and tests relate."}	Kubescape's control library includes more than 200 controls, tests that codify Kubernetes best practices derived from the most prevalent security frameworks in the industry. Metadata in the controls is used to generate documentation pages in the ARMO website. This project will update this automation to make this control documentation available on kubescape.io.\n- Expected Outcome: A full set of documentation for Kubescape controls on kubescape.io. Stretch goals include better README-style documentation inside the repository, and documentation pages on how the controls, frameworks and tests relate.	{python,rego}	2023	Term 3	https://github.com/kubescape/kubescape/issues/1302	https://github.com/kubescape	0	150
1	f3395b50-c968-459b-b93f-5504fde18b25	Hyperledger - Implement iroha-cpp library for Hyperledger Iroha 1	{"Hyperledger Iroha 1 is blockchain implemented in C++. To interact with Irohas' nodes (perform commands and queries) there are few client libraries - iroha-python, iroha-javascript, iroha-java, iroha-ios. Despite the fact that Iroha has implemented iroha-cli in C++ there is no client library in C++. What is more iroha-cli is deprecated and not all commands are fully supported.\n\nThe internship project in short words is to refactor iroha-cli and extract from its iroha-cpp library. Using the library would require project of entire library (inspiration are rest of libraries).\n\nOptionally the library should be compatible with C language (extern \\"C\\") to make it easy to use the library from multiple programming languages."}	Hyperledger Iroha 1 is blockchain implemented in C++. To interact with Irohas' nodes (perform commands and queries) there are few client libraries - iroha-python, iroha-javascript, iroha-java, iroha-ios. Despite the fact that Iroha has implemented iroha-cli in C++ there is no client library in C++. What is more iroha-cli is deprecated and not all commands are fully supported.\n\nThe internship project in short words is to refactor iroha-cli and extract from its iroha-cpp library. Using the library would require project of entire library (inspiration are rest of libraries).\n\nOptionally the library should be compatible with C language (extern "C") to make it easy to use the library from multiple programming languages.	{git,protobuf,cmake,python,json}	2022	Term 2	https://github.com/hyperledger/iroha	https://wiki.hyperledger.org/display/INTERN/Implement+iroha-cpp+library+for+Hyperledger+Iroha+1	0	1
505	8a6b5853-369c-48e6-9445-98520a8c28dc	Hyperledger - Iroha 2 FFI client library bindings	{"Currently, the Hyperledger Iroha v2 clients are linked statically or via a sophisticated code generation process that is both brittle and necessitates an inordinate amount of work for each ABI-breaking change to the Hyperledger Iroha v2 core library.\n\nInstead, we plan to create a shared C-ABI-based dynamically linked library that will be used by all clients and SDKs and the reference implementation of a Rust client: iroha_client_cli.\n\nLearning Objectives\n\nThe mentee shall learn about\n- The C-ABI\n- The Rust facilities for dynamic linkage\n- unsafe programming in Rust\n- low-level structure representations\n- Undefined behaviour\n\nExpected Outcome\n\nThis project would enable the dynamic linking of SDK, resulting in an improved SDK ecosystem and better support for existing languages like Python or TypeScript."}	Currently, the Hyperledger Iroha v2 clients are linked statically or via a sophisticated code generation process that is both brittle and necessitates an inordinate amount of work for each ABI-breaking change to the Hyperledger Iroha v2 core library.\n\nInstead, we plan to create a shared C-ABI-based dynamically linked library that will be used by all clients and SDKs and the reference implementation of a Rust client: iroha_client_cli.\n\nLearning Objectives\n\nThe mentee shall learn about\n- The C-ABI\n- The Rust facilities for dynamic linkage\n- unsafe programming in Rust\n- low-level structure representations\n- Undefined behaviour\n\nExpected Outcome\n\nThis project would enable the dynamic linking of SDK, resulting in an improved SDK ecosystem and better support for existing languages like Python or TypeScript.	{rust}	2023	Term 2	https://github.com/hyperledger/iroha	https://wiki.hyperledger.org/pages/viewpage.action?pageId=80778579	360000	1
510	6904ed62-2022-4451-bbc7-6bc0f940586f	Hyperledger - Onboarding Mentor and Mentee Program	{"In collaboration with the Mentors, the Mentee can provide valuable updates to the Hyperledger Onboarding Documentation and the Hyperledger Start Here guide (https:","",start-here.hyperledger.org,") to help new community members gain a quicker and deeper understanding of Hyperledger blockchain projects and technology.\n\nThe expected outcome would be for the mentee to update the Hyperledger Onboarding Documentation and the Hyperledger Start Here guide (https:","",start-here.hyperledger.org,") to help new community members gain a quicker and deeper understanding of Hyperledger blockchain projects and technology.  This would be through the development of content and graphics that are relevant to the onboarding process, based on research and outreach to members of the Hyperledger Community.  The mentee will create personas and process flow diagrams to arrive at the best framework for consolidating the Hyperledger onboarding documentation into a unified and easy to access resource.  Update the dashboard and the landing page for driving the user engagement.  Develop the ability to search events and happenings within the Hyperledger community."}	In collaboration with the Mentors, the Mentee can provide valuable updates to the Hyperledger Onboarding Documentation and the Hyperledger Start Here guide (https://start-here.hyperledger.org/) to help new community members gain a quicker and deeper understanding of Hyperledger blockchain projects and technology.\n\nThe expected outcome would be for the mentee to update the Hyperledger Onboarding Documentation and the Hyperledger Start Here guide (https://start-here.hyperledger.org/) to help new community members gain a quicker and deeper understanding of Hyperledger blockchain projects and technology.  This would be through the development of content and graphics that are relevant to the onboarding process, based on research and outreach to members of the Hyperledger Community.  The mentee will create personas and process flow diagrams to arrive at the best framework for consolidating the Hyperledger onboarding documentation into a unified and easy to access resource.  Update the dashboard and the landing page for driving the user engagement.  Develop the ability to search events and happenings within the Hyperledger community.	{html,css,javascript,git,react}	2023	Term 2	https://github.com/hyperledger	https://wiki.hyperledger.org/pages/viewpage.action?pageId=80778789	300000	1
300	7fbdf9c5-81f3-454e-9ae7-542aa3bf2db7	Hyperledger - Solang Solidity Compiler optimizations and error handling	{"Solang is a Solidity compiler that targets several blockchains, including Solana, Substrate and ewasm. The mentorship project consists of three goals that can provide mentees with a broad knowledge of a compiler's inner-workings. The three tasks are in different stages of the compiler.\n\nSolang’s parser analysis halts compilation entirely when there is any invalid expression or function declaration. This should be improved so that it continues parsing the Solidity contract after encountering an error, providing better diagnostics (warnings and errors) to the developer. The parser utilizes Lalrpop and the mentee would be using its error handling capabilities to implement a solution.\nOften the length of a dynamically allocated array is known at compile time as the allocation happens with a constant. When the length is required at some later point (e.g. for bounds checks for array indexing), the length is retrieved again. Mentees will add an optimization to make the length a compile time constant.\nSolang has an implementation of multiplication of integers of 256 and 128 bit. This does not have overflow detection. This requires improving the multiplication in the solang stdlib."}	Solang is a Solidity compiler that targets several blockchains, including Solana, Substrate and ewasm. The mentorship project consists of three goals that can provide mentees with a broad knowledge of a compiler's inner-workings. The three tasks are in different stages of the compiler.\n\nSolang’s parser analysis halts compilation entirely when there is any invalid expression or function declaration. This should be improved so that it continues parsing the Solidity contract after encountering an error, providing better diagnostics (warnings and errors) to the developer. The parser utilizes Lalrpop and the mentee would be using its error handling capabilities to implement a solution.\nOften the length of a dynamically allocated array is known at compile time as the allocation happens with a constant. When the length is required at some later point (e.g. for bounds checks for array indexing), the length is retrieved again. Mentees will add an optimization to make the length a compile time constant.\nSolang has an implementation of multiplication of integers of 256 and 128 bit. This does not have overflow detection. This requires improving the multiplication in the solang stdlib.	{compiler,algorithm,rust}	2022	Term 2	https://github.com/hyperledger-labs/solang	https://wiki.hyperledger.org/display/INTERN/Solang+Solidity+Compiler+optimizations+and+error+handling	300000	1
42	47d16213-d73a-4e24-bf8f-3deaf499d6d4	Hyperledger - The Giving Chain	{"The Giving Chain is a Social Impact project created by the Hyperledger Princeton Meetup. The Purpose of the project is to decentralize local charity giving by creating the Giving Chain and the R2D application. The application will use participants cell phones  to create an asset to be donated. The R2D (Recipient to Donor) then notifies the participant in the giving chain that an asset has been created and ready to be donated. The asset is then track till it is received by someone with need. This donation system lets donors track donation as well as creating a dignified way in which people can receive assistance.\n\nLearning Objectives\n1) Create a supply chain blockchain.\n2) Examine the relationships needed to create a Social Impact Project \n\nExpected Outcome\nAn operational blockchain that will revolutionize local charity giving.  can also be adapted for targeted disaster relief"}	The Giving Chain is a Social Impact project created by the Hyperledger Princeton Meetup. The Purpose of the project is to decentralize local charity giving by creating the Giving Chain and the R2D application. The application will use participants cell phones  to create an asset to be donated. The R2D (Recipient to Donor) then notifies the participant in the giving chain that an asset has been created and ready to be donated. The asset is then track till it is received by someone with need. This donation system lets donors track donation as well as creating a dignified way in which people can receive assistance.\n\nLearning Objectives\n1) Create a supply chain blockchain.\n2) Examine the relationships needed to create a Social Impact Project \n\nExpected Outcome\nAn operational blockchain that will revolutionize local charity giving.  can also be adapted for targeted disaster relief	{blockchain}	2021	Term 2	https://github.com/hyperledger/fabric	https://wiki.hyperledger.org/display/INTERN/The+Giving+Chain	302000	1
572	02031dab-d08f-4ccf-89ca-ec38c7ca6257	Hyperledger Collaborative Learning - A Distributed Smart Contract Management - Unpaid	{"Cello, as a decentralized BaaS, has been able to help users create blockchain networks and achieve channel management. Cello will help users to install, instantiate and test smart contracts.\n\nLearning Objectives\n- Work closely with community experts and developers to learn the open-source culture and skills;\n- Learn the advanced knowledge inside the blockchain and distributed ledgers;\n- Practice hand-on experience with web application design and implement;\n- Cultivate security awareness in software development process."}	Cello, as a decentralized BaaS, has been able to help users create blockchain networks and achieve channel management. Cello will help users to install, instantiate and test smart contracts.\n\nLearning Objectives\n- Work closely with community experts and developers to learn the open-source culture and skills;\n- Learn the advanced knowledge inside the blockchain and distributed ledgers;\n- Practice hand-on experience with web application design and implement;\n- Cultivate security awareness in software development process.	{blockchain,python}	2023	Term 3	https://github.com/hyperledger/cello	https://wiki.hyperledger.org/display/CLP/A+Distributed+Smart+Contract+Management	0	180
567	4e36b5fb-ea0c-4cba-aedd-05e2744ced1e	Hyperledger Collaborative Learning - Design and Spec DIDMan based openwallet - Unpaid	{"Designing an open wallet based on the DIDMan specification requires consideration of several key components. These include authentication, DID management, cryptographic operations, interoperability, user interface, security, and backup and recovery. The OpenWallet must have a secure authentication mechanism and support the DIDMan specification to manage DIDs, their associated keys, and metadata. The wallet must also support cryptographic operations, have a user-friendly interface, ensure security, and allow for backup and recovery. By adhering to the DIDMan specification, the wallet can ensure interoperability with other compliant systems and promote the adoption of decentralized identity technologies.\nExpected Outcome\n- The expected outcomes of designing and spec-ing an OpenWallet based on the DIDMan specification include developing a deep understanding of decentralized identity systems, gaining practical experience in implementation and cryptography, and prioritizing security and user experience.\n- Additionally, students can expect to learn about interoperability, open-source contribution, technical documentation, and real-world applications.\n- By achieving these outcomes, students will be equipped with the skills and knowledge needed to make meaningful contributions to the growing field of decentralized identity technologies."}	Designing an open wallet based on the DIDMan specification requires consideration of several key components. These include authentication, DID management, cryptographic operations, interoperability, user interface, security, and backup and recovery. The OpenWallet must have a secure authentication mechanism and support the DIDMan specification to manage DIDs, their associated keys, and metadata. The wallet must also support cryptographic operations, have a user-friendly interface, ensure security, and allow for backup and recovery. By adhering to the DIDMan specification, the wallet can ensure interoperability with other compliant systems and promote the adoption of decentralized identity technologies.\nExpected Outcome\n- The expected outcomes of designing and spec-ing an OpenWallet based on the DIDMan specification include developing a deep understanding of decentralized identity systems, gaining practical experience in implementation and cryptography, and prioritizing security and user experience.\n- Additionally, students can expect to learn about interoperability, open-source contribution, technical documentation, and real-world applications.\n- By achieving these outcomes, students will be equipped with the skills and knowledge needed to make meaningful contributions to the growing field of decentralized identity technologies.	{dlt}	2023	Term 3	https://github.com/DIDman/DRman	https://wiki.hyperledger.org/display/CLP/Design+and+Spec+DIDMan+based+openwallet	0	180
565	5d6ee2ff-8d0e-400b-8869-81b5011538e1	Hyperledger Collaborative Learning - Fabric Token SDK Lab Support for Hyperledger Caliper B - Unpaid	{"The Fabric Token SDK allows developers to create token-based distributed applications on Hyperledger Fabric. While the Fabric Token SDK offers many features (i.e., configurable privacy, p2p transaction orchestration, atomic swaps, and audits,...), performance is yet another important aspect. The Hyperledger Caliper project is a blockchain performance benchmark framework, which allows the evaluation of blockchain solutions. How about using Caliper to benchmark token-based applications built on top of the Fabric Token SDK?\n\n\nThe goal of this project is the design and the implementation of a Caliper plugin to evaluate applications using the Fabric Token SDK. This includes the creation of appropriate documentation and performance evaluation of sample applications. The project requires a solid understanding of theory and practice of software development using Golang and Node.JS, distributed systems, and performance benchmarking. During the project you will get hands-on experience with the deployment of Fabric networks, the transaction lifecycle, the Fabric Token SDK, and Hyperledger Caliper."}	The Fabric Token SDK allows developers to create token-based distributed applications on Hyperledger Fabric. While the Fabric Token SDK offers many features (i.e., configurable privacy, p2p transaction orchestration, atomic swaps, and audits,...), performance is yet another important aspect. The Hyperledger Caliper project is a blockchain performance benchmark framework, which allows the evaluation of blockchain solutions. How about using Caliper to benchmark token-based applications built on top of the Fabric Token SDK?\n\n\nThe goal of this project is the design and the implementation of a Caliper plugin to evaluate applications using the Fabric Token SDK. This includes the creation of appropriate documentation and performance evaluation of sample applications. The project requires a solid understanding of theory and practice of software development using Golang and Node.JS, distributed systems, and performance benchmarking. During the project you will get hands-on experience with the deployment of Fabric networks, the transaction lifecycle, the Fabric Token SDK, and Hyperledger Caliper.	{git,bash,docker,linux,golang}	2023	Term 3	https://github.com/hyperledger/caliper	https://wiki.hyperledger.org/display/CLP/Fabric+Token+SDK+Lab+Support+for+Hyperledger+Caliper+Benchmark	0	180
563	97580629-5b4e-40dc-8588-f82d60611dbd	Hyperledger Collaborative Learning - Hyperledger Fabric Gateway Support for Fabric Private - Unpaid	{"Fabric Private Chaincode (FPC) leverages modern Confidential Computing technology, such as Intel SGX, to enhance the protection of chaincode and their data while executed at the endorsing peers. Client applications interact with their private chaincode via the FPC Client SDK, which is responsible to encrypt and authenticate the invocation arguments before being sent to the endorsing peers. However, the FPC Client SDK currently does not support the new Fabric Gateway API that simplifies the transaction flow for the clients.\n\nThe goal of this mentorship project is the design and the implementation of the FPC Client SDK using the new Fabric Gateway API. This also includes the creation of appropriate samples and documentation. The project requires a solid understanding of theory and practice of software development using Golang, distributed systems, and security. During the project you will get hands-on experience with the deployment of Fabric networks, the transaction lifecycle, confidential computing technology."}	Fabric Private Chaincode (FPC) leverages modern Confidential Computing technology, such as Intel SGX, to enhance the protection of chaincode and their data while executed at the endorsing peers. Client applications interact with their private chaincode via the FPC Client SDK, which is responsible to encrypt and authenticate the invocation arguments before being sent to the endorsing peers. However, the FPC Client SDK currently does not support the new Fabric Gateway API that simplifies the transaction flow for the clients.\n\nThe goal of this mentorship project is the design and the implementation of the FPC Client SDK using the new Fabric Gateway API. This also includes the creation of appropriate samples and documentation. The project requires a solid understanding of theory and practice of software development using Golang, distributed systems, and security. During the project you will get hands-on experience with the deployment of Fabric networks, the transaction lifecycle, confidential computing technology.	{go}	2023	Term 3	https://github.com/hyperledger/fabric	https://wiki.hyperledger.org/display/CLP/BiniBFT+-+An+Optimized+BFT+on+Fabric	0	180
570	7c936217-2dbf-479e-9698-c2844ac904de	Hyperledger Collaborative Learning - Integrate new BFT protocol with Fabric - Unpaid	{"DLS consensus protocol is a BFT protocol that has been approved and hosted in the Hyperledger Lab to be integrated into Hyperledger Fabric for consideration as one of the ordering service BFT protocols. BDLS is an innovative BFT consensus algorithm that features safety and liveness by presenting a mathematically proven secure BFT protocol that is resilient in open networks such as the Internet. BDLS overcomes many problems, such as DoS attacks and the deadlock problem caused by unreliable p2p broadcast channels. These problems are all very relevant to existing realistic open network scenarios and are the focus of extensive work in improving Internet security. Still, it is an area largely ignored by most in mainstream BFT protocol design.\n\nLearning Objectives\nConsensus algorithm mechanism, Distributed messaging system, ability to integrate new consensus algorithm in the Orderer service of Fabric \n\nExpected Outcome\nBDLS consensus protocol is a BFT protocol that has been submitted for consideration to be integrated into Hyperledger Fabric as an ordering service."}	DLS consensus protocol is a BFT protocol that has been approved and hosted in the Hyperledger Lab to be integrated into Hyperledger Fabric for consideration as one of the ordering service BFT protocols. BDLS is an innovative BFT consensus algorithm that features safety and liveness by presenting a mathematically proven secure BFT protocol that is resilient in open networks such as the Internet. BDLS overcomes many problems, such as DoS attacks and the deadlock problem caused by unreliable p2p broadcast channels. These problems are all very relevant to existing realistic open network scenarios and are the focus of extensive work in improving Internet security. Still, it is an area largely ignored by most in mainstream BFT protocol design.\n\nLearning Objectives\nConsensus algorithm mechanism, Distributed messaging system, ability to integrate new consensus algorithm in the Orderer service of Fabric \n\nExpected Outcome\nBDLS consensus protocol is a BFT protocol that has been submitted for consideration to be integrated into Hyperledger Fabric as an ordering service.	{go,raft}	2023	Term 3	https://github.com/hyperledger-labs/bdls	https://wiki.hyperledger.org/display/CLP/Integrate+new+BFT+protocol+%28BDLS+consensus%29+with+Fabric	0	180
617	179e7834-cb89-47a1-8332-05bb670f4401	Hyperledger Collaborative Learning - Student chapter society onboarding and engagement - Unpaid	{"The Hyperledger Foundation is seeking a mentee to assist the Hyperledger India chapter's student society in improving their new student chaptor society membership and onboarding, marketing, and technical deck development efforts. The mentee will help in organize developer events and conduct research to identify potential opportunities for increasing new student chaptor society member onboarding and developer engagement. The mentee will be responsible for analyzing the education space, monitoring social media platforms such as LinkedIn and YouTube, and creating technical and promotional materials for outreach activities. In addition, the mentee will work with the Lead, student chapter society to conduct market research and analysis on the latest trends in developer engagement strategies. The mentee will also contribute to the content calendar by posting on social media platforms regularly and monitoring analytics with the team to identify potential growth patterns and viable ideas."}	The Hyperledger Foundation is seeking a mentee to assist the Hyperledger India chapter's student society in improving their new student chaptor society membership and onboarding, marketing, and technical deck development efforts. The mentee will help in organize developer events and conduct research to identify potential opportunities for increasing new student chaptor society member onboarding and developer engagement. The mentee will be responsible for analyzing the education space, monitoring social media platforms such as LinkedIn and YouTube, and creating technical and promotional materials for outreach activities. In addition, the mentee will work with the Lead, student chapter society to conduct market research and analysis on the latest trends in developer engagement strategies. The mentee will also contribute to the content calendar by posting on social media platforms regularly and monitoring analytics with the team to identify potential growth patterns and viable ideas.	{blockchain,dlt}	2023	Term 3	https://github.com/hyperledger	https://wiki.hyperledger.org/display/CLP/Student+chapter+society+onboarding+and+engagement	0	180
598	1e67c90b-de3e-4c4e-a2be-a5583a948864	CNCF - Jaeger: Combine three distinct graph views in Jaeger UI into one	{"Jaeger UI provides several views to visualize service dependencies, also known as service topology maps. However, these views are using different drawing libraries, resulting in very different look & feel and inconsistent experience. One of the views is using a `plexus` library that was purposely built as part of Jaeger UI that provides rich capabilities for displaying graphs, which may be a good candidate for the other views.\n- Expected Outcomes:\n  - Remove the dependency on react-vis library (https:","",github.com,jaegertracing,jaeger-ui,issues,"1597).\n  - Use a single library for graph visualizations.\n  - Provide consistent look and feel of different graph views."}	Jaeger UI provides several views to visualize service dependencies, also known as service topology maps. However, these views are using different drawing libraries, resulting in very different look & feel and inconsistent experience. One of the views is using a `plexus` library that was purposely built as part of Jaeger UI that provides rich capabilities for displaying graphs, which may be a good candidate for the other views.\n- Expected Outcomes:\n  - Remove the dependency on react-vis library (https://github.com/jaegertracing/jaeger-ui/issues/1597).\n  - Use a single library for graph visualizations.\n  - Provide consistent look and feel of different graph views.	{typescript,javascript,npm,yarn}	2023	Term 3	https://github.com/jaegertracing/jaeger-ui/issues/1466	https://www.jaegertracing.io/	0	178
542	b8009398-1252-4f63-82fe-363846ccc11d	CNCF - Jaeger: Upgrade Jaeger's internal telemetry to OpenTelemetry	{"Description: historically, the Jaeger backend used the OpenTracing API, with Jaeger's own Go SDK `jaeger-client-go`, for instrumenting its own internals for distributed tracing. Since Jaeger's SDKs have been deprecated, we want to upgrade the Jaeger backend to use the OpenTelemetry tracing API and SDK directly.\n- Expected Outcome:\n  - Replace the use of OpenTracing API with OpenTelemetry\n  - Remove `jaeger-client-go` and `jaeger-lib` as dependencies\n  - Remove `opentracing-go` and `opentracing-contrib","*` as dependencies\n  - Switch to standard instrumentation libraries where available (e.g. for HTTP, gRPC)\n  - Rethink","rework `crossdock` integration tests to test end-to-end flow with OpenTelemetry data\n  - Publish a blog post on medium.com","jaegertracing documenting the experience"}	Description: historically, the Jaeger backend used the OpenTracing API, with Jaeger's own Go SDK `jaeger-client-go`, for instrumenting its own internals for distributed tracing. Since Jaeger's SDKs have been deprecated, we want to upgrade the Jaeger backend to use the OpenTelemetry tracing API and SDK directly.\n- Expected Outcome:\n  - Replace the use of OpenTracing API with OpenTelemetry\n  - Remove `jaeger-client-go` and `jaeger-lib` as dependencies\n  - Remove `opentracing-go` and `opentracing-contrib/*` as dependencies\n  - Switch to standard instrumentation libraries where available (e.g. for HTTP, gRPC)\n  - Rethink/rework `crossdock` integration tests to test end-to-end flow with OpenTelemetry data\n  - Publish a blog post on medium.com/jaegertracing documenting the experience	{go}	2023	Term 2	https://github.com/jaegertracing/jaeger/issues/3381	https://www.jaegertracing.io/	300000	178
581	83cc55fe-b97a-4195-8dd2-cc9aed7e509c	CNCF - Jaeger: Upgrade Jaeger UI to the latest version of React.js	{"Jaeger UI is built on React. While we are seemingly already on v18.x of React, the upgrade was not done across the board and some other dependencies are still lagging behind, e.g. `\\"@types","react\\": \\"16.8.7\\"`. It's also blocking upgrades of other dependencies. This project is likely to involve a substantial amount of code contribution, as certain upgrade require fixing the code to use the new APIs, and sometimes we may run into dependencies that are EOL and need to be replaced altogether.\n- Expected Outcome: Ideal outcome is to have _all_ dependencies upgraded to the latest versions (with the help of @dependabot) and fix all deprecation warnings during the build. But incremental progress towards that goal is also acceptable."}	Jaeger UI is built on React. While we are seemingly already on v18.x of React, the upgrade was not done across the board and some other dependencies are still lagging behind, e.g. `"@types/react": "16.8.7"`. It's also blocking upgrades of other dependencies. This project is likely to involve a substantial amount of code contribution, as certain upgrade require fixing the code to use the new APIs, and sometimes we may run into dependencies that are EOL and need to be replaced altogether.\n- Expected Outcome: Ideal outcome is to have _all_ dependencies upgraded to the latest versions (with the help of @dependabot) and fix all deprecation warnings during the build. But incremental progress towards that goal is also acceptable.	{typescript,javascript,npm,yarn}	2023	Term 3	https://github.com/jaegertracing/jaeger-ui/issues/998	https://www.jaegertracing.io/	0	178
278	aedf6276-0aff-417c-96a6-ecc94697e378	CNCF - Karmada: Dashboard development	{"Description: The initial version of karmada-dashboard just getting on board, and more pages waiting for development."}	Description: The initial version of karmada-dashboard just getting on board, and more pages waiting for development.	{go,kubernetes}	2022	Term 1	https://github.com/karmada-io/dashboard/issues/10	https://karmada.io/	300000	124
381	40b17a86-e470-4406-b7f0-731e689a39f4	CNCF - Karmada: Enable configurable resource interpreter	{"Now Resource Interpreter framework enabled both built-in and customized interpreter, we are going to provide a way for people customize the interpreter by applying a configuration."}	Now Resource Interpreter framework enabled both built-in and customized interpreter, we are going to provide a way for people customize the interpreter by applying a configuration.	{go,kubernetes,lua}	2022	Term 3	https://github.com/karmada-io/karmada/issues/2371	https://karmada.io/	0	124
600	60b43efd-79e0-457e-989f-d4d59d55d8a6	CNCF - Karmada: Karmada supports promote dependent resources automatically	{"Provide an automatic promotion mechanism for dependent resources in karmadactl. When promoting a resource, all the resources that it depends on will be automatically promoted as well. For example, promoting the Secret that is dependent by a Deployment.\n- Expected Outcome:\n  - Technical Documentation: design description and analysis\n  - Function Implementation: support promote the dependent resources automatically\n  - Test coverage: add test cases to cover new functions"}	Provide an automatic promotion mechanism for dependent resources in karmadactl. When promoting a resource, all the resources that it depends on will be automatically promoted as well. For example, promoting the Secret that is dependent by a Deployment.\n- Expected Outcome:\n  - Technical Documentation: design description and analysis\n  - Function Implementation: support promote the dependent resources automatically\n  - Test coverage: add test cases to cover new functions	{go}	2023	Term 3	https://github.com/karmada-io/karmada/issues/3842	https://karmada.io/	0	124
274	3adb1a6d-73db-44db-8ae8-bf57367e345f	CNCF - Karmada: Refactor get command to leverage aggregated API	{"Description: Now karmadactl get command retrieves resources by Cluster token stored in Cluster object, we want to refactor it to leverage the Aggregated API."}	Description: Now karmadactl get command retrieves resources by Cluster token stored in Cluster object, we want to refactor it to leverage the Aggregated API.	{go,kubernetes}	2022	Term 1	https://github.com/karmada-io/karmada/issues/1329	https://karmada.io/	300000	124
256	6b2d49dd-fcd3-480e-838d-7310d63c5823	CNCF - Karmada: Refactor the scheduler framework	{"Description: Refactor the framework of karmada-scheduler to make it easier to extend and adopt more scheduling policies."}	Description: Refactor the framework of karmada-scheduler to make it easier to extend and adopt more scheduling policies.	{go,kubernetes}	2022	Term 1	https://github.com/karmada-io/karmada/issues/1330	https://karmada.io/	300000	124
534	830eb064-cf8a-4a8e-bba3-97d429a6ca79	CNCF - Knative: Porting Knative Serving to Microshift	{"More and more workload is moving towards running on the edge. We saw experiments running Kubernetes on vehicles, fighter jets, 5G antenna and various far edge, near edge and fat edge environments. We would like to see what the challenges are when Knative is run in a resource limited environment. While there are multiple edge-friendly Kubernetes distributions, we would like to see [Microshift](https:","",github.com,openshift,"microshift) used as the base platform. Knative consists of Serving and Eventing modules but focusing on Knative Serving as a first step should be more approachable. The project consists of 2 stages. First one is to run Knative on Microshift with minimal resources. This requires finding out problems here, solving them. A stretch goal is to find out what happens with architectures other than x86_64.\n- Expected outcome:  Having Knative Serving with an ingress layer running on top of Microshift. Having a Hello-World Knative Service running on top. Finding issues blocking the edge setup, and possibly fixing them."}	More and more workload is moving towards running on the edge. We saw experiments running Kubernetes on vehicles, fighter jets, 5G antenna and various far edge, near edge and fat edge environments. We would like to see what the challenges are when Knative is run in a resource limited environment. While there are multiple edge-friendly Kubernetes distributions, we would like to see [Microshift](https://github.com/openshift/microshift) used as the base platform. Knative consists of Serving and Eventing modules but focusing on Knative Serving as a first step should be more approachable. The project consists of 2 stages. First one is to run Knative on Microshift with minimal resources. This requires finding out problems here, solving them. A stretch goal is to find out what happens with architectures other than x86_64.\n- Expected outcome:  Having Knative Serving with an ingress layer running on top of Microshift. Having a Hello-World Knative Service running on top. Finding issues blocking the edge setup, and possibly fixing them.	{kubernetes,networking,knative}	2023	Term 2	https://github.com/knative/serving/issues/12718	https://knative.dev/	600000	175
521	b97b2f2d-4dbd-45f5-9121-0e865aa6dfd9	CNCF - Kubevela: Auto-generate the TypeScript and Java languages API SDK	{"The VelaUX API server follows the Open API schema. It could auto-generate the swagger configs via CLI. When VelaUX frontend or other projects need to call these API, they must write the model code and request the API code. We can provide SDK for them to start faster. [OpenAPI generator](https:","",openapi-generator.tech,") could help to generate most codes. But there are still some special cases like:\n  - Dynamic component",trait,policy,"workflowsteps properties need to be generated according to CUE.\n  - Automatically handles the user authentication process, including automatically refreshing tokens.\n  - The API definition may be incomplete accuracy, we should check it to generate high-quality code.\n- Expected Outcome: The outcome of this project will be expand two more database driver for KubeVela VelaUX API server:\n  - VelaUX APIServer TypeScript SDK\n  - VelaUX APIServer Java SDK"}	The VelaUX API server follows the Open API schema. It could auto-generate the swagger configs via CLI. When VelaUX frontend or other projects need to call these API, they must write the model code and request the API code. We can provide SDK for them to start faster. [OpenAPI generator](https://openapi-generator.tech/) could help to generate most codes. But there are still some special cases like:\n  - Dynamic component/trait/policy/workflowsteps properties need to be generated according to CUE.\n  - Automatically handles the user authentication process, including automatically refreshing tokens.\n  - The API definition may be incomplete accuracy, we should check it to generate high-quality code.\n- Expected Outcome: The outcome of this project will be expand two more database driver for KubeVela VelaUX API server:\n  - VelaUX APIServer TypeScript SDK\n  - VelaUX APIServer Java SDK	{go,kubernetes,cue}	2023	Term 2	https://github.com/kubevela/kubevela/issues/5428	https://kubevela.io/	600000	117
528	ac483209-2b08-4f74-aaa5-4ab3203b0677	CNCF - Knative: Self-Balancing Knative Kafka Broker partitions	{"Creating a Knative Kafka Broker requires developers to specify the number of partitions the backing Kafka topic should have, however, this is not an easy decision to make and it requires planning based on the expected load the Knative Broker could receive. This project aims to remove this configuration setting by having an autoscaler that is responsible to add or remove partitions based on the effective load the Knative Kafka Broker receives while preserving [ordered and unordered delivery features](https:","",knative.dev,docs,eventing,brokers,broker-types,kafka-broker,"#configuring-the-order-of-delivered-events) for Triggers.\n- Expected outcome: A Knative Kafka Broker can be created without setting the number of partitions and the number of partitions for a topic backing the Knative Kafka Broker increases or decreases based on the observed load received."}	Creating a Knative Kafka Broker requires developers to specify the number of partitions the backing Kafka topic should have, however, this is not an easy decision to make and it requires planning based on the expected load the Knative Broker could receive. This project aims to remove this configuration setting by having an autoscaler that is responsible to add or remove partitions based on the effective load the Knative Kafka Broker receives while preserving [ordered and unordered delivery features](https://knative.dev/docs/eventing/brokers/broker-types/kafka-broker/#configuring-the-order-of-delivered-events) for Triggers.\n- Expected outcome: A Knative Kafka Broker can be created without setting the number of partitions and the number of partitions for a topic backing the Knative Kafka Broker increases or decreases based on the observed load received.	{kubernetes,java}	2023	Term 2	https://github.com/knative-sandbox/eventing-kafka-broker/issues/2917	https://knative.dev/docs/	0	175
601	b9aad4e2-d9c7-405e-8482-5aced0a4ecdb	CNCF - Konveyor: Move2Kube: Advanced Resources support - ArgoCD, Tekton, Stateful Set, etc.	{"Move2Kube is a command-line tool for automating creation of Infrastructure as code (IaC) artifacts. It has inbuilt support for creating IaC artifacts for replatforming to Kubernetes","OpenShift. Currently we have rudimentary support for resources such as ArgoCD, Tekton, etc. We need to enhance this to make it useful. Example: https:","",github.com,konveyor,move2kube,issues,"930\n- Expected Outcome:\n  - More comprehensive support for advanced K8s resources so as to support real world use cases."}	Move2Kube is a command-line tool for automating creation of Infrastructure as code (IaC) artifacts. It has inbuilt support for creating IaC artifacts for replatforming to Kubernetes/OpenShift. Currently we have rudimentary support for resources such as ArgoCD, Tekton, etc. We need to enhance this to make it useful. Example: https://github.com/konveyor/move2kube/issues/930\n- Expected Outcome:\n  - More comprehensive support for advanced K8s resources so as to support real world use cases.	{go,kubernetes,argocd,tekton}	2023	Term 3	https://github.com/konveyor/move2kube/issues/1063	https://www.konveyor.io/	0	148
591	c2b5f721-2666-4d9e-85d6-7bedae27e144	CNCF - Konveyor: Move2Kube: Compile Move2Kube to WASM/WASI and run it in the browser	{"Move2Kube is a command-line tool for automating creation of Infrastructure as code (IaC) artifacts. It has inbuilt support for creating IaC artifacts for replatforming to Kubernetes","OpenShift. We want to compile targetting WASM","WASI and run the resulting WASM module in the browser. This will help up showcase Move2Kube for demos and allow users to quickly try out Move2Kube without having to install it or any of its dependencies.\n- Expected Outcome:\n  - Run Move2Kube CLI in the browser using WebAssembly."}	Move2Kube is a command-line tool for automating creation of Infrastructure as code (IaC) artifacts. It has inbuilt support for creating IaC artifacts for replatforming to Kubernetes/OpenShift. We want to compile targetting WASM/WASI and run the resulting WASM module in the browser. This will help up showcase Move2Kube for demos and allow users to quickly try out Move2Kube without having to install it or any of its dependencies.\n- Expected Outcome:\n  - Run Move2Kube CLI in the browser using WebAssembly.	{go,wasm}	2023	Term 3	https://github.com/konveyor/move2kube/issues/1062	https://www.konveyor.io/	0	148
400	9976a49b-0aa4-49db-ae71-6180f85218ef	CNCF - Konveyor: Move2Kube Consume Move2Kube through a plugin on Eclipse	{"Move2Kube is a command-line tool for automating creation of Infrastructure as code (IaC) artifacts. It has inbuilt support for creating IaC artifacts for replatforming to Kubernetes","OpenShift. Users currently have to use move2kube command line tool or UI to access move2kube and use it in their replatforming workflows. Allow Move2Kube to be accessible from Eclipse as a plugin. It can start with simple functionality like right clicking on a docker-compose file, and generating all Kubernetes artifacts. An eclipse plugin for Move2kube will promote fast integration in replatforming workflows.\n\nExpected Outcome: An end to end working eclipse plugin with a demo video showcasing the functionality."}	Move2Kube is a command-line tool for automating creation of Infrastructure as code (IaC) artifacts. It has inbuilt support for creating IaC artifacts for replatforming to Kubernetes/OpenShift. Users currently have to use move2kube command line tool or UI to access move2kube and use it in their replatforming workflows. Allow Move2Kube to be accessible from Eclipse as a plugin. It can start with simple functionality like right clicking on a docker-compose file, and generating all Kubernetes artifacts. An eclipse plugin for Move2kube will promote fast integration in replatforming workflows.\n\nExpected Outcome: An end to end working eclipse plugin with a demo video showcasing the functionality.	{golang,java,eclipse}	2023	Term 1	https://github.com/konveyor/move2kube/issues/396	https://www.konveyor.io/	0	148
15	d61e1b05-2a4f-432d-b715-57c818b3e120	CNCF - KubeArmor: Add BTF and BPF CO-RE Support to KubeArmor	{"Currently KubeArmor depends on kernel headers to use various kernel structures. This creates difficulty in having portability.\nLinux Kernel versions with BTF (BPF Type Format) information available allows us to write portable BPF CO-RE (or Compile Once - Run Everywhere) applications that can run on multiple kernel versions and configurations without any modification or runtime compilation on the target machine.  \nBut there is a restriction that CO-RE requires to have the BTF information of the target kernel, which is provided by the kernel itself when it's compiled with CONFIG_DEBUG_INFO_BTF=y. This option was introduced in Linux 5.2.  \nFor kernels < 5.2 we can use BTFGen to ship BTF information with KubeArmor code or use pahole to generate BTF information from the vmlinux image (with DWARF information) at runtime.  \nThe project aims to make KubeArmor truly portable across all kernel versions by reducing host environment dependencies."}	Currently KubeArmor depends on kernel headers to use various kernel structures. This creates difficulty in having portability.\nLinux Kernel versions with BTF (BPF Type Format) information available allows us to write portable BPF CO-RE (or Compile Once - Run Everywhere) applications that can run on multiple kernel versions and configurations without any modification or runtime compilation on the target machine.  \nBut there is a restriction that CO-RE requires to have the BTF information of the target kernel, which is provided by the kernel itself when it's compiled with CONFIG_DEBUG_INFO_BTF=y. This option was introduced in Linux 5.2.  \nFor kernels < 5.2 we can use BTFGen to ship BTF information with KubeArmor code or use pahole to generate BTF information from the vmlinux image (with DWARF information) at runtime.  \nThe project aims to make KubeArmor truly portable across all kernel versions by reducing host environment dependencies.	{kernel,go,c}	2022	Term 3	https://github.com/kubearmor/KubeArmor/issues/789	https://kubearmor.io/	0	9
45	aebb67b1-c918-4eee-8c25-df0cf7e38bee	CNCF - KubeArmor: Extending kubearmor-cli-tool filtering options	{"KubeArmor is Cloud Native Runtime Security Enforcement System that restricts the behavior (such as process execution, file access, and networking operation) of containers and nodes at the system level using Linux Kernel LSMs (Linux Security Modules) and eBPF. KubeArmor cli-tool (aka karmor) connects to the kubearmor-relay service to provide command-line telemetry","observability. Karmor cli options could be extended to support various other parameters as described in the given issue."}	KubeArmor is Cloud Native Runtime Security Enforcement System that restricts the behavior (such as process execution, file access, and networking operation) of containers and nodes at the system level using Linux Kernel LSMs (Linux Security Modules) and eBPF. KubeArmor cli-tool (aka karmor) connects to the kubearmor-relay service to provide command-line telemetry/observability. Karmor cli options could be extended to support various other parameters as described in the given issue.	{go,kubernetes}	2022	Term 1	https://github.com/kubearmor/kubearmor-client/issues/40	https://kubearmor.com/	300000	9
533	cfa22331-36f3-4d20-abf0-667a31fd2ba8	CNCF - KubeArmor: Implement DNS visibility with KubeArmor	{"The project aims to provide better visibility into the domains accessed from pods, with a focus on identifying and containing attacks that use techniques like Domain Generation Algorithms (DGA) to connect to remote command and control (C&C) servers. By gathering information on which domains are being accessed and applying network rules to allow only specific domains, the project aims to empower security operations (secops) teams to better prevent and respond to such attacks.\n* Expected Outcome:  \n  * KubeArmor to emit telemetry events for any DNS lookups from any pods.\n  * Ability to see egress DNS lookups done from any pods using karmor summary.\n  * Documentation"}	The project aims to provide better visibility into the domains accessed from pods, with a focus on identifying and containing attacks that use techniques like Domain Generation Algorithms (DGA) to connect to remote command and control (C&C) servers. By gathering information on which domains are being accessed and applying network rules to allow only specific domains, the project aims to empower security operations (secops) teams to better prevent and respond to such attacks.\n* Expected Outcome:  \n  * KubeArmor to emit telemetry events for any DNS lookups from any pods.\n  * Ability to see egress DNS lookups done from any pods using karmor summary.\n  * Documentation	{go,kubernetes,kubearmor}	2023	Term 2	https://github.com/kubearmor/KubeArmor/issues/1219	https://kubearmor.io/	300000	9
416	a0696db8-509e-44ff-ae61-82a3442853c1	CNCF - KubeArmor: KubeArmor Telemetry Monitoring and Dashboards	{"Description: KubeArmor generates a large amount of data through logs and alerts, but interpreting this data can be difficult. To make it easier to understand, it is necessary to parse the telemetry, create meaningful metrics, enable data filtering, and create visualizations such as graphs to display on a dashboard.\n\nExpected Outcome: Create a telemetry dashboard, write setup documentation and usage guide."}	Description: KubeArmor generates a large amount of data through logs and alerts, but interpreting this data can be difficult. To make it easier to understand, it is necessary to parse the telemetry, create meaningful metrics, enable data filtering, and create visualizations such as graphs to display on a dashboard.\n\nExpected Outcome: Create a telemetry dashboard, write setup documentation and usage guide.	{fluentd,loki,grafana}	2023	Term 1	https://github.com/kubearmor/KubeArmor/issues/836	https://kubearmor.io/	300000	9
380	3cc962b4-cd8b-46ea-9c77-83304145fd51	CNCF - KubeArmor: Use non-privileged containers for KubeArmor daemonset	{"KubeArmor currently uses privileged mode for its daemonset containers. But it is not a good practice. Privileged containers are usually frowned upon. In many cases, specific admission controllers are deployed to not allow containers to be installed in privileged mode.\nIt is best to not use privileged mode but to define specific capabilities for KubeArmor.  \nThe aim of the project is to analyse and reduce the system privileges required by KubeArmor, thereby reducing the potential attack surface."}	KubeArmor currently uses privileged mode for its daemonset containers. But it is not a good practice. Privileged containers are usually frowned upon. In many cases, specific admission controllers are deployed to not allow containers to be installed in privileged mode.\nIt is best to not use privileged mode but to define specific capabilities for KubeArmor.  \nThe aim of the project is to analyse and reduce the system privileges required by KubeArmor, thereby reducing the potential attack surface.	{kernel,go,kubernetes}	2022	Term 3	https://github.com/kubearmor/KubeArmor/issues/781	https://kubearmor.io/	0	9
595	12dda5ae-a123-4e2a-985c-13d33f8a25f0	CNCF - KubeEdge: Add case study center in website	{"Now we have had many user cases in the community. However, the KubeEdge website does not have a page to display user cases. Many users lack ways to understand and learn KubeEdge implementation cases., we hope to build a case center to display them, so that more users can consult and learn. \n- Expected Outcome: Add user case study center to display all KubeEdge user cases. Users can upload their own cases. Also users and learners can also manage and view cases by industry tag."}	Now we have had many user cases in the community. However, the KubeEdge website does not have a page to display user cases. Many users lack ways to understand and learn KubeEdge implementation cases., we hope to build a case center to display them, so that more users can consult and learn. \n- Expected Outcome: Add user case study center to display all KubeEdge user cases. Users can upload their own cases. Also users and learners can also manage and view cases by industry tag.	{kubeedge,javascript,html}	2023	Term 3	https://github.com/kubeedge/website/issues/347	https://kubeedge.io/en/	0	74
602	36bfe273-9059-47e0-88f7-afb38b2d9ebb	CNCF - KubeEdge: Support latest version installation demo in killercoda	{"We have created a tutorial in the interactive learning platform killercoda for KubeEdge deployment. This can give a hands-on experience of KubeEdge deployment. Now we need to support the latest version of KubeEdge and integrate example for developers.\n- Expected Outcome: It can install the latest version of KubeEdge example, developers can experience these cloud native edge-computing demos online."}	We have created a tutorial in the interactive learning platform killercoda for KubeEdge deployment. This can give a hands-on experience of KubeEdge deployment. Now we need to support the latest version of KubeEdge and integrate example for developers.\n- Expected Outcome: It can install the latest version of KubeEdge example, developers can experience these cloud native edge-computing demos online.	{kubeedge,golang,kubernetes}	2023	Term 3	https://github.com/kubeedge/killercoda-scenarios/issues/8	https://kubeedge.io/en/	0	74
407	55469b74-0c98-44f1-b8e1-4244a736bf82	CNCF - Kubernetes: CAPG Add telemetry and profiling support	{"Description: Cluster API Provider GCP (CAPG) enables the creation of Kubernetes clusters in GCP with Cluster API. With increasing adoption of Cluster API (CAPI) in general and of CAPG we want to improve the supportability of CAPG, especially for production environments. The first part of this is to add telemetry","tracing using OpenTelemetry so that we can understand and visualize the flow of reconciliation within the provider. The next part is to add a **pprof** endpoint that can be optionally enabled to enable operations","support users to collect profiling information from a running instances of CAPG.\n\nExpected Outcome: This work will enable tracing and profiling of a running instance of CAPG (along with supporting docs) to supports operations","support engineers."}	Description: Cluster API Provider GCP (CAPG) enables the creation of Kubernetes clusters in GCP with Cluster API. With increasing adoption of Cluster API (CAPI) in general and of CAPG we want to improve the supportability of CAPG, especially for production environments. The first part of this is to add telemetry/tracing using OpenTelemetry so that we can understand and visualize the flow of reconciliation within the provider. The next part is to add a **pprof** endpoint that can be optionally enabled to enable operations/support users to collect profiling information from a running instances of CAPG.\n\nExpected Outcome: This work will enable tracing and profiling of a running instance of CAPG (along with supporting docs) to supports operations/support engineers.	{go,kubernetes}	2023	Term 1	https://github.com/kubernetes-sigs/cluster-api-provider-gcp/issues/810	http://kubernetes.io/	600000	75
340	e799bb33-a695-420b-af32-e596938c6960	CNCF - Kubernetes: Cluster API Provider GCP	{"Cluster API Provider GCP (a.k.a. CAPG) enables the creation of Kubernetes clusters in GCP with Cluster API. Currently the clusters it creates do not support running workloads in them that take advantage of GPUs and so this rules out things like highly performant machine learning and computational heavy work workloads. We want to enhance CAPG so that it supports GPUs so that users can run these type of workloads. This will require creating a proposal for the change (i.e. design work) and then implementing the propsal which may include changes to the api","controllers, base images and driver installation.\n\nExpected outcome: This work will enable CAPG to support the creation of Kubernetes clusters where workfloads can take advantage of GPUs."}	Cluster API Provider GCP (a.k.a. CAPG) enables the creation of Kubernetes clusters in GCP with Cluster API. Currently the clusters it creates do not support running workloads in them that take advantage of GPUs and so this rules out things like highly performant machine learning and computational heavy work workloads. We want to enhance CAPG so that it supports GPUs so that users can run these type of workloads. This will require creating a proposal for the change (i.e. design work) and then implementing the propsal which may include changes to the api/controllers, base images and driver installation.\n\nExpected outcome: This work will enable CAPG to support the creation of Kubernetes clusters where workfloads can take advantage of GPUs.	{go,kubernetes}	2022	Term 2	https://github.com/kubernetes-sigs/cluster-api-provider-gcp/issues/289		300000	75
49	1c860c94-b1d3-401c-b2c1-20527cb80391	CNCF - Kubernetes: Improve SIG-Node testing using Kubetest2	{"Kubernetes currently uses Kubetest as the interface for launching and running e2e tests. There is a new [kubetest2](https:","",github.com,kubernetes-sigs,"kubetest2) that is in the process of being developed and will need to be rolled out to various CI harnesses and jobs. As part of this project we will focus on SIG-Node related CI jobs. Here's how we currently test SIG-Node related code - [e2e-node-tests.md](https:","",github.com,kubernetes,community,blob,master,contributors,devel,sig-node,"e2e-node-tests.md). There will be a lot of interesting problems to solve and this work is critical to how we test kubernetes not just in GCP, but also across all the other cloud providers going forward."}	Kubernetes currently uses Kubetest as the interface for launching and running e2e tests. There is a new [kubetest2](https://github.com/kubernetes-sigs/kubetest2) that is in the process of being developed and will need to be rolled out to various CI harnesses and jobs. As part of this project we will focus on SIG-Node related CI jobs. Here's how we currently test SIG-Node related code - [e2e-node-tests.md](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-node/e2e-node-tests.md). There will be a lot of interesting problems to solve and this work is critical to how we test kubernetes not just in GCP, but also across all the other cloud providers going forward.	{go,testing,python,bash}	2021	Term 3	https://github.com/kubernetes/enhancements/tree/master/keps/sig-testing/2464-kubetest2-ci-migration	http://kubernetes.io/	300000	75
255	682ea527-4a16-4310-b104-aa00a15d1786	CNCF - Kubernetes SIG ContribEx: Creating Katacoda Scenarios To Help New Contributors	{"This internship involves improving the existing Katacoda scenario and adding new scenarios to further include aspects of contributing such as spinning up a kind cluster with the changes made and testing those changes out. Through the course of this internship, you will also learn how one can contribute to other projects of the Kubernetes community such as the Kubernetes website, and document these processes as Katacoda scenarios to help new contributors get started in their contribution journey."}	This internship involves improving the existing Katacoda scenario and adding new scenarios to further include aspects of contributing such as spinning up a kind cluster with the changes made and testing those changes out. Through the course of this internship, you will also learn how one can contribute to other projects of the Kubernetes community such as the Kubernetes website, and document these processes as Katacoda scenarios to help new contributors get started in their contribution journey.	{go,kubernetes}	2022	Term 1	https://github.com/kubernetes/community/issues/5576	http://kubernetes.io/	300000	75
264	d4cca618-f091-415e-a74d-bb11267795e7	CNCF - Kubevela: Enhance multi-cluster observability	{"Description: KubeVela is a modern application delivery platform based on Kubernetes. It is currently a CNCF Sandbox project. KubeVela supports managing application delivery in multi-clusters. One of the basic problem is to validate the health status of managed clusters. Besides, it is also useful to integrate other metrics like CPU core usage or number of available graphical cards. This project aims to establish a mechanism to support these features."}	Description: KubeVela is a modern application delivery platform based on Kubernetes. It is currently a CNCF Sandbox project. KubeVela supports managing application delivery in multi-clusters. One of the basic problem is to validate the health status of managed clusters. Besides, it is also useful to integrate other metrics like CPU core usage or number of available graphical cards. This project aims to establish a mechanism to support these features.	{go,kubernetes}	2022	Term 1	https://github.com/oam-dev/kubevela/issues/3177	https://kubevela.io/	300000	117
273	bf31f5d2-21a3-4dc4-bd85-c23f9088bad3	CNCF - Kuma: Active monitoring of Cross Zone communication	{"Description: Kuma is a modern Envoy-based service mesh that can run on every cloud, in a single or multi-zone capacity, across both Kubernetes and VMs. It is currently a CNCF Sandbox project. Because Kuma is heavily built with multi zones in mind it is needed for Kuma to provide a good level of observability of connectivity between these zones. This project aims to provide active monitoring of connections between each zone and create new apis to bubble up this information in the GUI and in our Grafana dashboards. This project goes from design to complete implementation, documentation and demonstration."}	Description: Kuma is a modern Envoy-based service mesh that can run on every cloud, in a single or multi-zone capacity, across both Kubernetes and VMs. It is currently a CNCF Sandbox project. Because Kuma is heavily built with multi zones in mind it is needed for Kuma to provide a good level of observability of connectivity between these zones. This project aims to provide active monitoring of connections between each zone and create new apis to bubble up this information in the GUI and in our Grafana dashboards. This project goes from design to complete implementation, documentation and demonstration.	{go,kubernetes}	2022	Term 1	https://github.com/kumahq/kuma/issues/1907	https://kuma.io/	0	127
260	c70ff3c2-f145-4396-bc48-559a03000a3c	CNCF - Kuma: Add status infos in Kubernetes CRDs	{"Description: Kuma is a modern Envoy-based service mesh that can run on every cloud, in a single or multi-zone capacity, across both Kubernetes and VMs. It is currently a CNCF Sandbox project. While Kuma currently exposes information about status in its api Kubernetes users usualy expect these to be also present in the Status fields of their resources. This project aims in adding status to all Kuma CRD and to improve our controllers to set these as cluster state changes."}	Description: Kuma is a modern Envoy-based service mesh that can run on every cloud, in a single or multi-zone capacity, across both Kubernetes and VMs. It is currently a CNCF Sandbox project. While Kuma currently exposes information about status in its api Kubernetes users usualy expect these to be also present in the Status fields of their resources. This project aims in adding status to all Kuma CRD and to improve our controllers to set these as cluster state changes.	{go,kubernetes}	2022	Term 1	https://github.com/kumahq/kuma/issues/3734	https://kuma.io/	0	127
513	4689c5fa-165e-4015-ad21-951d9babcb7e	CNCF - Kyverno: Cleanup Policies, Phase 2	{"Kyverno has a policy type called Cleanup Policies which allow removal of resources defined in a policy. In this second phase, we would like to extend this ability to cleanup resources based upon defining a label for even more fine-grained control.\n- Expected outcome: Extend Cleanup Policies feature by allowing per-resource removal based upon label assignment\n- Recommended Skills: Golang, Kubernetes, Kyverno\n- Mentor(s): Charles-Edouard Brétéché @eddycharly (charles.edouard AT nirmata DOT com)\n- Upstream Issue (URL):\n  - https:","",github.com,kyverno,kyverno,issues,"5748\n  - https:","",github.com,kyverno,KDP,blob,main,proposals,cleanup.md#proposal}	Kyverno has a policy type called Cleanup Policies which allow removal of resources defined in a policy. In this second phase, we would like to extend this ability to cleanup resources based upon defining a label for even more fine-grained control.\n- Expected outcome: Extend Cleanup Policies feature by allowing per-resource removal based upon label assignment\n- Recommended Skills: Golang, Kubernetes, Kyverno\n- Mentor(s): Charles-Edouard Brétéché @eddycharly (charles.edouard AT nirmata DOT com)\n- Upstream Issue (URL):\n  - https://github.com/kyverno/kyverno/issues/5748\n  - https://github.com/kyverno/KDP/blob/main/proposals/cleanup.md#proposal	{kubernetes,kyverno,go}	2023	Term 2	https://github.com/kyverno/kyverno/issues/5748	https://kyverno.io/	300000	17
26	0e385057-2c8f-48b2-8ff9-b4244870c10c	CNCF - Kyverno: CLI test schema and enhancements	{"The Kyverno CLI does not have a formalized schema with proper validation for its test command. Create a formal schema which is documented allowing for full validation and related other capabilities which enhance its usage.\n\nUpstream Issue (URL): \n* https:","",github.com,kyverno,kyverno,issues,"2323\n* https:","",github.com,kyverno,kyverno,issues,"2315\n* https:","",github.com,kyverno,kyverno,issues,"2302\n* https:","",github.com,kyverno,kyverno,issues,"2857\n* https:","",github.com,kyverno,kyverno,issues,"2945\n* https:","",github.com,kyverno,kyverno,issues,3271}	The Kyverno CLI does not have a formalized schema with proper validation for its test command. Create a formal schema which is documented allowing for full validation and related other capabilities which enhance its usage.\n\nUpstream Issue (URL): \n* https://github.com/kyverno/kyverno/issues/2323\n* https://github.com/kyverno/kyverno/issues/2315\n* https://github.com/kyverno/kyverno/issues/2302\n* https://github.com/kyverno/kyverno/issues/2857\n* https://github.com/kyverno/kyverno/issues/2945\n* https://github.com/kyverno/kyverno/issues/3271	{go,kubernetes}	2022	Term 2	https://github.com/kyverno/kyverno/issues/2323	https://kyverno.io/	300000	17
371	25e0fa72-8260-4c6f-819b-d87b865e58f2	CNCF - Kyverno: Enable resource clean-up	{"Support a new type of Kyverno rule to delete resources based on various criterias, such as the type, age, metadata and status.\n\nUpstream Issue (URL):\n  - https:","",github.com,kyverno,kyverno,issues,"3483\n  - https:","",github.com,kyverno,KDP,pull,25}	Support a new type of Kyverno rule to delete resources based on various criterias, such as the type, age, metadata and status.\n\nUpstream Issue (URL):\n  - https://github.com/kyverno/kyverno/issues/3483\n  - https://github.com/kyverno/KDP/pull/25	{go,kubernetes}	2022	Term 3	https://github.com/kyverno/kyverno/issues/3483	https://kyverno.io/	0	17
373	e5ef8032-3dd3-44c3-8746-620f4f678d60	CNCF - Kyverno: Logging in JSON plus other enhancements	{"Add an ability allowing a user to tell Kyverno to log in JSON format rather than klog."}	Add an ability allowing a user to tell Kyverno to log in JSON format rather than klog.	{go}	2022	Term 3	https://github.com/kyverno/kyverno/issues/3411	https://kyverno.io/	0	17
135	d158a68c-a48d-44b5-885d-f33551348209	Expose and manage PCI device reset	{"PCI and PCIe devices may support a number of possible reset mechanisms, for example Function Level Reset (FLR) provided via Advanced Feature or PCIe capabilities, Power Management reset, bus reset, or device specific reset.  Currently the PCI subsystem creates a policy prioritizing these reset methods which provides neither visibility nor control to userspace.  This project would work to expose the reset methods available per device to userspace, likely via sysfs, and allow a administrative user or device owner to have some ability to manage per device reset method priorities or exclusions.\n\nThis feature aims to allow greater control of a device for use cases as device assignment, where specific device or platform issues may interact poorly with a given reset method, and for which device specific quirks have not been developed."}	PCI and PCIe devices may support a number of possible reset mechanisms, for example Function Level Reset (FLR) provided via Advanced Feature or PCIe capabilities, Power Management reset, bus reset, or device specific reset.  Currently the PCI subsystem creates a policy prioritizing these reset methods which provides neither visibility nor control to userspace.  This project would work to expose the reset methods available per device to userspace, likely via sysfs, and allow a administrative user or device owner to have some ability to manage per device reset method priorities or exclusions.\n\nThis feature aims to allow greater control of a device for use cases as device assignment, where specific device or platform issues may interact poorly with a given reset method, and for which device specific quirks have not been developed.	{c,shell,kernel}	2021	Term 1	https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/	https://wiki.linuxfoundation.org/lkmp	300000	21
61	958fe36a-d763-4422-81af-c5ecf2465957	Introduction to Linux Kernel Development	{"APPLICATIONS ARE CLOSED. Please apply again in the Fall! We will post more information when it is available.\n\nLinux is the most widely used operating system in the world. The core software component of the Linux operating system is the kernel. Some of its roles include managing hardware interactions, virtualizing system resources, and enforcing security constraints. In effect, the Linux kernel powers almost all of the world's top supercomputers, android phones, and an innumerable variety of other computers. This course will introduce students to the Linux kernel development by focusing on device driver development, particularly character devices and the ","proc and","sys interfaces. This will give students hands-on experience working with internal Linux kernel APIs for hardware access, memory management, DMA and interrupts, among other, and provide an overview of some of the core features and components of the kernel, such as scheduling, system calls, the boot process, and hardware description trees. Gaining an understand of the inner workings of the operating system and how to make changes to it will give students an invaluable perspective on how computers work behind the scenes, which will reveal a new layer of understanding to apply to any future software engineering practice.\n\nThis is a new format of the following courses we created:\nhttps:","",www.uml.edu,catalog,courses,COMP,"3085 or https:","",www.uml.edu,catalog,courses,COMP,5170}	APPLICATIONS ARE CLOSED. Please apply again in the Fall! We will post more information when it is available.\n\nLinux is the most widely used operating system in the world. The core software component of the Linux operating system is the kernel. Some of its roles include managing hardware interactions, virtualizing system resources, and enforcing security constraints. In effect, the Linux kernel powers almost all of the world's top supercomputers, android phones, and an innumerable variety of other computers. This course will introduce students to the Linux kernel development by focusing on device driver development, particularly character devices and the /proc and/sys interfaces. This will give students hands-on experience working with internal Linux kernel APIs for hardware access, memory management, DMA and interrupts, among other, and provide an overview of some of the core features and components of the kernel, such as scheduling, system calls, the boot process, and hardware description trees. Gaining an understand of the inner workings of the operating system and how to make changes to it will give students an invaluable perspective on how computers work behind the scenes, which will reveal a new layer of understanding to apply to any future software engineering practice.\n\nThis is a new format of the following courses we created:\nhttps://www.uml.edu/catalog/courses/COMP/3085 or https://www.uml.edu/catalog/courses/COMP/5170	{c,linux,patience}	2023	Term 1	https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/	https://kdlp.underground.software/	0	21
108	bb9973ac-5b46-4e1a-8052-7a4541d7909f	Linux Kernel Add KUnit Support for Parameterized Testing	{"Parameterized tests allow the test logic and the data passed into the test to be specified separately. Such a feature would have wide applicability in Linux kernel testing.\n\nThis project would entail designing a new API for defining parameterized tests, integrating parameterized tests into the KUnit reporting mechanism and converting some tests over to use this new API. We envision something similar to  https:","",dzone.com,articles,"junit-parameterized-test\n\nAdding the feature makes it easier to write Kunit tests that need to iterate over a set of values without duplicated code and with clear reporting of failures."}	Parameterized tests allow the test logic and the data passed into the test to be specified separately. Such a feature would have wide applicability in Linux kernel testing.\n\nThis project would entail designing a new API for defining parameterized tests, integrating parameterized tests into the KUnit reporting mechanism and converting some tests over to use this new API. We envision something similar to  https://dzone.com/articles/junit-parameterized-test\n\nAdding the feature makes it easier to write Kunit tests that need to iterate over a set of values without duplicated code and with clear reporting of failures.	{c,shell,kernel}	2020	Term 3	https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/	https://wiki.linuxfoundation.org/lkmp	0	21
55	cdfbfdad-9bc1-4623-9624-f8b7133c9167	Linux kernel: Refurbishing make coccicheck	{"Coccicheck is a Linux kernel target that makes it easy to run a number of Coccinelle bug finding scripts on the Linux kernel source code.\n\nWhile make coccicheck has provide useful, it has some aspects that are not so user friendly. Also the Coccinelle scripts it runs could use some refreshing. This project will address these issues.\n\nThe project will consist of three parts:\n\n1. Improve make coccicheck to make it more user friendly. Issues to address include error reporting and the control of parallelism.\n\n2. Test the Coccinelle scripts found in the Linux kernel and determine whether they are ok as is, whether they should be improved, or whether they should be removed.\n\n3. Improve Coccinelle so that it can take a patch or commit id as input and only report on results that relate to that patch or commit id. The goal would be not to simply suppress irrelevant results, but to actually focus Coccinelle's effort on the changed code, to reduce the running time.\n\nAll projects to which contributions will be made in this internship are GPLv2 and contributions require signing the Developer's Certificate of Origin (DCO)."}	Coccicheck is a Linux kernel target that makes it easy to run a number of Coccinelle bug finding scripts on the Linux kernel source code.\n\nWhile make coccicheck has provide useful, it has some aspects that are not so user friendly. Also the Coccinelle scripts it runs could use some refreshing. This project will address these issues.\n\nThe project will consist of three parts:\n\n1. Improve make coccicheck to make it more user friendly. Issues to address include error reporting and the control of parallelism.\n\n2. Test the Coccinelle scripts found in the Linux kernel and determine whether they are ok as is, whether they should be improved, or whether they should be removed.\n\n3. Improve Coccinelle so that it can take a patch or commit id as input and only report on results that relate to that patch or commit id. The goal would be not to simply suppress irrelevant results, but to actually focus Coccinelle's effort on the changed code, to reduce the running time.\n\nAll projects to which contributions will be made in this internship are GPLv2 and contributions require signing the Developer's Certificate of Origin (DCO).	{shell,c}	2020	Term 2	https://github.com/coccinelle/coccinelle	http://coccinelle.lip6.fr/	0	21
461	7e3382be-5d82-443e-b0bc-4dcd2194705d	CNCF - Meshery: Distributed client-side policy evaluation in WASM and Rego	{"Meshery's highly dynamic infrastructure configuration capabilities require real-time evaluation of complex policies. Policies of various types and with a high number of parameters need to be evaluted client-side. With policies expressed in Rego, the goal of this project is to incorporate use of the https:","",github.com,open-policy-agent,"golang-opa-wasm project into Meshery UI, so that a powerful, real-time user experience is possible."}	Meshery's highly dynamic infrastructure configuration capabilities require real-time evaluation of complex policies. Policies of various types and with a high number of parameters need to be evaluted client-side. With policies expressed in Rego, the goal of this project is to incorporate use of the https://github.com/open-policy-agent/golang-opa-wasm project into Meshery UI, so that a powerful, real-time user experience is possible.	{go,webassembly}	2023	Term 1	https://github.com/meshery/meshery/issues/7019	https://meshery.io/	300000	186
553	983193ea-9cca-405f-baa5-e6ade4df1ba2	CNCF - LitmusChaos: Enhance/improve chaos center code base and redesign chaos workflow apis	{"This project focuses on enhancing and improving the Chaos Center code base, specifically redesigning the Chaos Workflow APIs to provide an enhanced user experience. The main objectives include refining the functionality of the Chaos Workflow and Workflow Run APIs, modularizing the chaos-workflow package into separate packages, and addressing security vulnerabilities and golangci-lint issues in the Chaos Center backend components. The project aims to deliver a more robust and secure Chaos Center platform, offering improved usability and performance for users.\n- Expected outcome: The outcome of this project will be improved functionality, security, and usability of the chaos workflow GraphQL APIs and chaos-center backend components through the implementation of new features, refactoring of existing code, and addressing of security vulnerabilities."}	This project focuses on enhancing and improving the Chaos Center code base, specifically redesigning the Chaos Workflow APIs to provide an enhanced user experience. The main objectives include refining the functionality of the Chaos Workflow and Workflow Run APIs, modularizing the chaos-workflow package into separate packages, and addressing security vulnerabilities and golangci-lint issues in the Chaos Center backend components. The project aims to deliver a more robust and secure Chaos Center platform, offering improved usability and performance for users.\n- Expected outcome: The outcome of this project will be improved functionality, security, and usability of the chaos workflow GraphQL APIs and chaos-center backend components through the implementation of new features, refactoring of existing code, and addressing of security vulnerabilities.	{go,kubernetes}	2023	Term 2	https://github.com/litmuschaos/litmus/issues/3970	https://litmuschaos.io/	300000	13
544	bd6e875a-a64c-4405-af1c-677d8c45014b	CNCF - LitmusChaos: Enhance/Upgrade chaos operator and chaos exporter module	{"LitmusChaos is an open source Chaos Engineering platform that enables teams to identify weaknesses & potential outages in infrastructures by inducing chaos tests in a controlled way. This project idea involves upgrading the Chaos Operator and Chaos Exporter repositories by updating their dependencies, addressing security vulnerabilities, and adding new functionality. Specifically, the project aims to upgrade the operator-sdk and Prometheus exporter versions, add new Prometheus metrics to the Chaos Exporter, and fix security vulnerabilities pointed out by trivy and golangci-lint. Furthermore, the project seeks to add unit test cases to both repositories to ensure that their functionality is robust and reliable. Overall, this project aims to improve the stability, security, and functionality of the Chaos Operator and Chaos Exporter repositories, making them better suited for use in production environments.\n- Expected outcome: The outcome of this project will be improved stability, security, and functionality of the Chaos Operator and Chaos Exporter modules through the upgrade of dependencies, addition of new metrics, and implementation of unit tests."}	LitmusChaos is an open source Chaos Engineering platform that enables teams to identify weaknesses & potential outages in infrastructures by inducing chaos tests in a controlled way. This project idea involves upgrading the Chaos Operator and Chaos Exporter repositories by updating their dependencies, addressing security vulnerabilities, and adding new functionality. Specifically, the project aims to upgrade the operator-sdk and Prometheus exporter versions, add new Prometheus metrics to the Chaos Exporter, and fix security vulnerabilities pointed out by trivy and golangci-lint. Furthermore, the project seeks to add unit test cases to both repositories to ensure that their functionality is robust and reliable. Overall, this project aims to improve the stability, security, and functionality of the Chaos Operator and Chaos Exporter repositories, making them better suited for use in production environments.\n- Expected outcome: The outcome of this project will be improved stability, security, and functionality of the Chaos Operator and Chaos Exporter modules through the upgrade of dependencies, addition of new metrics, and implementation of unit tests.	{go,kubernetes,prometheus}	2023	Term 2	https://github.com/litmuschaos/litmus/issues/3969	https://litmuschaos.io/	300000	13
603	237b7300-d749-4f14-bd4c-9375e5ec39b6	CNCF - LitmusChaos: Improve Chaoscenter Web and Authentication Server: Add Unit Test Cases, Enhance	{"LitmusChaos is an open-source chaos engineering platform for Kubernetes, enabling users to test and improve the resilience of their cloud-native applications. The task is add unit tests for Chaoscenter Web and test cases for the Authentication Server. The GraphQL API documentation will be updated with the latest APIs, while the GraphQL server's APIs and handler functions will be optimized to reduce code duplicacy. Additionally, comprehensive documentation and video tutorials will be created for local development setup, promoting easier onboarding and collaboration.\n- Expected outcome: The expected outcome of this issue is an improved Chaoscenter Web and Authentication Server with added unit tests, updated GraphQL API documentation, and optimized APIs and handler functions. The enhancements will result in a more reliable, efficient, and user-friendly chaos engineering platform, promoting better collaboration within the community."}	LitmusChaos is an open-source chaos engineering platform for Kubernetes, enabling users to test and improve the resilience of their cloud-native applications. The task is add unit tests for Chaoscenter Web and test cases for the Authentication Server. The GraphQL API documentation will be updated with the latest APIs, while the GraphQL server's APIs and handler functions will be optimized to reduce code duplicacy. Additionally, comprehensive documentation and video tutorials will be created for local development setup, promoting easier onboarding and collaboration.\n- Expected outcome: The expected outcome of this issue is an improved Chaoscenter Web and Authentication Server with added unit tests, updated GraphQL API documentation, and optimized APIs and handler functions. The enhancements will result in a more reliable, efficient, and user-friendly chaos engineering platform, promoting better collaboration within the community.	{go,typescript}	2023	Term 3	https://github.com/litmuschaos/litmus/issues/4102	https://litmuschaos.io/	0	13
611	fde90e7f-0410-4b84-ad9c-99f7139267ed	CNCF - LitmusChaos: Improve litmusctl UX and codebase and add new functionalities to litmusctl	{"LitmusChaos is an open-source chaos engineering platform for Kubernetes, enabling users to test and improve the resilience of their cloud-native applications. The project focuses on improving litmusctl by enhancing its interactive mode with promptui, and refactoring code to Go interfaces for better unit testing and code quality. Additionally, it aims to replace kubectl with client-go for more efficient Kubernetes operations, resulting in a more user-friendly and reliable command-line tool for chaos engineering and workload management.\n- Expected outcome: The expected outcome of the project includes an improved litmusctl tool with a user-friendly promptui-based interactive mode, enhanced code quality through Go interfaces, and a robust test suite. The migration to client-go for Kubernetes operations will ensure better performance and reduced external dependencies, providing users with a reliable and efficient command-line utility for chaos engineering and Kubernetes management tasks."}	LitmusChaos is an open-source chaos engineering platform for Kubernetes, enabling users to test and improve the resilience of their cloud-native applications. The project focuses on improving litmusctl by enhancing its interactive mode with promptui, and refactoring code to Go interfaces for better unit testing and code quality. Additionally, it aims to replace kubectl with client-go for more efficient Kubernetes operations, resulting in a more user-friendly and reliable command-line tool for chaos engineering and workload management.\n- Expected outcome: The expected outcome of the project includes an improved litmusctl tool with a user-friendly promptui-based interactive mode, enhanced code quality through Go interfaces, and a robust test suite. The migration to client-go for Kubernetes operations will ensure better performance and reduced external dependencies, providing users with a reliable and efficient command-line utility for chaos engineering and Kubernetes management tasks.	{go,kubernetes}	2023	Term 3	https://github.com/litmuschaos/litmus/issues/4101	https://litmuschaos.io/	0	13
558	bb8ddf84-31d7-4a89-9e4b-e6aa9601c0db	CNCF - Meshery: OCI compatible Kubernetes ontology	{"Network topologies and graph databases go hand-in-hand. The OpenAPI specifications for Kubernetes provides taxonomy, but augmenting a graph data model with formalized ontologies enables any number of capabilities, one of the more straightforward is the inferencing requisite for natural language processing, and consequently, a human-centric query "," response interaction becomes becomes possible. More importantly, more advanced systems can be built when a graph data model of connected systems is upgraded to be a knowledge semantic graph. Deliverables (among other items):\n\n- MeshModel capabilities browser\n- Import","export of MeshModel models and components as OCI images\n- augmentation of cuelang-based component generator"}	Network topologies and graph databases go hand-in-hand. The OpenAPI specifications for Kubernetes provides taxonomy, but augmenting a graph data model with formalized ontologies enables any number of capabilities, one of the more straightforward is the inferencing requisite for natural language processing, and consequently, a human-centric query / response interaction becomes becomes possible. More importantly, more advanced systems can be built when a graph data model of connected systems is upgraded to be a knowledge semantic graph. Deliverables (among other items):\n\n- MeshModel capabilities browser\n- Import/export of MeshModel models and components as OCI images\n- augmentation of cuelang-based component generator	{go,cuelang,oci}	2023	Term 2	https://github.com/cncf/tag-network/issues/24	https://meshery.io/	600000	186
524	007ca3e9-c121-4428-8e63-57bc0418e98a	CNCF - Notary: Develop content for Notary documentation and blogs	{"Develop content for Notary documentation and write blog posts to educate users about the Notary use cases. Write user guides, contributing guides, and developer guides for every new Notary release and keep those content up-to-date.\n- Expected Outcome: \n   - Write user guides with end-to-end scenarios based on given doc structure and requirement\n   - Write contributing guides and developer guides, ensure new developers can easily build and start contributing to Notary subprojects \n   - Write blog posts to educate users to use Notation with cloud-native ecosystem tools"}	Develop content for Notary documentation and write blog posts to educate users about the Notary use cases. Write user guides, contributing guides, and developer guides for every new Notary release and keep those content up-to-date.\n- Expected Outcome: \n   - Write user guides with end-to-end scenarios based on given doc structure and requirement\n   - Write contributing guides and developer guides, ensure new developers can easily build and start contributing to Notary subprojects \n   - Write blog posts to educate users to use Notation with cloud-native ecosystem tools	{oci,docker,kubernetes,notary,git,markdown}	2023	Term 2	https://github.com/notaryproject/notaryproject.dev/issues/195	https://notaryproject.dev	300000	155
437	9710c834-913d-487d-9ebf-8205cdf48ab4	CNCF - Notary: HashiCorp Vault plugin for Notary	{"Description: Notary is a CNCF incubating project that aims to provide signing and verification capabilities to ensure delivery integrity and security. It supports creating and storing signatures for container images, SBOM, vulnerability scanning results, etc. to ensure the artifacts someone produced have not been tampered by others. Notary only has an Azure Key Vault plugin for storing keys in Azure Key Vault, which is used to sign and verify artifacts in the OCI registry. [HashiCorp Vault](https:","",github.com,hashicorp,"vault) is a popular KMS and we see more and more users rely on it in the on-premise environment.\n\nExpected Outcome: Develop a Key Management System (KMS) plugin with [HashiCorp Vault](https:","",github.com,hashicorp,"vault) for Notary CLI (Notation), which can be used to store the keys for Notation signing and verification."}	Description: Notary is a CNCF incubating project that aims to provide signing and verification capabilities to ensure delivery integrity and security. It supports creating and storing signatures for container images, SBOM, vulnerability scanning results, etc. to ensure the artifacts someone produced have not been tampered by others. Notary only has an Azure Key Vault plugin for storing keys in Azure Key Vault, which is used to sign and verify artifacts in the OCI registry. [HashiCorp Vault](https://github.com/hashicorp/vault) is a popular KMS and we see more and more users rely on it in the on-premise environment.\n\nExpected Outcome: Develop a Key Management System (KMS) plugin with [HashiCorp Vault](https://github.com/hashicorp/vault) for Notary CLI (Notation), which can be used to store the keys for Notation signing and verification.	{go,notary}	2023	Term 1	https://github.com/notaryproject/notation/issues/521	https://notaryproject.dev	300000	155
121	6a442970-08e6-4fc5-a9e9-4e8af8741e64	Enable ONAP to deploy Kubernetes workloads	{"Increasingly, edge locations are becoming Kubernetes (K8s) based as K8s can support multiple deployment types (Virtual Network Functions, Containerized Network Functions, Virtual Machines and containers). ONAP4K8s is the project that enables ONAP to deploy workloads in K8s based sites.\n\nONAP4K8s enables operators to use a K8s based sites to deploy workloads. In ONAP they can use this to deploy workloads on both Openstack based sites as well K8s based. It also enables usage of common compute resources for both network functions and applications, thereby utilizing compute infrastructure efficiently. ONAP4K8s is being extended to support deploying and connecting apps","services that span across multiple clusters.\n\nMajor work items where intern can help are –\n1.\tAPI enhancements\n2.\tImprove code coverage\n3.\tAdding support for application upgrades\n4.\tAdding test cases for new use cases\n5.\tImproving monitoring, tracing, logging of microservices"}	Increasingly, edge locations are becoming Kubernetes (K8s) based as K8s can support multiple deployment types (Virtual Network Functions, Containerized Network Functions, Virtual Machines and containers). ONAP4K8s is the project that enables ONAP to deploy workloads in K8s based sites.\n\nONAP4K8s enables operators to use a K8s based sites to deploy workloads. In ONAP they can use this to deploy workloads on both Openstack based sites as well K8s based. It also enables usage of common compute resources for both network functions and applications, thereby utilizing compute infrastructure efficiently. ONAP4K8s is being extended to support deploying and connecting apps/services that span across multiple clusters.\n\nMajor work items where intern can help are –\n1.\tAPI enhancements\n2.\tImprove code coverage\n3.\tAdding support for application upgrades\n4.\tAdding test cases for new use cases\n5.\tImproving monitoring, tracing, logging of microservices	{scripting}	2020	Term 2	https://github.com/onap/multicloud-k8s	https://wiki.onap.org/pages/viewpage.action?pageId=60889650	600000	67
507	82a552f5-fc93-499c-87f9-5510e63dba23	LF Networking - 2023 OpenDaylight's CI pipelines and containers Modernization	{"Interested Interns: DO NOT apply through this platform.  Due to a bug, it does not contact the Mentor.  Please go to https:","",wiki.lfnetworking.org,x,"EqsZB. nRead the complete description.  When you a ready to apply send your application documents to your Mentor(s) and mentorship@lfnetworking.org nSelected interns will then apply here to finalize their paperwork. n nODL CI","CD toolchain, as most of the LFN ones, is based on Jenkins which has developed new key features around Kubernetes not yet implemented by ODL (more generally LFN). nAs part of ODL IT modernization, the proposal here is to update our system by switching from agents running on baremetal or on virtual machines to containers running on a Kubernetes cluster. nThe clear first benefit is to make sure that resources are used effectively when testing our patchsets and when building our distributions. nBut, as important, it also enforces clean isolation between jobs and increase the capability to troubleshot the continuous integration issues more easily. nIt should be noted that this topic was also discussed during LFN Events in Anuket tracks and many projects could be very interested in these actions."}	Interested Interns: DO NOT apply through this platform.  Due to a bug, it does not contact the Mentor.  Please go to https://wiki.lfnetworking.org/x/EqsZB. nRead the complete description.  When you a ready to apply send your application documents to your Mentor(s) and mentorship@lfnetworking.org nSelected interns will then apply here to finalize their paperwork. n nODL CI/CD toolchain, as most of the LFN ones, is based on Jenkins which has developed new key features around Kubernetes not yet implemented by ODL (more generally LFN). nAs part of ODL IT modernization, the proposal here is to update our system by switching from agents running on baremetal or on virtual machines to containers running on a Kubernetes cluster. nThe clear first benefit is to make sure that resources are used effectively when testing our patchsets and when building our distributions. nBut, as important, it also enforces clean isolation between jobs and increase the capability to troubleshot the continuous integration issues more easily. nIt should be noted that this topic was also discussed during LFN Events in Anuket tracks and many projects could be very interested in these actions.	{docker,kubernetes,jenkins,bash,python,packer}	2023	Term 2	https://wiki.lfnetworking.org/x/EqsZB	https://wiki.lfnetworking.org/x/EqsZB	300000	161
463	2995c2cc-d72e-43f8-aae2-4d87f284b0ff	LF Networking - Anuket AI/ML Models for NFV Usecases R&D	{"his project aims to develop AI","ML models for NFV-usecases. Any two of the following three problems can be considered. \n   VNF","CNF resource",performance,"failure prediction\n   NFV log analysis with NLP\n   Synthetic monitoring and logging data generation using GANs\n\nLearning Objectives\n   ML Techniques: Deep_learning.\n   ML model development\n   AI","ML for Telco Usecases.\n\nInterested Interns: DO NOT apply through this platform.  Due to a bug, please go to https:","",wiki.lfnetworking.org,x,"EasZB.\nRead the complete description.  When you a ready to apply send your application documents to your Mentor(s) and mentorship@lfnetworking.org"}	his project aims to develop AI/ML models for NFV-usecases. Any two of the following three problems can be considered. \n   VNF/CNF resource/performance/failure prediction\n   NFV log analysis with NLP\n   Synthetic monitoring and logging data generation using GANs\n\nLearning Objectives\n   ML Techniques: Deep_learning.\n   ML model development\n   AI/ML for Telco Usecases.\n\nInterested Interns: DO NOT apply through this platform.  Due to a bug, please go to https://wiki.lfnetworking.org/x/EasZB.\nRead the complete description.  When you a ready to apply send your application documents to your Mentor(s) and mentorship@lfnetworking.org	{tensorflow,anuket,nfv,cnf}	2022	Term 2	https://wiki.lfnetworking.org/x/EasZB	https://wiki.lfnetworking.org/x/EasZB	600000	161
464	9457747c-09ca-4b1f-b7fc-e38640fd6c55	LF Networking - Modernize OpenDaylight's CI pipelines and containers	{"Interested Interns: DO NOT apply through this platform.  Due to a bug, it does not contact the Mentor.  Please go to https:","",wiki.lfnetworking.org,x,"EqsZB.\nRead the complete description.  When you a ready to apply send your application documents to your Mentor(s) and mentorship@lfnetworking.org\nSelected interns will then apply here to finalize their paperwork.\n\nODL CI","CD toolchain, as most of the LFN ones, is based on Jenkins which has developed new key features around Kubernetes not yet implemented by ODL (more generally LFN).\nAs part of ODL IT modernization, the proposal here is to update our system by switching from agents running on baremetal or on virtual machines to containers running on a Kubernetes cluster.\nThe clear first benefit is to make sure that resources are used effectively when testing our patchsets and when building our distributions.\nBut, as important, it also enforces clean isolation between jobs and increase the capability to troubleshot the continuous integration issues more easily.\nIt should be noted that this topic was also discussed during LFN Events in Anuket tracks and many projects could be very interested in these actions."}	Interested Interns: DO NOT apply through this platform.  Due to a bug, it does not contact the Mentor.  Please go to https://wiki.lfnetworking.org/x/EqsZB.\nRead the complete description.  When you a ready to apply send your application documents to your Mentor(s) and mentorship@lfnetworking.org\nSelected interns will then apply here to finalize their paperwork.\n\nODL CI/CD toolchain, as most of the LFN ones, is based on Jenkins which has developed new key features around Kubernetes not yet implemented by ODL (more generally LFN).\nAs part of ODL IT modernization, the proposal here is to update our system by switching from agents running on baremetal or on virtual machines to containers running on a Kubernetes cluster.\nThe clear first benefit is to make sure that resources are used effectively when testing our patchsets and when building our distributions.\nBut, as important, it also enforces clean isolation between jobs and increase the capability to troubleshot the continuous integration issues more easily.\nIt should be noted that this topic was also discussed during LFN Events in Anuket tracks and many projects could be very interested in these actions.	{docker,kubernetes,jenkins,bash,python,packer}	2022	Term 2	https://wiki.lfnetworking.org/x/EqsZB	https://wiki.lfnetworking.org/x/EqsZB	0	161
101	aefe1ce7-77ea-48bf-8668-0def865896a6	LF Networking ONAP - ETSI NFV APIs Conformance Test for OVP	{"Now VTP as the VNF test platform can support OVP VNF compliance and validation test. It can manage and orchestrate different VNF testing scenario across different SUT environment like ONAP MANO, OpenStack Cloud, Vendor VNFM, SDN Controller, etc under one umbrella across different Test providers and partner labs.\n\nAs the open source test platform, it should include standard test case from standard organization, so that it can support the testing for standard component which can accelerate OVP test and certification process for commercial products implemented by reference standards.\n\nVTP is going to support ETSI NFV APIs conformance test. The mentee will be required to make the technical research and develop related test cases  and integrate with VTP  under guide of our team."}	Now VTP as the VNF test platform can support OVP VNF compliance and validation test. It can manage and orchestrate different VNF testing scenario across different SUT environment like ONAP MANO, OpenStack Cloud, Vendor VNFM, SDN Controller, etc under one umbrella across different Test providers and partner labs.\n\nAs the open source test platform, it should include standard test case from standard organization, so that it can support the testing for standard component which can accelerate OVP test and certification process for commercial products implemented by reference standards.\n\nVTP is going to support ETSI NFV APIs conformance test. The mentee will be required to make the technical research and develop related test cases  and integrate with VTP  under guide of our team.	{python,git,database,oop}	2020	Term 2	https://gerrit.onap.org/	https://wiki.lfnetworking.org/pages/viewpage.action?pageId=33423631	300000	161
102	55fb6aae-5cf8-4c08-9775-fcf8d4553277	LF Networking OPNFV - Software Delivery Verification Tool Development and Testing	{"The validation of post-software deployment, conducted in two parts:  Part 1) Validation of the installation manifests will be performed against the requirements (probably defined in a machine-readable format in PDF 2.0) and the software repositories; Part 2) Validation (comparison) of actual installation vs. the expected installation through log, directory, components, configuration, and software stack.  Apart from ensuring that the requirements are met, this validation helps in minimizing","eliminating any deployment errors, drives test-automation, and checks for consistency to achieve efficient automation.\n\nHave a comprehensive understanding of the CNTT community, especially Reference Implementation and Reference Compliance\n\nHands-on opportunity for software tool development, testing and verification\n\nHave a comprehensive understanding of software deployment tools (e.g. Airship) and related hardware provision interface, e.g. IPMI, Redfish\n"}	The validation of post-software deployment, conducted in two parts:  Part 1) Validation of the installation manifests will be performed against the requirements (probably defined in a machine-readable format in PDF 2.0) and the software repositories; Part 2) Validation (comparison) of actual installation vs. the expected installation through log, directory, components, configuration, and software stack.  Apart from ensuring that the requirements are met, this validation helps in minimizing/eliminating any deployment errors, drives test-automation, and checks for consistency to achieve efficient automation.\n\nHave a comprehensive understanding of the CNTT community, especially Reference Implementation and Reference Compliance\n\nHands-on opportunity for software tool development, testing and verification\n\nHave a comprehensive understanding of software deployment tools (e.g. Airship) and related hardware provision interface, e.g. IPMI, Redfish	{python,git}	2020	Term 2	https://github.com/cntt-n/CNTT/blob/master/doc/ref_impl/cntt-ri/chapters/chapter03.md	https://wiki.lfnetworking.org/display/LN/SD	600000	161
8	a721e8b8-5999-45a7-920a-279eaca3983d	OpenHPC Summer Mentorship 2022	{"OpenHPC is accepting applications for a new mentorship program that aims to give students practical experience contributing to an open-source project and an opportunity to work with volunteers from the project’s Technical Steering Committee (TSC). OpenHPC is a software stack that aggregates a variety of software components ranging from administrative tools like bare metal provisioning and resource management to end-user development that span a range of scientific","numerical uses. If your interest is in the domain of high performance computing , scientific computing or cluster management, this might be the right program for you. This program will occur during the Summer of 2022. See our website for details and project ideas - https:","",bit.ly,"ohpcmentorship2022. For questions feel free to email - mentorship@openhpc.community"}	OpenHPC is accepting applications for a new mentorship program that aims to give students practical experience contributing to an open-source project and an opportunity to work with volunteers from the project’s Technical Steering Committee (TSC). OpenHPC is a software stack that aggregates a variety of software components ranging from administrative tools like bare metal provisioning and resource management to end-user development that span a range of scientific/numerical uses. If your interest is in the domain of high performance computing , scientific computing or cluster management, this might be the right program for you. This program will occur during the Summer of 2022. See our website for details and project ideas - https://bit.ly/ohpcmentorship2022. For questions feel free to email - mentorship@openhpc.community	{git,bash,latex}	2022	Term 2	https://github.com/openhpc/ohpc	https://github.com/openhpc/ohpc/wiki/Mentorship-Program-Call-for-Participation---Summer-2022	1860000	4
449	d3a1507a-b132-4c7c-aead-dfe78fd34eb8	CNCF - OpenKruise: Bring progressive delivery to daemon workload	{"Description: Kruise Rollout enable progressive delivery of various workload ranging from stateless workload such as Deployment to stateful workload such as StatefulSet or customized operators. This project aims to bring progress delivery capability to daemon workload, which is run on each node of a k8s cluster. The project involves implementing common API of progressive delivery for OpenKruise Advance DaemonSet, and integrate with the Kruise Rollout framework. \n\nExpected Outcome: Support progressive delivery for OpenKruise Advance DaemonSet(along with supporting test cases and docs) , that is, update new version of daemon pods in batches with user defined pause strategy. Traffic scheduling is not required for this project."}	Description: Kruise Rollout enable progressive delivery of various workload ranging from stateless workload such as Deployment to stateful workload such as StatefulSet or customized operators. This project aims to bring progress delivery capability to daemon workload, which is run on each node of a k8s cluster. The project involves implementing common API of progressive delivery for OpenKruise Advance DaemonSet, and integrate with the Kruise Rollout framework. \n\nExpected Outcome: Support progressive delivery for OpenKruise Advance DaemonSet(along with supporting test cases and docs) , that is, update new version of daemon pods in batches with user defined pause strategy. Traffic scheduling is not required for this project.	{go,kubernetes}	2023	Term 1	https://github.com/openkruise/rollouts/issues/69	https://openkruise.io	600000	152
541	9aaaa048-baae-4ec7-aedd-5d3594dd95d8	Open Mainframe Project- Software Discovery Tool back end improvements and deployment automation	{"The Software Discovery Tool is an online search tool for Linux distributions and other software sources that support the s390x (IBM zSystems) hardware architecture. At the core, it’s written in Flask, micro web framework written in Python. The mentorship this summer has several components, giving a potential students the ability to review outstanding issues and craft a program that leans into their strengths and stretches into areas they may be interested in. The core task we’d like to see completed is the implementation of a MySQL back-end, which is currently in a “proposed” state and needs to be reviewed and tested for robustness. The administration and deployment of the tool in production also needs to be automated further, as some of this is still a manual process for the version in production. Finally, there are also UI improvements needed for the tool to address common pain points.  Experience with Python will be needed to complete this mentorship, and the potential mentee should also have interest in learning how to manage a production environment on Linux."}	The Software Discovery Tool is an online search tool for Linux distributions and other software sources that support the s390x (IBM zSystems) hardware architecture. At the core, it’s written in Flask, micro web framework written in Python. The mentorship this summer has several components, giving a potential students the ability to review outstanding issues and craft a program that leans into their strengths and stretches into areas they may be interested in. The core task we’d like to see completed is the implementation of a MySQL back-end, which is currently in a “proposed” state and needs to be reviewed and tested for robustness. The administration and deployment of the tool in production also needs to be automated further, as some of this is still a manual process for the version in production. Finally, there are also UI improvements needed for the tool to address common pain points.  Experience with Python will be needed to complete this mentorship, and the potential mentee should also have interest in learning how to manage a production environment on Linux.	{python,mysql,linux,flask}	2023	Term 2	https://github.com/openmainframeproject		600000	177
3	491ad5ae-67d0-40dc-8078-a5b1c0f5d944	Open Mainframe Project 2020 Mentorship - Kube CF - Endgame Platform on Z	{"Following up on the previous projects that helped build the Cloud Foundry components and the CF Operator on Z. The Kube CF project will be incubated in the Cloud Foundry Foundation in 2020, and there are release integration efforts happening, that will allow us to plugin tests and builds specifically for s390x."}	Following up on the previous projects that helped build the Cloud Foundry components and the CF Operator on Z. The Kube CF project will be incubated in the Cloud Foundry Foundation in 2020, and there are release integration efforts happening, that will allow us to plugin tests and builds specifically for s390x.	{kubernetes}	2020	Term 2	https://github.com/openmainframeproject-internship/Kube-CF---Endgame-Platform-on-Z	https://www.openmainframeproject.org/projects/mentorship-program	600000	2
165	6bfb3b5f-0819-482f-b90a-a5ebf458c69b	Open@RIT - University OSPO Playbook	{"Build a documentation playbook detailing the specifics and resources needed for running Open Programs Offices within a university. This work will involve developing an outline of topics required, summarizing various resources, and working with team members to re-format documentation to be used in our playbook.\n\nEssential skills\n\n - Writing educational material that evolves over time\n - Ability to quickly learn & understand complex topics\n - Copywriting branding content - simplifying technical concepts for non-technical folks\n - Social Media","Journalism\n - Computing generalist\n\nPrevious co-op","internship experience a plus·\n\nExperience in working with and","or contributing to Open Source Software, Open Data, Open Hardware or similar communities a plus\n\nMentees will learn how to work within a multi-disciplinary team of developers, designers, and writers to collate and re-package technical information for a broader, more generalized audience.\n\n\nEssential skills\n\n - Writing educational material that evolves over time\n - Ability to quickly learn & understand complex topics\n - Copywriting branding content - simplifying technical concepts for non-technical folks\n - Social Media","Journalism\n - Computing generalist\n\nPrevious co-op","internship experience a plus·\n\nExperience in working with and","or contributing to Open Source Software, Open Data, Open Hardware or similar communities a plus"}	Build a documentation playbook detailing the specifics and resources needed for running Open Programs Offices within a university. This work will involve developing an outline of topics required, summarizing various resources, and working with team members to re-format documentation to be used in our playbook.\n\nEssential skills\n\n - Writing educational material that evolves over time\n - Ability to quickly learn & understand complex topics\n - Copywriting branding content - simplifying technical concepts for non-technical folks\n - Social Media/Journalism\n - Computing generalist\n\nPrevious co-op/internship experience a plus·\n\nExperience in working with and/or contributing to Open Source Software, Open Data, Open Hardware or similar communities a plus\n\nMentees will learn how to work within a multi-disciplinary team of developers, designers, and writers to collate and re-package technical information for a broader, more generalized audience.\n\n\nEssential skills\n\n - Writing educational material that evolves over time\n - Ability to quickly learn & understand complex topics\n - Copywriting branding content - simplifying technical concepts for non-technical folks\n - Social Media/Journalism\n - Computing generalist\n\nPrevious co-op/internship experience a plus·\n\nExperience in working with and/or contributing to Open Source Software, Open Data, Open Hardware or similar communities a plus	{documentation}	2021	Term 2	https://opensource.ieee.org/rit/ospo-playbook		600000	97
394	f56cbce6-eed6-4633-8f1e-77169e8911ce	Adding Single Precision Floating Point (F) extension in NucleusRV Core	{"\\"NucleusRV Core:\nNucleusRV is the RISC-V based Core that currently supports I, M and C extensions. It is used along with the SoC-Now SoC Generator that accepts parameters and generates an entire SoC comprising of a Core with selective extensions and other configuration, devices which are selected via parameters and a Bus Interconnect that is used as the communication medium between the Core and the Devices, which is also selected by means of parameters.\nAdding the support of Single Precision Floating Point (F) extension in NucleusRV Core would enable RISC-V to reach even more use-cases on the deeply embedded side. This way the SoC-Now Generator’s scope will also expand so that now the parameter to include the F extension in the Core will become available. Henceforth increasing the value of the Generator and the Core standalone itself. The SoC-Now generator furthermore uses two Frameworks Caravan and Jigsaw, for connecting Buses and Devices respectively in any CHISEL Design","IP.\n\nSingle Precision Floating Point (F):\nRISC-V ISA contains an extension for single precision floating point instructions, which means that all arithmetic and logic operations can be executed with floating point (single precision) values.\\""}	"NucleusRV Core:\nNucleusRV is the RISC-V based Core that currently supports I, M and C extensions. It is used along with the SoC-Now SoC Generator that accepts parameters and generates an entire SoC comprising of a Core with selective extensions and other configuration, devices which are selected via parameters and a Bus Interconnect that is used as the communication medium between the Core and the Devices, which is also selected by means of parameters.\nAdding the support of Single Precision Floating Point (F) extension in NucleusRV Core would enable RISC-V to reach even more use-cases on the deeply embedded side. This way the SoC-Now Generator’s scope will also expand so that now the parameter to include the F extension in the Core will become available. Henceforth increasing the value of the Generator and the Core standalone itself. The SoC-Now generator furthermore uses two Frameworks Caravan and Jigsaw, for connecting Buses and Devices respectively in any CHISEL Design/IP.\n\nSingle Precision Floating Point (F):\nRISC-V ISA contains an extension for single precision floating point instructions, which means that all arithmetic and logic operations can be executed with floating point (single precision) values."	{hdl,rtl}	2023	Term 1	https://github.com/merledu/nucleusrv		300000	171
257	75ebb813-9588-4b60-9fe1-258d90788220	Feature optimizations for RISCV-CTG (Compatibility Test Generator) and RISCV-ISAC (RISCV ISA Coverag	{"Introduction RISCV-ISAC: RISC-V ISAC is an ISA coverage extraction tool. Given a set of cover points and an execution trace of a test","application run on a model, ISAC can provide a report indicating in detail which of those cover points were covered by the test","application. ISAC also holds the capability to provide a detailed quality analysis on data propagation occurring within the test","application.RISCV-CTG is the RISC-V based Compatibility Test Generator. This tool is used to generate tests used in the official RISC-V Architectural Test Suite and the RISC-V architectural test framework RISCOF. All tests generated by the CTG are compliant with the official Test Format Spec.\nThe CTG is similar to a constrained test generator capable of generating tests targeting a specific set of constraints. These constraints are supplied to the CTG using the Coverage Group Format (CGF) File. The CGF file contains various cover-points for different instructions. The CTG treats each cover-point as a constraint and employs a solver to identify potential solutions. CTG uses the constraint satisfaction problem (CSPs) solvers offered by the python-constraint package."}	Introduction RISCV-ISAC: RISC-V ISAC is an ISA coverage extraction tool. Given a set of cover points and an execution trace of a test/application run on a model, ISAC can provide a report indicating in detail which of those cover points were covered by the test/application. ISAC also holds the capability to provide a detailed quality analysis on data propagation occurring within the test/application.RISCV-CTG is the RISC-V based Compatibility Test Generator. This tool is used to generate tests used in the official RISC-V Architectural Test Suite and the RISC-V architectural test framework RISCOF. All tests generated by the CTG are compliant with the official Test Format Spec.\nThe CTG is similar to a constrained test generator capable of generating tests targeting a specific set of constraints. These constraints are supplied to the CTG using the Coverage Group Format (CGF) File. The CGF file contains various cover-points for different instructions. The CTG treats each cover-point as a constraint and employs a solver to identify potential solutions. CTG uses the constraint satisfaction problem (CSPs) solvers offered by the python-constraint package.	{python,git}	2022	Term 1	https://github.com/riscv-software-src/riscv-ctg/		300000	171
501	47aa2e38-cd34-4f6f-9db1-5530b795c614	LoRaRISCV	{"We propose to recreate the MIT IoTNet solution in silica. It is currently facilitated by mostly ARM processors and external (USB connected) radios. To do this modernization we are going to design a RISC-V solution that incorporates SDR. We are looking at the trade space now since we know there are some vendors that have products that might be ideal for us. However, we are looking specifically at GNU radio. \n\nThe background for this project is substantially at\nhttps:","",tinyurl.com,"mitiotnet\nIn our first attempts we used TI Beagleboards and Pandaboards. For our next version we will use homegrown MIT Beaverboards and BU Terrierboards. We don’t anticipate having a chip design together soon, either through this effort or our ongoing research. But we may participate with a vendor who already has a board in the marketplace. This of course would depend on whether we could either put an (external) radio on a board or find a way to get our firmware onto the board."}	We propose to recreate the MIT IoTNet solution in silica. It is currently facilitated by mostly ARM processors and external (USB connected) radios. To do this modernization we are going to design a RISC-V solution that incorporates SDR. We are looking at the trade space now since we know there are some vendors that have products that might be ideal for us. However, we are looking specifically at GNU radio. \n\nThe background for this project is substantially at\nhttps://tinyurl.com/mitiotnet\nIn our first attempts we used TI Beagleboards and Pandaboards. For our next version we will use homegrown MIT Beaverboards and BU Terrierboards. We don’t anticipate having a chip design together soon, either through this effort or our ongoing research. But we may participate with a vendor who already has a board in the marketplace. This of course would depend on whether we could either put an (external) radio on a board or find a way to get our firmware onto the board.	{ide,eclipse}	2023	Term 2	https://github.com/clusterchallenge/BTScan		1500000	171
258	2021e650-c533-4671-afed-bf87c089af09	RISC-V Mentorship: Porting V8 to RISC-V R32G	{"The RISC-V Mentorship Program enables one or more 12-week internship-style projects per session, funded by RISC-V, to match mentors","project leaders together with mentees","interns . Mentees are guided through a series of milestones by one or more project mentors, with whom the mentees meet on a weekly basis.\n\nThis program pairs one mentee with an experienced mentor to deliver a V8 JavaScript engine port for a 32-bit RISC-V core.\n\nThe V8 JavaScript engine for RISCV64G has been upstreamed to Chromium recently. As a basic component for the Chromium web browser and node.js, it would enlarge RISCV’s application scenario. Although RV32G V8 port would be quite similar to RV64G V8 port , it is still in the TODO list. Porting and enable the RV32G on V8 will bring the embedded RISCV software ecosystem more applications, make RISC-V embed processors more competitive.\n\nDeliverables (bullet list of components and the changes expected):\n- Turbofan backend implementation\n- Embedded simulator implementation\n- Corresponding unit tests implementation\n- Regression tests pass\n \nAcceptance criteria (bullet list with measurable results defined):\n- RV32G cross-compiled and simulator build on both debug and release configuration should be passed\n- A helloworld demo should run successfully on both the embedded simulator and a real or emulated hardware (i.e. a real board or QEMU emulation).\n- 97% of the regression test should pass"}	The RISC-V Mentorship Program enables one or more 12-week internship-style projects per session, funded by RISC-V, to match mentors/project leaders together with mentees/interns . Mentees are guided through a series of milestones by one or more project mentors, with whom the mentees meet on a weekly basis.\n\nThis program pairs one mentee with an experienced mentor to deliver a V8 JavaScript engine port for a 32-bit RISC-V core.\n\nThe V8 JavaScript engine for RISCV64G has been upstreamed to Chromium recently. As a basic component for the Chromium web browser and node.js, it would enlarge RISCV’s application scenario. Although RV32G V8 port would be quite similar to RV64G V8 port , it is still in the TODO list. Porting and enable the RV32G on V8 will bring the embedded RISCV software ecosystem more applications, make RISC-V embed processors more competitive.\n\nDeliverables (bullet list of components and the changes expected):\n- Turbofan backend implementation\n- Embedded simulator implementation\n- Corresponding unit tests implementation\n- Regression tests pass\n \nAcceptance criteria (bullet list with measurable results defined):\n- RV32G cross-compiled and simulator build on both debug and release configuration should be passed\n- A helloworld demo should run successfully on both the embedded simulator and a real or emulated hardware (i.e. a real board or QEMU emulation).\n- 97% of the regression test should pass	{javascript}	2021	Term 2	https://github.com/riscv	https://riscv.org/community/risc-v-mentorship-program/	1080000	171
458	2597fc3d-eb2c-411f-b02d-940c8347328d	CNCF - Service Mesh Performance: Adaptive Load Controller II	{"The adaptive load controller is to execute optimization routines recursivley to determine the maximum load a system can sustain. The maximum load is usually defined by the maximum requests per second (rps) the system can handle. The metrics (CPU usage, latency etc) collected from the system under test are the constraints we provide to judge whether a system under test (SUT) is sustaining the load.\n\nA use-case that fits very well is be the ability to use it to run performance tests on a schedule and track the maximum load a system can handle over time. This could give insights to performance improvements or degradations."}	The adaptive load controller is to execute optimization routines recursivley to determine the maximum load a system can sustain. The maximum load is usually defined by the maximum requests per second (rps) the system can handle. The metrics (CPU usage, latency etc) collected from the system under test are the constraints we provide to judge whether a system under test (SUT) is sustaining the load.\n\nA use-case that fits very well is be the ability to use it to run performance tests on a schedule and track the maximum load a system can handle over time. This could give insights to performance improvements or degradations.	{grpc,docker,kubernetes,golang}	2023	Term 1	https://github.com/service-mesh-performance/service-mesh-performance/issues/350	https://smp-spec.io/	300000	121
364	2c4510d6-7b73-4082-a3f4-209f61767263	CNCF - Service Mesh Performance: Convergence of Network and Graph topologies	{"Use Neo4j's ability to create graph projections, which copy a subgraph to RAM so that algorithms can be efficiently run. This opens the door to leveraging algorithms in the areas of Centrality, Community Detection, Pathfinding, Topological Link Prediction, etc. Bringing to bear advances made in Machine Learning "," AI "," recommendation systems, fraud detection could really help to derive meaning and comprehension for future tools. Another example is how ML + graph approaches are used to find and determine the optimal molecular structure of atoms such that desired physical properties are targeted. This approach could be applied to the problem of workload sizing and estimation for service mesh operators and would-be adopters."}	Use Neo4j's ability to create graph projections, which copy a subgraph to RAM so that algorithms can be efficiently run. This opens the door to leveraging algorithms in the areas of Centrality, Community Detection, Pathfinding, Topological Link Prediction, etc. Bringing to bear advances made in Machine Learning / AI / recommendation systems, fraud detection could really help to derive meaning and comprehension for future tools. Another example is how ML + graph approaches are used to find and determine the optimal molecular structure of atoms such that desired physical properties are targeted. This approach could be applied to the problem of workload sizing and estimation for service mesh operators and would-be adopters.	{go,cuelang}	2022	Term 3	https://github.com/service-mesh-performance/service-mesh-performance/issues/351	https://smp-spec.io/	300000	121
289	2ba7b837-6385-46d7-9bdd-f8f5d4d570c5	CNCF - Service Mesh Performance: Definition of MeshMark (extended)	{"Description: Create MeshMark provides a universal performance index to gauge your mesh’s efficiency against deployments in other organizations’ environments. MeshMark functions as a service mesh performance index (a scale) to provide people the ability to weigh the value of their service mesh versus the overhead of their service mesh and assess whether they are getting out of the mesh what they are “paying” for in it. Work with maintainers from Layer5, Intel, Red Hat, and HashiCorp on researching cloud native infrastructure performance. Internship involves: machine learning, adaptive algorithms, running and analyzing performance statistics."}	Description: Create MeshMark provides a universal performance index to gauge your mesh’s efficiency against deployments in other organizations’ environments. MeshMark functions as a service mesh performance index (a scale) to provide people the ability to weigh the value of their service mesh versus the overhead of their service mesh and assess whether they are getting out of the mesh what they are “paying” for in it. Work with maintainers from Layer5, Intel, Red Hat, and HashiCorp on researching cloud native infrastructure performance. Internship involves: machine learning, adaptive algorithms, running and analyzing performance statistics.	{analytics,algorithms,go}	2022	Term 1	https://github.com/service-mesh-performance/service-mesh-performance/issues/227	https://smp-spec.io/	300000	121
224	4c06915e-8e72-4321-95c5-ed0ee9428a32	CNCF - Thanos: Add metrics to track the progress for compaction and downsampling	{"This project is about improving the observability of the Thanos compactor component to help user track the current progress. Thanos compactor usually has to deal with large TSDB blocks for compaction and downsampling. When the data are large, it takes a long time to finish the compaction or downsampling. Now users have to check the bucket UI manually to see whether the work is done. It would be better to have a more fine-grained way to tell the progress of compaction and downsampling. We can have some metrics to track the work and even integrate the stats into the UI."}	This project is about improving the observability of the Thanos compactor component to help user track the current progress. Thanos compactor usually has to deal with large TSDB blocks for compaction and downsampling. When the data are large, it takes a long time to finish the compaction or downsampling. Now users have to check the bucket UI manually to see whether the work is done. It would be better to have a more fine-grained way to tell the progress of compaction and downsampling. We can have some metrics to track the work and even integrate the stats into the UI.	{go}	2021	Term 3	https://github.com/thanos-io/thanos/issues/3985	https://thanos.io/	300000	26
196	dfccf818-c7f7-4074-9e1b-2805698d9d89	CNCF - Thanos: Descriptive API definitions using OpenAPI and Protobuf	{"In order to improve Thanos usage for users, we would like to define our APIs, both HTTP and gRPC, in protobuf","OpenAPI and expose the automatically generated documentation in the website. We also want to define the configuration of our components in protobuf. This would allow users to use tools for documentation, validation, type checking and even code generation to use our APIs efficiently. During this project we also expect collaboration with the Prometheus project to implement similar improvements on Prometheus' side. https:","",github.com,cncf,mentoring,blob,master,summerofcode,"2021.md#port-the-prometheus-api-to-openapi. Optionally we would like to work on the index page on every Thanos component server that will expose those resources for easier debug."}	In order to improve Thanos usage for users, we would like to define our APIs, both HTTP and gRPC, in protobuf/OpenAPI and expose the automatically generated documentation in the website. We also want to define the configuration of our components in protobuf. This would allow users to use tools for documentation, validation, type checking and even code generation to use our APIs efficiently. During this project we also expect collaboration with the Prometheus project to implement similar improvements on Prometheus' side. https://github.com/cncf/mentoring/blob/master/summerofcode/2021.md#port-the-prometheus-api-to-openapi. Optionally we would like to work on the index page on every Thanos component server that will expose those resources for easier debug.	{go,yaml}	2021	Term 2	https://github.com/thanos-io/thanos/issues/4102	https://thanos.io/	300000	26
582	5a96f43c-d858-40c2-b556-2770ba6b03d4	CNCF - Thanos: Implement fan-out query observability in Thanos	{"In the previous mentorship sessions we added the foundation required for query observability in Thanos's new [promql-engine](https:","",github.com,thanos-io,"promql-engine) and hooked it up in the UI. We now have the foundation to record telemetry from our query engine as well such as time consumed per operator.\n  This project aims to expand on this and add more metadata to the query execution, both on the promql-engine operator tree level and Thanos Query `Select()` calls for fan-out query observability.\n  Once we have this metadata, we would like to visualize it in the Query UI.\n- Expected Outcome:\n  The end goal is to have a query execution tree decorated with the metadata, collected during execution (ideally even visualized in the Thanos UI). This will help users to understand the performance implications of their PromQL queries and the bottlenecks in their Thanos Query setups.\n\nhttps:","",github.com,thanos-io,thanos,issues,"6517\nhttps:","",github.com,thanos-community,promql-engine,issues,106}	In the previous mentorship sessions we added the foundation required for query observability in Thanos's new [promql-engine](https://github.com/thanos-io/promql-engine) and hooked it up in the UI. We now have the foundation to record telemetry from our query engine as well such as time consumed per operator.\n  This project aims to expand on this and add more metadata to the query execution, both on the promql-engine operator tree level and Thanos Query `Select()` calls for fan-out query observability.\n  Once we have this metadata, we would like to visualize it in the Query UI.\n- Expected Outcome:\n  The end goal is to have a query execution tree decorated with the metadata, collected during execution (ideally even visualized in the Thanos UI). This will help users to understand the performance implications of their PromQL queries and the bottlenecks in their Thanos Query setups.\n\nhttps://github.com/thanos-io/thanos/issues/6517\nhttps://github.com/thanos-community/promql-engine/issues/106	{golang,typescript,git,github}	2023	Term 3	https://github.com/thanos-io/thanos/issues/6517	https://thanos.io/	0	26
147	328fcf60-0dd2-425d-93a4-06046e4166fb	CNCF - Thanos: Multi-Tenant Instrumentation for Thanos operations	{"Thanos can store and serve the data for multiple tenants at once. However, currently, Thanos does not always provide the needed introspective information about actions related to the tenant (e.g external labels). Allowing admins to obtain tenants’ information on per tenant queries, operations and ingestion would give actionable insight and answer questions such as: What data is used","queried the most for a tenant X? During this mentorship, you will implement logic that will enormously improve the experience of running multi-tenant Thanos on the scale. You will learn more about Go, instrumentation, multitenancy, APIs, and SRE concepts like SLOs."}	Thanos can store and serve the data for multiple tenants at once. However, currently, Thanos does not always provide the needed introspective information about actions related to the tenant (e.g external labels). Allowing admins to obtain tenants’ information on per tenant queries, operations and ingestion would give actionable insight and answer questions such as: What data is used/queried the most for a tenant X? During this mentorship, you will implement logic that will enormously improve the experience of running multi-tenant Thanos on the scale. You will learn more about Go, instrumentation, multitenancy, APIs, and SRE concepts like SLOs.	{go,prometheus}	2021	Term 1	https://github.com/thanos-io/thanos/issues/3572	https://thanos.io/	300000	26
282	06ecd0e0-8d29-44e4-b249-80dd07704564	CNCF - Tremor: CI and Release process improvements	{"Description: Tremor has a lot of headroom when it comes to improving the CI and the build process. Those improvements will make the day-to-day life of contributors better and gives end-users more frequent and up-to-date builds allowing them to be used in a more cloud-native fashion. The goal is to make the general developer and user experience around contributing and releasing better. This project is well suited for someone interested in the DevOps","SRE world but offer stretch goals to reach into other topics."}	Description: Tremor has a lot of headroom when it comes to improving the CI and the build process. Those improvements will make the day-to-day life of contributors better and gives end-users more frequent and up-to-date builds allowing them to be used in a more cloud-native fashion. The goal is to make the general developer and user experience around contributing and releasing better. This project is well suited for someone interested in the DevOps/SRE world but offer stretch goals to reach into other topics.	{make,git,ci,gitops,devops}	2022	Term 1	https://github.com/tremor-rs/tremor-runtime/issues/1452	https://www.tremor.rs/	300000	20
320	759a56fe-5e07-4078-9ad9-165ae85a0939	CNCF - Vitess: Add complete parsing support for MySQL functions	{"Description: Vitess is a database clustering system for horizontal scaling of MySQL. One of the key goals of Vitess is to emulate MySQL behavior even while running multiple MySQL instances so that ORMs and frameworks work seamlessly. Vitess has its own in-built SQL-parser which it uses to understand the query and represent as structs for further processing. As of now, a lot of MySQL functions are not parsed correctly and result in syntax errors. Parsing for a lot of the newer features in MySQL 8.0 is also missing. The task of the mentee would be to add parsing support for such functions and features."}	Description: Vitess is a database clustering system for horizontal scaling of MySQL. One of the key goals of Vitess is to emulate MySQL behavior even while running multiple MySQL instances so that ORMs and frameworks work seamlessly. Vitess has its own in-built SQL-parser which it uses to understand the query and represent as structs for further processing. As of now, a lot of MySQL functions are not parsed correctly and result in syntax errors. Parsing for a lot of the newer features in MySQL 8.0 is also missing. The task of the mentee would be to add parsing support for such functions and features.	{go,sql,yacc}	2022	Term 1	https://github.com/vitessio/vitess/issues/8604	https://vitess.io/	300000	107
361	845ccf34-d7aa-45cf-abc2-1b3064e96af1	CNCF - Vitess: Add complete parsing support for Spatial MySQL functions II	{"Vitess is a database clustering system for horizontal scaling of MySQL. One of the key goals of Vitess is to emulate MySQL behavior even while running multiple MySQL instances so that ORMs and frameworks work seamlessly. Vitess has its own in-built SQL-parser which it uses to understand the query and represent as structs for further processing. As of now, a lot of spatial MySQL functions are not parsed correctly and result in syntax errors. The task of the mentee would be to add parsing support for such functions and features which can be found at https:","",dev.mysql.com,doc,refman,8.0,en,spatial-analysis-functions.html}	Vitess is a database clustering system for horizontal scaling of MySQL. One of the key goals of Vitess is to emulate MySQL behavior even while running multiple MySQL instances so that ORMs and frameworks work seamlessly. Vitess has its own in-built SQL-parser which it uses to understand the query and represent as structs for further processing. As of now, a lot of spatial MySQL functions are not parsed correctly and result in syntax errors. The task of the mentee would be to add parsing support for such functions and features which can be found at https://dev.mysql.com/doc/refman/8.0/en/spatial-analysis-functions.html	{go,sql,yacc,compiler,lexers}	2022	Term 3	https://github.com/vitessio/vitess/issues/8604	https://vitess.io/	0	107
396	b903d812-c3ff-47bf-8626-0b9274fec742	CNCF - Vitess: Implement a benchmarking and load testing framework for the VReplication module	{"Description: Vitess is a distributed database system built around MySQL. VReplication is core technology built into Vitess that is used to enable many features like vertical and horizontal sharding, change data capture and materialized views. The project involves designing and implementing a customizable framework that enables us to test different VReplication workflows at scale and to obtain benchmarks that can be used to monitor performance improvements and regression from code changes. The framework will consist of a custom DSL (Domain Specific Language) which will be used to define each test case and a driver which will read the DSLs and execute the tests. The DSL will be based on the Hashicorp Configuration Language (https:","",github.com,hashicorp,"hcl). The driver will be written in Golang and target AWS using Terraform for provisioning and Ansible for automation. The results and benchmarks will be stored in PlanetScale (https:","",planetscale.com,") in the existing vitess benchmark database.\n\nExpected Outcome: The test framework with at least one working test and stored benchmark metrics for a MoveTables workflow."}	Description: Vitess is a distributed database system built around MySQL. VReplication is core technology built into Vitess that is used to enable many features like vertical and horizontal sharding, change data capture and materialized views. The project involves designing and implementing a customizable framework that enables us to test different VReplication workflows at scale and to obtain benchmarks that can be used to monitor performance improvements and regression from code changes. The framework will consist of a custom DSL (Domain Specific Language) which will be used to define each test case and a driver which will read the DSLs and execute the tests. The DSL will be based on the Hashicorp Configuration Language (https://github.com/hashicorp/hcl). The driver will be written in Golang and target AWS using Terraform for provisioning and Ansible for automation. The results and benchmarks will be stored in PlanetScale (https://planetscale.com/) in the existing vitess benchmark database.\n\nExpected Outcome: The test framework with at least one working test and stored benchmark metrics for a MoveTables workflow.	{go}	2023	Term 1	https://github.com/vitessio/vitess/issues/12136	https://vitess.io/	300000	107
386	29ec853c-3ab9-4457-ac91-d273fa073d49	CNCF - Vitess: Improve evaluation engine	{"Improve the compatbility of Vitess' evaluation engine against MySQL by adding support for more built-in SQL functions.\n\nDetailed description: The evaluation engine in Vitess is one of the most critical parts of our query serving infrastructure. This engine is capable of evaluating arbitrary SQL expressions directly inside Vitess' process, without reaching out to a live MySQL instance, and this allows us to plan and execute complex user queries (e.g. queries that contain WHERE and similar filter clauses) between Vitess shards much more efficiently. If you're interested in this GSoC project, your task for the summer will involve continuing the work on this evaluation engine by implementing support for as many built-in SQL functions as possible, using the behavior of MySQL as a reference.\n\nExpected outcomes: We expect the Evaluation Engine in Vitess to be close to 100% compatible with MySQL after all the leftover SQL built-ins have been implemented."}	Improve the compatbility of Vitess' evaluation engine against MySQL by adding support for more built-in SQL functions.\n\nDetailed description: The evaluation engine in Vitess is one of the most critical parts of our query serving infrastructure. This engine is capable of evaluating arbitrary SQL expressions directly inside Vitess' process, without reaching out to a live MySQL instance, and this allows us to plan and execute complex user queries (e.g. queries that contain WHERE and similar filter clauses) between Vitess shards much more efficiently. If you're interested in this GSoC project, your task for the summer will involve continuing the work on this evaluation engine by implementing support for as many built-in SQL functions as possible, using the behavior of MySQL as a reference.\n\nExpected outcomes: We expect the Evaluation Engine in Vitess to be close to 100% compatible with MySQL after all the leftover SQL built-ins have been implemented.	{go,sql}	2022	Term 3	https://github.com/vitessio/vitess/issues/9647	https://vitess.io/	0	107
540	8299d27a-9e36-4de6-abbc-c9282634ee03	CNCF - Vitess: Rework the frontend UI of Vitess’ benchmarking tools	{"Vitess uses a couple of tools to benchmark its codebase and to make sure that new code doesn’t introduce performance regressions. These tools are: arewefastyet and the VReplication Benchmarking Framework. We currently have an old frontend UI that serves arewefastyet. However, this UI is slow, not optimized and not easily extensible. It uses the built-in Golang template system to serve pages. We would like to create a common frontend UI that will be used by both benchmarking tools and that will replace the current arewefastyet’s UI. The mentee will have the responsibility of creating the UI using (most likely) React","Vite on Vercel. The frontend component will connect to our already-existing backend components: a MySQL database and arewefastyet’s REST API.\n- Expected Outcome: The expected outcome is to have a working frontend UI that integrates well with our different backends (databases and benchmarking tools’ APIs)."}	Vitess uses a couple of tools to benchmark its codebase and to make sure that new code doesn’t introduce performance regressions. These tools are: arewefastyet and the VReplication Benchmarking Framework. We currently have an old frontend UI that serves arewefastyet. However, this UI is slow, not optimized and not easily extensible. It uses the built-in Golang template system to serve pages. We would like to create a common frontend UI that will be used by both benchmarking tools and that will replace the current arewefastyet’s UI. The mentee will have the responsibility of creating the UI using (most likely) React/Vite on Vercel. The frontend component will connect to our already-existing backend components: a MySQL database and arewefastyet’s REST API.\n- Expected Outcome: The expected outcome is to have a working frontend UI that integrates well with our different backends (databases and benchmarking tools’ APIs).	{react,vite,vercel}	2023	Term 2	https://github.com/vitessio/arewefastyet/issues/328	https://vitess.io/	540000	107
376	9f0d56c0-9781-4912-988f-86443b0dd161	CNCF - Volcano: Pick out reasonable victim pods for rescheduling plugin	{"Currently, rescheduling is a little rough to evict victim pods without difference. It should distinguish pods with more consideration such as pod priority, namespace and so on. Your task is to take a full consideration about all the scenarios, provide a design documentation, implement your idea and give a full test."}	Currently, rescheduling is a little rough to evict victim pods without difference. It should distinguish pods with more consideration such as pod priority, namespace and so on. Your task is to take a full consideration about all the scenarios, provide a design documentation, implement your idea and give a full test.	{go,volcano}	2022	Term 3	https://github.com/volcano-sh/volcano/issues/2425	https://volcano.sh/	0	34
613	f21bec94-085e-4e2d-94ac-3b61b4471818	CNCF - WasmEdge: Add matrix operations for OpenCVMini-Wasm-Plugin	{"WasmEdge is a WebAssembly runtime that supports both interpreter and ahead-of-time modes. To expand its capabilities, WasmEdge provides a plugin system that helps people attach more existing software. OpenCVMini is one of them, intended to help users get a limited OpenCV interface. With this feature, AI inference will have more flexible helper functions for pre-processing and post-processing. In this mentorship, we aim to add more OpenCV capabilities to the WasmEdge environment.\n- Expected Outcome:\n  - Define the new interfaces for the OpenCVMini plugin, which supports those functions listed in the upstream issue.\n  - Use the above interface and generate related APIs.\n  - Implement the functions of the plugin with OpenCV 4.x.\n  - Design unit tests and examples for verifying the above functions.\n  - Enable the building, testing, and packaging on the upstream CI.\n  - Write documents about how to build and use this plugin."}	WasmEdge is a WebAssembly runtime that supports both interpreter and ahead-of-time modes. To expand its capabilities, WasmEdge provides a plugin system that helps people attach more existing software. OpenCVMini is one of them, intended to help users get a limited OpenCV interface. With this feature, AI inference will have more flexible helper functions for pre-processing and post-processing. In this mentorship, we aim to add more OpenCV capabilities to the WasmEdge environment.\n- Expected Outcome:\n  - Define the new interfaces for the OpenCVMini plugin, which supports those functions listed in the upstream issue.\n  - Use the above interface and generate related APIs.\n  - Implement the functions of the plugin with OpenCV 4.x.\n  - Design unit tests and examples for verifying the above functions.\n  - Enable the building, testing, and packaging on the upstream CI.\n  - Write documents about how to build and use this plugin.	{wasm,rust,tokio}	2023	Term 3	https://github.com/WasmEdge/WasmEdge/issues/2680	https://wasmedge.org/	0	31
338	666271e6-7bdf-4050-a2a2-3adaac5a7c13	CNCF - WasmEdge: Create a Tokio-like async runtime in WasmEdge	{"One of the most important features of WasmEdge is its support for non-blocking network sockets. However, the current WasmEdge API for async networking is still cumbersome. Rust developers would prefer to use a Tokio-like async "," await API for such tasks. But Tokio is multi-threaded and cannot run correctly in standard single-threaded WebAssembly. Yet, it is possible to provide a single-threaded Tokio runtime. Our goal is to create a WebAssembly compatible Tokio scheduler."}	One of the most important features of WasmEdge is its support for non-blocking network sockets. However, the current WasmEdge API for async networking is still cumbersome. Rust developers would prefer to use a Tokio-like async / await API for such tasks. But Tokio is multi-threaded and cannot run correctly in standard single-threaded WebAssembly. Yet, it is possible to provide a single-threaded Tokio runtime. Our goal is to create a WebAssembly compatible Tokio scheduler.	{wasm,rust,tokio}	2022	Term 2	https://github.com/WasmEdge/WasmEdge/issues/1429	https://wasmedge.org/	300000	31
379	17fc622c-5674-4381-b597-2f49409fda01	CNCF - WasmEdge: OpenCV SDKs for Wasm in WasmEdge	{"WasmEdge is a leading WebAssembly runtime for AI inference. It supports AI frameworks such as Tensorflow, OpenVINO and PyTorch. A compelling use case is computer vision applications on the edge. Computer vision applications need to pre-process images and videos into tensor formats before applying the AI model. They then often need to overlay the tensor results onto the original image. In our existing demos, we use the Rust [image crate](https:","",crates.io,crates,"image) to process images. However, the crate only has limited features and is inadequate for many computer vision applications. In the Python-based computer vision applications, the image pre-processing is often done with the Python wrapper for OpenCV library. The OpenCV library itself is written in C and can be compiled into WebAssembly. We would like to create an OpenCV SDK that allows WebAssembly applications to call OpenCV functions."}	WasmEdge is a leading WebAssembly runtime for AI inference. It supports AI frameworks such as Tensorflow, OpenVINO and PyTorch. A compelling use case is computer vision applications on the edge. Computer vision applications need to pre-process images and videos into tensor formats before applying the AI model. They then often need to overlay the tensor results onto the original image. In our existing demos, we use the Rust [image crate](https://crates.io/crates/image) to process images. However, the crate only has limited features and is inadequate for many computer vision applications. In the Python-based computer vision applications, the image pre-processing is often done with the Python wrapper for OpenCV library. The OpenCV library itself is written in C and can be compiled into WebAssembly. We would like to create an OpenCV SDK that allows WebAssembly applications to call OpenCV functions.	{rust}	2022	Term 3	https://github.com/WasmEdge/WasmEdge/issues/1747	https://wasmedge.org/	300000	31
372	d01efa41-87a7-4f34-adfe-63c7bab7c1ca	CNCF - WasmEdge: Porting OpenVINO on multiple platforms for the WASI-NN proposal in WasmEdge	{"The [OpenVINO](https:","",www.intel.com,content,www,us,en,developer,tools,openvino-toolkit,"download.html) official release supports various platforms. WasmEdge supports the WASI-NN proposal with OpenVINO backend now, but only in Ubuntu 20.04. In this project, we want to porting and integrating the OpenVINO installation for the multiple platforms such as MacOS, Windows, or manylinux with the WasmEdge WASI-NN plugin."}	The [OpenVINO](https://www.intel.com/content/www/us/en/developer/tools/openvino-toolkit/download.html) official release supports various platforms. WasmEdge supports the WASI-NN proposal with OpenVINO backend now, but only in Ubuntu 20.04. In this project, we want to porting and integrating the OpenVINO installation for the multiple platforms such as MacOS, Windows, or manylinux with the WasmEdge WASI-NN plugin.	{webassembly}	2022	Term 3	https://github.com/WasmEdge/WasmEdge/issues/1742	https://wasmedge.org/	300000	31
614	3ee046ab-29d9-4e5c-bb7a-4564bd4e2765	CNCF - WasmEdge: Support AOT mode in proxy-wasm	{"WasmEdge is a WebAssembly runtime that supports both interpreter and ahead-of-time modes. For proxy-wasm support, WasmEdge only provides the interpreter mode currently. Such as the other runtimes, WasmEdge should be able to support the AOT mode for better performance. In this mentorship, the mentees will help the WasmEdge project to complete the AOT mode in proxy-wasm proposal and write the docs for examples of running with proxy-wasm.\n- Expected Outcome:\n  - Modify the Bazel file to include the LLVM dependency.\n  - Modify the code to support running WASM in AOT mode.\n  - Add the documentation of proxy-wasm in the WasmEdge docs repo."}	WasmEdge is a WebAssembly runtime that supports both interpreter and ahead-of-time modes. For proxy-wasm support, WasmEdge only provides the interpreter mode currently. Such as the other runtimes, WasmEdge should be able to support the AOT mode for better performance. In this mentorship, the mentees will help the WasmEdge project to complete the AOT mode in proxy-wasm proposal and write the docs for examples of running with proxy-wasm.\n- Expected Outcome:\n  - Modify the Bazel file to include the LLVM dependency.\n  - Modify the code to support running WASM in AOT mode.\n  - Add the documentation of proxy-wasm in the WasmEdge docs repo.	{bazel}	2023	Term 3	https://github.com/WasmEdge/WasmEdge/issues/2686	https://wasmedge.org/	0	31
431	1d5d1fcd-b671-4367-b6db-13ef263aece1	CNCF - WasmEdge: WasmEdge C++ SDK	{"Description: WasmEdge provides C SDK as the based library and uses this to implement other languages SDK such as Golang, Rust, Java, and Python(developing). We would like to provide C++ SDK in this task.\n\nExpected Outcome: A document to explain the C++ SDK, a test suite cover the implementation details, and the implementation of WasmEdge Basics and VM sections in the C SDK."}	Description: WasmEdge provides C SDK as the based library and uses this to implement other languages SDK such as Golang, Rust, Java, and Python(developing). We would like to provide C++ SDK in this task.\n\nExpected Outcome: A document to explain the C++ SDK, a test suite cover the implementation details, and the implementation of WasmEdge Basics and VM sections in the C SDK.	{webassembly}	2023	Term 1	https://github.com/WasmEdge/WasmEdge/issues/2241	https://wasmedge.org/	300000	31
167	b4a7d639-aefb-485a-a5ab-96cbed8f344f	Improve the Zowe doc site generator and UI	{"Zowe docs consist of several high-level sections such as Getting Started, User Guide, Extending, etc. Due to the limitation of the Vuepress (current site generator) sidebar depth, the sidebar or navigation experience is not satisfactory. Also most open source docs now provide a three column layout and many other social functions such as Share. The goal is to provide a good and more industry-standard doc navigation experience for the Zowe doc site with better UI and functions that engage users. A new site generator can be explored.\n\nDetails at https:","",github.com,zowe,docs-site,issues,1587}	Zowe docs consist of several high-level sections such as Getting Started, User Guide, Extending, etc. Due to the limitation of the Vuepress (current site generator) sidebar depth, the sidebar or navigation experience is not satisfactory. Also most open source docs now provide a three column layout and many other social functions such as Share. The goal is to provide a good and more industry-standard doc navigation experience for the Zowe doc site with better UI and functions that engage users. A new site generator can be explored.\n\nDetails at https://github.com/zowe/docs-site/issues/1587	{documentation}	2021	Term 2	https://github.com/zowe/docs-site	https://zowe.org	300000	38
560	38377e17-64b3-41e3-8b61-38d881b60809	Open Mainframe- Move COBOL Check to the Next Level	{"COBOL Check is a framework that enables unit testing of COBOL programs.\n\nCurrently, the framework is useable and supports the goal of unit testing COBOL programs. But we have many ideas to improve the project.\n\nWe would like a mentee to help us add improvements, like code completion and a linter for the Visual Studio Code extension, more mocking functionality, additional output formats, extended expander functionality, and many other features.\n\nOther goals for the mentee are to get more of the existing code under tests, add to the documentation of the project, and improve the maintainability of the project. \n\nThe ultimate goal is to release version 1.0.0 of COBOL Check.\n\nDuring the mentorship, the mentee is expected to have dailies with the mentor and will get access to the European team heavily involved in developing the current version of COBOL Check."}	COBOL Check is a framework that enables unit testing of COBOL programs.\n\nCurrently, the framework is useable and supports the goal of unit testing COBOL programs. But we have many ideas to improve the project.\n\nWe would like a mentee to help us add improvements, like code completion and a linter for the Visual Studio Code extension, more mocking functionality, additional output formats, extended expander functionality, and many other features.\n\nOther goals for the mentee are to get more of the existing code under tests, add to the documentation of the project, and improve the maintainability of the project. \n\nThe ultimate goal is to release version 1.0.0 of COBOL Check.\n\nDuring the mentorship, the mentee is expected to have dailies with the mentor and will get access to the European team heavily involved in developing the current version of COBOL Check.	{java,typescript}	2023	Term 2	https://github.com/openmainframeproject/cobol-check		540000	38
\.


--
-- Name: parentorgs_id_seq; Type: SEQUENCE SET; Schema: public; Owner: admin
--

SELECT pg_catalog.setval('public.parentorgs_id_seq', 186, true);


--
-- Name: projects_id_seq; Type: SEQUENCE SET; Schema: public; Owner: admin
--

SELECT pg_catalog.setval('public.projects_id_seq', 626, true);


--
-- Name: parentorgs parentorgs_name_key; Type: CONSTRAINT; Schema: public; Owner: admin
--

ALTER TABLE ONLY public.parentorgs
    ADD CONSTRAINT parentorgs_name_key UNIQUE (name);


--
-- Name: parentorgs parentorgs_pkey; Type: CONSTRAINT; Schema: public; Owner: admin
--

ALTER TABLE ONLY public.parentorgs
    ADD CONSTRAINT parentorgs_pkey PRIMARY KEY (id);


--
-- Name: projects projects_lfxprojectid_key; Type: CONSTRAINT; Schema: public; Owner: admin
--

ALTER TABLE ONLY public.projects
    ADD CONSTRAINT projects_lfxprojectid_key UNIQUE (lfxprojectid);


--
-- Name: projects projects_name_key; Type: CONSTRAINT; Schema: public; Owner: admin
--

ALTER TABLE ONLY public.projects
    ADD CONSTRAINT projects_name_key UNIQUE (name);


--
-- Name: projects projects_pkey; Type: CONSTRAINT; Schema: public; Owner: admin
--

ALTER TABLE ONLY public.projects
    ADD CONSTRAINT projects_pkey PRIMARY KEY (id);


--
-- Name: projects fk_organizationid; Type: FK CONSTRAINT; Schema: public; Owner: admin
--

ALTER TABLE ONLY public.projects
    ADD CONSTRAINT fk_organizationid FOREIGN KEY (organizationid) REFERENCES public.parentorgs(id) ON DELETE CASCADE;


--
-- PostgreSQL database dump complete
--

